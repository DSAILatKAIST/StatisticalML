{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf3d5b07",
   "metadata": {},
   "source": [
    "<b>DISCLAUMER: </b> Due to working on two systems for ideation and execution, it was hard compiling them back. I was under the impression the codes submitted were only to verify cheating, therefore, I didn't incoporated towards my approach during the competition. I am sorry for the inconvinience, but I have summarized my code in this file and I have attached all my work in the zip file, please find it attached."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02a5b526",
   "metadata": {},
   "source": [
    "# 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2bb3bf4",
   "metadata": {},
   "source": [
    "# 1. Load Base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not change this cell. \n",
    "## The reason for this is to prevent cheating using the original data from DACON. \n",
    "## If an assertion error occurs in that cell when TAs evaluate the submitted code of each student, it is considered cheating.\n",
    "\n",
    "train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "park = pd.read_csv(os.path.join(INPUT_DIR, 'park.csv'))\n",
    "dcc = pd.read_csv(os.path.join(INPUT_DIR, 'day_care_center.csv'))\n",
    "\n",
    "assert train.shape[0] == 329690 and train.shape[1] == 13, 'Do not change the format of the input data.'\n",
    "assert test.shape[0] == 85097 and test.shape[1] == 12, 'Do not change the format of the input data.'\n",
    "assert park.shape[0] == 1359 and park.shape[1] == 7, 'Do not change the format of the input data.'\n",
    "assert dcc.shape[0] == 7373 and dcc.shape[1] == 10, 'Do not change the format of the input data.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a719a230",
   "metadata": {},
   "source": [
    "# 2. Load additional dataset\n",
    "- Explain here the additional dataset that you used.\n",
    "- If you do not use any additional dataset, it's ok and keep empty the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87e74561",
   "metadata": {},
   "source": [
    "# 3. EDA and Preprocessing\n",
    "- Conduct exploratory data analysis and explain the insights from your analyses.\n",
    "- Conduct data preprocessing, including feature engineering, and explain your insights that derive such techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0407236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ae0a8c",
   "metadata": {},
   "source": [
    "### PARK AND DCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dcc.copy()\n",
    "dc = dc.sort_values(by=['gu','city'])\n",
    "# dc = dc.fillna(method='backfill')\n",
    "# dc = dc.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcb3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.groupby(['gu','city']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e178af",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_dc = pd.merge(park,dc,on=['gu','city'],how='left')\n",
    "park_dc = park_dc.sort_values(by=['gu','city'])\n",
    "park_dc = park_dc.drop(['park_open_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "dong_gu = park_dc.groupby(['gu','city'])['dong'].unique()\n",
    "dong_gu = pd.DataFrame(dong_gu)\n",
    "dong_gu = dong_gu.reset_index()\n",
    "dong_gu = dong_gu.explode('dong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dong_gu = dong_gu.drop_duplicates(subset = ['city','dong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_dc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "park_dc[['day_care_baby_num','day_care_teacher_num','day_care_baby_per_teacher']] = imputer.fit_transform(park_dc[['day_care_baby_num','day_care_teacher_num','day_care_baby_per_teacher']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_dc = park_dc.drop(['park_name'], axis=1)\n",
    "park_dc = pd.get_dummies(park_dc, columns=['park_type'])\n",
    "park_dc = park_dc.groupby(['gu','city']).sum().reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b1a9eb5",
   "metadata": {},
   "source": [
    "### PREPROCESSING TRAINING AND TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f774280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train,dong_gu[['gu','city','dong']],on=['dong','city'],how='left')\n",
    "test = pd.merge(test,dong_gu[['gu','city','dong']],on=['dong','city'],how='left')\n",
    "\n",
    "park_dc = park_dc.drop(['dong'],axis=1)\n",
    "\n",
    "train = pd.merge(train,park_dc,on=['gu','city'],how='left')\n",
    "test = pd.merge(test,park_dc,on=['gu','city'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01513ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_train):\n",
    "    df_train['month_end'] = np.where(df_train['transaction_day'].isin(df_train['transaction_day'].unique()[-4:]),1,0)\n",
    "    df_train['month_mid'] = np.where(df_train['transaction_day'] == df_train['transaction_day'].unique()[0],1,0)\n",
    "    df_train['month_start'] = np.where(df_train['transaction_day'] == df_train['transaction_day'].unique()[1],1,0)\n",
    "    df_train.drop(columns=['transaction_day'],axis=1,inplace=True)\n",
    "\n",
    "    df_train['building_age'] = df_train['transaction_year'] - df_train['built_year']\n",
    "\n",
    "    df_train['total_amenities_child'] = np.log(df_train['day_care_baby_num'] + df_train['teacher_num']+df_train['nursing_room_num']\n",
    "                                         +df_train['playground_num']+df_train['CCTV_num'])\n",
    "    \n",
    "    # df_train['total_amenities_child_percentile'] = df_train['total_amenities_child'].rank(pct=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_train['teacher_per_child'] = df_train['teacher_num']/df_train['day_care_baby_num']\n",
    "    # df_train['teacher_per_child_percentile'] = df_train['teacher_per_child'].rank(pct=True)\n",
    "\n",
    "    #use KNN to fill missing values of lat and long\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_train[['lat','long']] = imputer.fit_transform(df_train[['lat','long']])\n",
    "    df_train['lat'] = df_train['lat'].round(6)\n",
    "    df_train['long'] = df_train['long'].round(6)\n",
    "\n",
    "    # df_train['lat_norm'] = df_train.groupby('gu').lat.transform(lambda x: (x-x.mean())/x.std())\n",
    "    # df_train['long_norm'] = df_train.groupby('gu').long.transform(lambda x: (x-x.mean())/x.std())\n",
    "    # df_train = df_train.drop(columns=['lat','long'],axis=1)\n",
    "\n",
    "    # df_train['height_percentile_gu'] = df_train.groupby('gu').floor.transform(lambda x: round(x/x.max(),3))\n",
    "    df_train['height_percentile_dong'] = df_train.groupby('dong').floor.transform(lambda x: round(x/x.max(),3))\n",
    "    df_train['floor_percentile'] = df_train.groupby('apartment_id').floor.transform(lambda x: round(x/x.max(),3))\n",
    "\n",
    "    # df_train['house_area_percentile_gu'] = df_train.groupby('gu').house_area.transform(lambda x: round(x/x.mean(),3))\n",
    "    # df_train['house_area_percentile_dong'] = df_train.groupby('dong').house_area.transform(lambda x: round(x/x.mean(),3))\n",
    "    df_train['house_area_percentile'] = df_train.groupby('apartment_id').house_area.transform(lambda x: round(x/x.mean(),3))\n",
    "\n",
    "    \n",
    "\n",
    "    #LOG TRANSFORM\n",
    "    #day_care_baby_num, teacher_num, nursing_room_num, playground_num, CCTV_num divided \n",
    "    \n",
    "    # df_train['day_care_baby_num'] = np.log(df_train['day_care_baby_num']+1)\n",
    "    # df_train['teacher_num'] = np.log(df_train['teacher_num']+1)\n",
    "    # df_train['nursing_room_num'] = np.log(df_train['nursing_room_num']+1)\n",
    "    # df_train['playground_num'] = np.log(df_train['playground_num']+1)\n",
    "    # df_train['CCTV_num'] = np.log(df_train['CCTV_num']+1)\n",
    "    # df_train['park_area'] = np.log(df_train['park_area']+1)\n",
    "    # df_train['house_area'] = np.log(df_train['house_area'])\n",
    "\n",
    "    df_train['city'] = np.where(df_train['city']=='seoul',1,0)\n",
    "\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ed642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train,test],axis=0)\n",
    "df_train = train\n",
    "df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea56d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['index'].isin(df_train['index'])]\n",
    "test = df[df['index'].isin(df_test['index'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train.csv')\n",
    "test.to_csv('data/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a28a944",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d392152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv', index_col=0)\n",
    "df_test = pd.read_csv('data/test.csv', index_col=0)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_train['dong'] = df_train['dong'].astype(int)\n",
    "df_test['dong'] = df_test['dong'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54813f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['PRICE']\n",
    "X = df_train.drop(['index','PRICE'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "X,y = shuffle_in_unison(np.array(X),np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(X))\n",
    "\n",
    "regularizer = 0.005\n",
    "\n",
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=l2(regularizer)),\n",
    "\n",
    "\n",
    "    \n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error',metrics=['mean_absolute_error'],\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.5,\n",
    "                              # monitor='mean_absolute_error', factor=0.2,\n",
    "                              patience=3, min_lr=0.0001)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_absolute_error',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X, y, \n",
    "                    validation_split=0.05,\n",
    "                    epochs=50, \n",
    "                    callbacks=[reduce_lr, early_stopping],\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_absolute_error',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.5,\n",
    "                              # monitor='mean_absolute_error', factor=0.2,\n",
    "                              patience=3, min_lr=0.00001)\n",
    "\n",
    "model.fit(X, y, \n",
    "                    validation_split=0.05,\n",
    "                    epochs=50, \n",
    "                    callbacks=[reduce_lr, early_stopping],\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.fit(X, y, \n",
    "                    validation_split=0.05,\n",
    "                    epochs=50, \n",
    "                    callbacks=[reduce_lr, early_stopping],\n",
    "                    batch_size=128\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "053b947b",
   "metadata": {},
   "source": [
    "# 5. Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test\n",
    "X_test = test.drop(['index','PRICE'],axis=1)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test = X_test.to_numpy()\n",
    "test['PRICE'] = model.predict(X_test)\n",
    "\n",
    "test = test[['index','PRICE']]\n",
    "test.to_csv('last_1_last_last.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
