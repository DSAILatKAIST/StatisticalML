{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpZniY39kOiv"
      },
      "source": [
        "# set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBe80y_OjJVm",
        "outputId": "11bc6b35-e60e-430f-bce3-5ccc6381a588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/KAIST/semester6/IE343/IE343 Kaggle\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FnjpQRYjMmq",
        "outputId": "6e44bf97-6c54-4d4d-cd68-3a1d96e4541b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(329690, 13) (85097, 12) (1359, 7) (7373, 10)\n"
          ]
        }
      ],
      "source": [
        "INPUT_DIR='kaist-ie343-2023-spring'\n",
        "OUTPUT_DIR='kaist-ie343-2023-spring'\n",
        "\n",
        "## Do not change this cell. \n",
        "## The reason for this is to prevent cheating using the original data from DACON. \n",
        "## If an assertion error occurs in that cell when TAs evaluate the submitted code of each student, it is considered cheating.\n",
        "\n",
        "train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
        "park = pd.read_csv(os.path.join(INPUT_DIR, 'park.csv'))\n",
        "dcc = pd.read_csv(os.path.join(INPUT_DIR, 'day_care_center.csv'))\n",
        "\n",
        "assert train.shape[0] == 329690 and train.shape[1] == 13, 'Do not change the format of the input data.'\n",
        "assert test.shape[0] == 85097 and test.shape[1] == 12, 'Do not change the format of the input data.'\n",
        "assert park.shape[0] == 1359 and park.shape[1] == 7, 'Do not change the format of the input data.'\n",
        "assert dcc.shape[0] == 7373 and dcc.shape[1] == 10, 'Do not change the format of the input data.'\n",
        "print(train.shape, test.shape, park.shape, dcc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRRzBqtkq5e",
        "outputId": "a1baf4c8-fb2f-462b-96af-19c959a6056b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg4EWREwSRHH"
      },
      "source": [
        "# park analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21LMLzr4SQ77",
        "outputId": "fd5d5ae3-6cb6-4d7e-bdf0-71cb9d6d6a03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         127\n",
              "1         127\n",
              "2         127\n",
              "3         127\n",
              "4         127\n",
              "         ... \n",
              "329685    204\n",
              "329686    204\n",
              "329687    204\n",
              "329688    204\n",
              "329689    204\n",
              "Name: dong, Length: 329690, dtype: int64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_t = dcc.groupby(\"gu\").count()[\"city\"]\n",
        "gu_dcc_num_dict =  dict(df_t)\n",
        "\n",
        "df = park[[\"dong\", \"gu\"]]\n",
        "df_grouped = df.groupby('dong')['gu'].agg(lambda x: pd.Series.mode(x)[0])\n",
        "\n",
        "# Convert the Series to a dictionary\n",
        "dong_to_gu_dict = df_grouped.to_dict()\n",
        "\n",
        "# dong_to_gu_dict\n",
        "train.dong.map(dong_to_gu_dict).map(gu_dcc_num_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-HTIbdU6tbq"
      },
      "source": [
        "# data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5tf8zJb6viK",
        "outputId": "b50f2a24-d078-4cee-b166-a2200278980f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "109    11665\n",
              "195     8363\n",
              "228     8145\n",
              "198     6488\n",
              "138     6209\n",
              "       ...  \n",
              "204       35\n",
              "212       17\n",
              "74        11\n",
              "177        6\n",
              "176        4\n",
              "Name: dong, Length: 203, dtype: int64"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train.dong.value_counts()\n",
        "# df_ids = train.apartment_id.value_counts()\n",
        "# ids = np.array(df_ids[df_ids>100].index)\n",
        "# def map_app_ids (id):\n",
        "#   if id in ids:\n",
        "#     return  id\n",
        "#   else: \n",
        "#     return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUhTwmdl6vgB",
        "outputId": "e6958009-3b7e-4c55-ee82-b5b0def986c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-efcd9a0e-f1f9-4c7d-b8e1-6866612c1642\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>apartment_id</th>\n",
              "      <th>city</th>\n",
              "      <th>dong</th>\n",
              "      <th>house_area</th>\n",
              "      <th>built_year</th>\n",
              "      <th>floor</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>transaction_year</th>\n",
              "      <th>transaction_month</th>\n",
              "      <th>transaction_day</th>\n",
              "      <th>PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>125.865988</td>\n",
              "      <td>1993</td>\n",
              "      <td>5</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>11~20</td>\n",
              "      <td>229250.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>12</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2021</td>\n",
              "      <td>10</td>\n",
              "      <td>1~10</td>\n",
              "      <td>215320.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>91.511175</td>\n",
              "      <td>1993</td>\n",
              "      <td>6</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2020</td>\n",
              "      <td>3</td>\n",
              "      <td>21~31</td>\n",
              "      <td>161740.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>13</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2020</td>\n",
              "      <td>5</td>\n",
              "      <td>11~20</td>\n",
              "      <td>199781.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>4</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2022</td>\n",
              "      <td>6</td>\n",
              "      <td>21~30</td>\n",
              "      <td>219606.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329685</th>\n",
              "      <td>329685</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>101.431912</td>\n",
              "      <td>2014</td>\n",
              "      <td>4</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2022</td>\n",
              "      <td>5</td>\n",
              "      <td>21~31</td>\n",
              "      <td>885070.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329686</th>\n",
              "      <td>329686</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>101.431912</td>\n",
              "      <td>2014</td>\n",
              "      <td>14</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2021</td>\n",
              "      <td>10</td>\n",
              "      <td>1~10</td>\n",
              "      <td>826132.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329687</th>\n",
              "      <td>329687</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>71.687641</td>\n",
              "      <td>2014</td>\n",
              "      <td>2</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2022</td>\n",
              "      <td>11</td>\n",
              "      <td>21~30</td>\n",
              "      <td>697540.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329688</th>\n",
              "      <td>329688</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>137.192013</td>\n",
              "      <td>2014</td>\n",
              "      <td>18</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2020</td>\n",
              "      <td>9</td>\n",
              "      <td>21~30</td>\n",
              "      <td>870656.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329689</th>\n",
              "      <td>329689</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>137.192013</td>\n",
              "      <td>2014</td>\n",
              "      <td>18</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2020</td>\n",
              "      <td>8</td>\n",
              "      <td>21~31</td>\n",
              "      <td>871139.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>329690 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efcd9a0e-f1f9-4c7d-b8e1-6866612c1642')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efcd9a0e-f1f9-4c7d-b8e1-6866612c1642 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efcd9a0e-f1f9-4c7d-b8e1-6866612c1642');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         index  apartment_id   city  dong  house_area  built_year  floor  \\\n",
              "0            0             0  busan   197  125.865988        1993      5   \n",
              "1            1             0  busan   197  101.647190        1993     12   \n",
              "2            2             0  busan   197   91.511175        1993      6   \n",
              "3            3             0  busan   197  101.647190        1993     13   \n",
              "4            4             0  busan   197  101.647190        1993      4   \n",
              "...        ...           ...    ...   ...         ...         ...    ...   \n",
              "329685  329685          4419  seoul    37  101.431912        2014      4   \n",
              "329686  329686          4419  seoul    37  101.431912        2014     14   \n",
              "329687  329687          4419  seoul    37   71.687641        2014      2   \n",
              "329688  329688          4419  seoul    37  137.192013        2014     18   \n",
              "329689  329689          4419  seoul    37  137.192013        2014     18   \n",
              "\n",
              "              lat        long  transaction_year  transaction_month  \\\n",
              "0       35.149929  129.006071              2021                  7   \n",
              "1       35.149929  129.006071              2021                 10   \n",
              "2       35.149929  129.006071              2020                  3   \n",
              "3       35.149929  129.006071              2020                  5   \n",
              "4       35.149929  129.006071              2022                  6   \n",
              "...           ...         ...               ...                ...   \n",
              "329685  37.452039  127.070842              2022                  5   \n",
              "329686  37.452039  127.070842              2021                 10   \n",
              "329687  37.452039  127.070842              2022                 11   \n",
              "329688  37.452039  127.070842              2020                  9   \n",
              "329689  37.452039  127.070842              2020                  8   \n",
              "\n",
              "       transaction_day      PRICE  \n",
              "0                11~20  229250.80  \n",
              "1                 1~10  215320.00  \n",
              "2                21~31  161740.00  \n",
              "3                11~20  199781.80  \n",
              "4                21~30  219606.40  \n",
              "...                ...        ...  \n",
              "329685           21~31  885070.00  \n",
              "329686            1~10  826132.00  \n",
              "329687           21~30  697540.00  \n",
              "329688           21~30  870656.98  \n",
              "329689           21~31  871139.20  \n",
              "\n",
              "[329690 rows x 13 columns]"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_t = train.copy()\n",
        "df_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USuQYMu6EXzK",
        "outputId": "041c102e-af39-4dc5-d216-114de00b59e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         1\n",
              "1         1\n",
              "2         1\n",
              "3         1\n",
              "4         1\n",
              "         ..\n",
              "329685    0\n",
              "329686    0\n",
              "329687    0\n",
              "329688    0\n",
              "329689    0\n",
              "Name: city, Length: 329690, dtype: int32"
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao8v05_x6tTe",
        "outputId": "473f7baf-9360-4a8e-868b-f7cf721f4cd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-59ed9493-df64-4bce-8cab-c594fa5e0b22\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>apartment_id</th>\n",
              "      <th>city</th>\n",
              "      <th>dong</th>\n",
              "      <th>house_area</th>\n",
              "      <th>built_year</th>\n",
              "      <th>floor</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>transaction_year</th>\n",
              "      <th>transaction_month</th>\n",
              "      <th>transaction_day</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>bus_lat</th>\n",
              "      <th>bus_long</th>\n",
              "      <th>seoul_lat</th>\n",
              "      <th>seoul_long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>125.865988</td>\n",
              "      <td>1993</td>\n",
              "      <td>5</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>11~20</td>\n",
              "      <td>229250.80</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>12</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2021</td>\n",
              "      <td>10</td>\n",
              "      <td>1~10</td>\n",
              "      <td>215320.00</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>91.511175</td>\n",
              "      <td>1993</td>\n",
              "      <td>6</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2020</td>\n",
              "      <td>3</td>\n",
              "      <td>21~31</td>\n",
              "      <td>161740.00</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>13</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2020</td>\n",
              "      <td>5</td>\n",
              "      <td>11~20</td>\n",
              "      <td>199781.80</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>4</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2022</td>\n",
              "      <td>6</td>\n",
              "      <td>21~30</td>\n",
              "      <td>219606.40</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329685</th>\n",
              "      <td>329685</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>101.431912</td>\n",
              "      <td>2014</td>\n",
              "      <td>4</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2022</td>\n",
              "      <td>5</td>\n",
              "      <td>21~31</td>\n",
              "      <td>885070.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329686</th>\n",
              "      <td>329686</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>101.431912</td>\n",
              "      <td>2014</td>\n",
              "      <td>14</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2021</td>\n",
              "      <td>10</td>\n",
              "      <td>1~10</td>\n",
              "      <td>826132.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329687</th>\n",
              "      <td>329687</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>71.687641</td>\n",
              "      <td>2014</td>\n",
              "      <td>2</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2022</td>\n",
              "      <td>11</td>\n",
              "      <td>21~30</td>\n",
              "      <td>697540.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329688</th>\n",
              "      <td>329688</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>137.192013</td>\n",
              "      <td>2014</td>\n",
              "      <td>18</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2020</td>\n",
              "      <td>9</td>\n",
              "      <td>21~30</td>\n",
              "      <td>870656.98</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329689</th>\n",
              "      <td>329689</td>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>137.192013</td>\n",
              "      <td>2014</td>\n",
              "      <td>18</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "      <td>2020</td>\n",
              "      <td>8</td>\n",
              "      <td>21~31</td>\n",
              "      <td>871139.20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>329690 rows × 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59ed9493-df64-4bce-8cab-c594fa5e0b22')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59ed9493-df64-4bce-8cab-c594fa5e0b22 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59ed9493-df64-4bce-8cab-c594fa5e0b22');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         index  apartment_id   city  dong  house_area  built_year  floor  \\\n",
              "0            0             0  busan   197  125.865988        1993      5   \n",
              "1            1             0  busan   197  101.647190        1993     12   \n",
              "2            2             0  busan   197   91.511175        1993      6   \n",
              "3            3             0  busan   197  101.647190        1993     13   \n",
              "4            4             0  busan   197  101.647190        1993      4   \n",
              "...        ...           ...    ...   ...         ...         ...    ...   \n",
              "329685  329685          4419  seoul    37  101.431912        2014      4   \n",
              "329686  329686          4419  seoul    37  101.431912        2014     14   \n",
              "329687  329687          4419  seoul    37   71.687641        2014      2   \n",
              "329688  329688          4419  seoul    37  137.192013        2014     18   \n",
              "329689  329689          4419  seoul    37  137.192013        2014     18   \n",
              "\n",
              "              lat        long  transaction_year  transaction_month  \\\n",
              "0       35.149929  129.006071              2021                  7   \n",
              "1       35.149929  129.006071              2021                 10   \n",
              "2       35.149929  129.006071              2020                  3   \n",
              "3       35.149929  129.006071              2020                  5   \n",
              "4       35.149929  129.006071              2022                  6   \n",
              "...           ...         ...               ...                ...   \n",
              "329685  37.452039  127.070842              2022                  5   \n",
              "329686  37.452039  127.070842              2021                 10   \n",
              "329687  37.452039  127.070842              2022                 11   \n",
              "329688  37.452039  127.070842              2020                  9   \n",
              "329689  37.452039  127.070842              2020                  8   \n",
              "\n",
              "       transaction_day      PRICE    bus_lat    bus_long  seoul_lat  \\\n",
              "0                11~20  229250.80  35.149929  129.006071   0.000000   \n",
              "1                 1~10  215320.00  35.149929  129.006071   0.000000   \n",
              "2                21~31  161740.00  35.149929  129.006071   0.000000   \n",
              "3                11~20  199781.80  35.149929  129.006071   0.000000   \n",
              "4                21~30  219606.40  35.149929  129.006071   0.000000   \n",
              "...                ...        ...        ...         ...        ...   \n",
              "329685           21~31  885070.00   0.000000    0.000000  37.452039   \n",
              "329686            1~10  826132.00   0.000000    0.000000  37.452039   \n",
              "329687           21~30  697540.00   0.000000    0.000000  37.452039   \n",
              "329688           21~30  870656.98   0.000000    0.000000  37.452039   \n",
              "329689           21~31  871139.20   0.000000    0.000000  37.452039   \n",
              "\n",
              "        seoul_long  \n",
              "0         0.000000  \n",
              "1         0.000000  \n",
              "2         0.000000  \n",
              "3         0.000000  \n",
              "4         0.000000  \n",
              "...            ...  \n",
              "329685  127.070842  \n",
              "329686  127.070842  \n",
              "329687  127.070842  \n",
              "329688  127.070842  \n",
              "329689  127.070842  \n",
              "\n",
              "[329690 rows x 17 columns]"
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_busan = (df_t[\"city\"]==\"busan\").astype(np.int32)\n",
        "df_t[\"bus_lat\"] = df_t[\"lat\"]*is_busan\n",
        "df_t[\"bus_long\"] = df_t[\"long\"]*is_busan\n",
        "\n",
        "df_t[\"seoul_lat\"] = df_t[\"lat\"]*(1-is_busan)\n",
        "df_t[\"seoul_long\"] = df_t[\"long\"]*(1-is_busan)\n",
        "\n",
        "df_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2eC1yS7JDUK"
      },
      "source": [
        "# inflation adjustment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbemQRvDEfc2",
        "outputId": "2164ab22-ada5-4942-f389-4e5048d68ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ec179c7f29f3>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
            "  df = train.groupby(\"transaction_year\").mean()[\"PRICE\"]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2018: 1.2791254174647047,\n",
              " 2019: 1.2576299204650916,\n",
              " 2020: 1.198718876547877,\n",
              " 2021: 1.1136705032908139,\n",
              " 2022: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = train.groupby(\"transaction_year\").mean()[\"PRICE\"]\n",
        "df.plot()\n",
        "mean_prices = list(df)\n",
        "mean_prices\n",
        "inflation_adjustion = [ (mean_prices[4])/mean_prices[i] for i in range(4)] + [1]\n",
        "\n",
        "\n",
        "inflation_adjustion_dick = dict(zip ([2018, 2019, 2020, 2021, 2022], inflation_adjustion))\n",
        "inflation_adjustion_dick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-nC-1jrJGmK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImnwTEf6JGg9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gleEn7w8kQdX"
      },
      "source": [
        "# dataset pre procc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fOljiAcnk8Rb",
        "outputId": "c1d354b4-cc11-4128-d008-249393b196f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  apartment_id   city  dong  house_area  built_year  floor        lat  \\\n",
              "0      0             0  busan   197  125.865988        1993      5  35.149929   \n",
              "1      1             0  busan   197  101.647190        1993     12  35.149929   \n",
              "2      2             0  busan   197   91.511175        1993      6  35.149929   \n",
              "3      3             0  busan   197  101.647190        1993     13  35.149929   \n",
              "4      4             0  busan   197  101.647190        1993      4  35.149929   \n",
              "\n",
              "         long  transaction_year  transaction_month transaction_day     PRICE  \n",
              "0  129.006071              2021                  7           11~20  229250.8  \n",
              "1  129.006071              2021                 10            1~10  215320.0  \n",
              "2  129.006071              2020                  3           21~31  161740.0  \n",
              "3  129.006071              2020                  5           11~20  199781.8  \n",
              "4  129.006071              2022                  6           21~30  219606.4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b72d90b-ba73-4d89-ad81-0c13f0c67004\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>apartment_id</th>\n",
              "      <th>city</th>\n",
              "      <th>dong</th>\n",
              "      <th>house_area</th>\n",
              "      <th>built_year</th>\n",
              "      <th>floor</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>transaction_year</th>\n",
              "      <th>transaction_month</th>\n",
              "      <th>transaction_day</th>\n",
              "      <th>PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>125.865988</td>\n",
              "      <td>1993</td>\n",
              "      <td>5</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>11~20</td>\n",
              "      <td>229250.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>12</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2021</td>\n",
              "      <td>10</td>\n",
              "      <td>1~10</td>\n",
              "      <td>215320.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>91.511175</td>\n",
              "      <td>1993</td>\n",
              "      <td>6</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2020</td>\n",
              "      <td>3</td>\n",
              "      <td>21~31</td>\n",
              "      <td>161740.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>13</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2020</td>\n",
              "      <td>5</td>\n",
              "      <td>11~20</td>\n",
              "      <td>199781.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>4</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>2022</td>\n",
              "      <td>6</td>\n",
              "      <td>21~30</td>\n",
              "      <td>219606.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b72d90b-ba73-4d89-ad81-0c13f0c67004')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b72d90b-ba73-4d89-ad81-0c13f0c67004 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b72d90b-ba73-4d89-ad81-0c13f0c67004');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOxlLf6Sh8E6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeY4j7USh6sX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9b536d-b305-4c36-c3a7-a8560b7ce9db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         127\n",
              "1         127\n",
              "2         127\n",
              "3         127\n",
              "4         127\n",
              "         ... \n",
              "329685    204\n",
              "329686    204\n",
              "329687    204\n",
              "329688    204\n",
              "329689    204\n",
              "Name: dong, Length: 329690, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_t = dcc.groupby(\"gu\").count()[\"city\"]\n",
        "gu_dcc_num_dict =  dict(df_t)\n",
        "\n",
        "df = park[[\"dong\", \"gu\"]]\n",
        "df_grouped = df.groupby('dong')['gu'].agg(lambda x: pd.Series.mode(x)[0])\n",
        "\n",
        "# Convert the Series to a dictionary\n",
        "dong_to_gu_dict = df_grouped.to_dict()\n",
        "\n",
        "# dong_to_gu_dict\n",
        "train.dong.map(dong_to_gu_dict).map(gu_dcc_num_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1XtTu5pqsxO",
        "outputId": "5b0cfef7-3827-4706-aebc-1d53bc048bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-d3cacdcaf7b5>:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
            "  df_t = park.groupby(\"dong\").sum()[\"park_area\"]\n"
          ]
        }
      ],
      "source": [
        "df_t = park.groupby(\"dong\").count()[\"city\"]\n",
        "dong_park_num_dict = dict(df_t)\n",
        "df_t = park.groupby(\"dong\").sum()[\"park_area\"]\n",
        "dong_park_area_dict = dict(df_t)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dong_mean_price_dict = dict(train.groupby(\"dong\")[\"PRICE\"].mean())\n",
        "dong_mean_price_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4NgTe_Mtw_v",
        "outputId": "3b4769a7-5e48-48f9-c377-72ca55640725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 593889.9797331163,\n",
              " 1: 266659.3818181818,\n",
              " 2: 232350.8322095672,\n",
              " 3: 341457.218474196,\n",
              " 4: 300138.25625000003,\n",
              " 5: 153720.67265734266,\n",
              " 6: 173445.40027906976,\n",
              " 7: 462363.864957377,\n",
              " 8: 193666.23842004075,\n",
              " 9: 362664.4363955119,\n",
              " 10: 780260.6354357824,\n",
              " 11: 394138.7841149653,\n",
              " 12: 328125.41652551683,\n",
              " 13: 566847.6970996138,\n",
              " 14: 345054.6717525773,\n",
              " 15: 421501.2658227848,\n",
              " 16: 229892.33587827426,\n",
              " 17: 685332.6745768567,\n",
              " 18: 244389.43000000002,\n",
              " 19: 189679.38663303189,\n",
              " 20: 329976.3471208791,\n",
              " 21: 320761.1706772908,\n",
              " 22: 304002.97228397045,\n",
              " 23: 576765.8970494037,\n",
              " 24: 158330.9337363344,\n",
              " 25: 182860.24448398576,\n",
              " 26: 450998.42746268655,\n",
              " 27: 350163.0,\n",
              " 28: 174111.59150585873,\n",
              " 29: 149080.72537313434,\n",
              " 30: 361042.2228327869,\n",
              " 31: 260909.26860683082,\n",
              " 32: 465233.7533229269,\n",
              " 33: 215235.056097561,\n",
              " 34: 189048.52891217257,\n",
              " 35: 408650.1434270123,\n",
              " 36: 487339.59433962265,\n",
              " 37: 800066.8066141732,\n",
              " 38: 476761.4748141026,\n",
              " 39: 434065.18176785717,\n",
              " 40: 365926.45861756377,\n",
              " 42: 810978.6911926606,\n",
              " 44: 181074.31880800944,\n",
              " 46: 186944.18332610838,\n",
              " 47: 205703.88563803682,\n",
              " 48: 484108.1861218638,\n",
              " 49: 627739.8220164384,\n",
              " 50: 487467.76473542594,\n",
              " 51: 266633.1363780069,\n",
              " 53: 201230.264507772,\n",
              " 54: 1205594.2197777778,\n",
              " 55: 437119.1376470589,\n",
              " 56: 149236.42882335177,\n",
              " 57: 165066.35356,\n",
              " 58: 1164275.7936611134,\n",
              " 59: 471698.28782532754,\n",
              " 61: 304111.0499328859,\n",
              " 62: 135123.40762332696,\n",
              " 64: 719677.6398163934,\n",
              " 65: 641528.8220072202,\n",
              " 66: 384205.65542545955,\n",
              " 67: 553507.9204294479,\n",
              " 68: 415090.4497502973,\n",
              " 69: 452124.4760615385,\n",
              " 70: 196943.43888558954,\n",
              " 72: 257856.32578034682,\n",
              " 73: 317353.7412023,\n",
              " 74: 487019.30909090914,\n",
              " 76: 467209.2200963855,\n",
              " 77: 164550.21518749997,\n",
              " 78: 380903.3955452038,\n",
              " 79: 155135.5023649635,\n",
              " 80: 618014.7051020408,\n",
              " 81: 358883.4611702703,\n",
              " 82: 626683.23616,\n",
              " 84: 620842.5325087109,\n",
              " 85: 244538.20712912566,\n",
              " 86: 390857.00736655854,\n",
              " 87: 402314.17433293414,\n",
              " 88: 130911.33219170508,\n",
              " 89: 247576.7322905983,\n",
              " 90: 1478820.5057013389,\n",
              " 91: 901337.8990355515,\n",
              " 92: 868363.1559039999,\n",
              " 93: 320531.44398319325,\n",
              " 94: 289837.06409992656,\n",
              " 95: 319103.2734532872,\n",
              " 96: 211325.99371428572,\n",
              " 97: 299977.22430769226,\n",
              " 98: 523735.1738127786,\n",
              " 99: 246581.17733719246,\n",
              " 100: 268091.27791729657,\n",
              " 101: 516689.102377551,\n",
              " 102: 422217.4968,\n",
              " 103: 432968.9358576349,\n",
              " 104: 333657.71407407406,\n",
              " 105: 508492.5615802838,\n",
              " 106: 316725.3658308457,\n",
              " 107: 1111014.8563063177,\n",
              " 108: 336321.5,\n",
              " 109: 276486.531145135,\n",
              " 111: 587047.5004361604,\n",
              " 112: 501346.9522147239,\n",
              " 113: 183524.9055730337,\n",
              " 114: 279329.08774193545,\n",
              " 115: 133725.50173760933,\n",
              " 116: 901197.3258768985,\n",
              " 117: 665040.9586206897,\n",
              " 118: 807761.7551085714,\n",
              " 119: 701936.676745098,\n",
              " 120: 301312.46220809245,\n",
              " 121: 667261.4456045845,\n",
              " 122: 375975.51936507935,\n",
              " 123: 572110.7841668178,\n",
              " 124: 220005.80291343282,\n",
              " 125: 335733.8487230321,\n",
              " 126: 478002.3285714286,\n",
              " 127: 310818.2986122449,\n",
              " 129: 307235.97583265067,\n",
              " 130: 554480.9801567164,\n",
              " 131: 525863.2927384196,\n",
              " 132: 500060.73106070585,\n",
              " 133: 342187.50627695734,\n",
              " 135: 368785.5318597092,\n",
              " 137: 774479.892,\n",
              " 138: 507952.23114833306,\n",
              " 140: 878411.9788651911,\n",
              " 141: 174709.22051591836,\n",
              " 142: 213962.79330472104,\n",
              " 143: 259553.71526494896,\n",
              " 144: 220562.2938472469,\n",
              " 145: 548094.9126310905,\n",
              " 146: 1641676.6881150855,\n",
              " 147: 601989.69765625,\n",
              " 148: 262117.482139165,\n",
              " 149: 211813.80518477663,\n",
              " 150: 810883.7877294685,\n",
              " 151: 366101.37922580645,\n",
              " 152: 270136.9022377778,\n",
              " 153: 285678.4194117647,\n",
              " 154: 378709.0948837209,\n",
              " 155: 419447.9004984071,\n",
              " 156: 446999.17479833105,\n",
              " 157: 133603.63076923075,\n",
              " 158: 480597.3297660377,\n",
              " 159: 173581.18,\n",
              " 160: 480780.0935483871,\n",
              " 161: 568502.0546583851,\n",
              " 162: 308458.9410738255,\n",
              " 163: 680147.5536974063,\n",
              " 164: 427533.40197530866,\n",
              " 165: 262853.99420146516,\n",
              " 166: 386077.83636363636,\n",
              " 167: 231171.1588433735,\n",
              " 170: 349562.05997792346,\n",
              " 171: 561779.3565652759,\n",
              " 172: 726728.7946238533,\n",
              " 173: 182133.38997142858,\n",
              " 174: 348021.7844444444,\n",
              " 176: 692932.12,\n",
              " 177: 408565.2,\n",
              " 178: 484043.8559907514,\n",
              " 179: 402287.8091633238,\n",
              " 183: 798776.1660408162,\n",
              " 184: 831210.3123999999,\n",
              " 185: 1033743.5567358229,\n",
              " 186: 930471.3374662378,\n",
              " 188: 362281.6110680313,\n",
              " 189: 607149.2310264357,\n",
              " 190: 287211.4675549121,\n",
              " 192: 182440.73003464568,\n",
              " 195: 283214.6219284946,\n",
              " 196: 176942.23286411152,\n",
              " 197: 198805.33537254902,\n",
              " 198: 344252.08600184956,\n",
              " 199: 422762.8187924528,\n",
              " 200: 344499.31923076924,\n",
              " 201: 181960.42915463916,\n",
              " 202: 555725.0823614224,\n",
              " 203: 307411.0094242997,\n",
              " 204: 205777.5550857143,\n",
              " 205: 426915.3557783375,\n",
              " 206: 1072031.174485549,\n",
              " 208: 238811.54933333333,\n",
              " 210: 237978.99484892085,\n",
              " 212: 152473.81176470587,\n",
              " 213: 472408.81537508796,\n",
              " 214: 217583.74972845335,\n",
              " 215: 461514.1495921376,\n",
              " 216: 156750.91881206498,\n",
              " 220: 577169.7586206896,\n",
              " 221: 380842.51111111115,\n",
              " 222: 532114.7189725364,\n",
              " 223: 480388.4388785047,\n",
              " 224: 359570.1947860049,\n",
              " 225: 341537.8517241379,\n",
              " 226: 375797.42015338043,\n",
              " 227: 466900.4736520211,\n",
              " 228: 282536.0606629834,\n",
              " 229: 302009.6509041096,\n",
              " 230: 536505.572845283,\n",
              " 232: 170123.20923076922,\n",
              " 236: 653544.8335088607}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-qxlwPvqsSo"
      },
      "outputs": [],
      "source": [
        "app_id_count_less_then = 10\n",
        "\n",
        "\n",
        "df_ids = train.apartment_id.value_counts()\n",
        "ids = np.array(df_ids[df_ids>app_id_count_less_then].index)\n",
        "def map_app_ids (id):\n",
        "  if id in ids:\n",
        "    return  id\n",
        "  else: \n",
        "    return -1\n",
        "  \n",
        "df_t = train.apartment_id.map(map_app_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQawbgdQ76gi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQkXLplYjztA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load some data\n",
        "# Load some data\n",
        "\n",
        "def get_features_train(df , features_to_del = None):\n",
        "  if features_to_del is None:\n",
        "    features_to_del = []\n",
        "  \n",
        "  X = df.drop(['index'], axis = 1)\n",
        "\n",
        "  X['transaction_day'] = X['transaction_day'].map({'1~10': 1, '11~20': 2, '21~31': 3, \"21~30\":3,\"21~28\":3,\"21~29\":3,})\n",
        "  X['transaction_time'] = X[\"transaction_year\"] + (X[\"transaction_month\"]-1)/12\n",
        "  X['apartment_id'] = X['apartment_id'].map(map_app_ids)\n",
        "\n",
        "  X['dong_mean_price'] = X['dong'].map(dong_mean_price_dict)\n",
        "\n",
        "  X['building_age'] = X['transaction_time']-X['built_year']\n",
        "  X['dong_park_num'] = X['dong'].map(dong_park_num_dict)\n",
        "  X['dong_park_area'] = X['dong'].map(dong_park_area_dict)\n",
        "  X['dong_dcc_num'] = X['dong'].map(dong_to_gu_dict).map(gu_dcc_num_dict)\n",
        "  \n",
        "\n",
        "  split_long_lat = True\n",
        "  if split_long_lat: \n",
        "    is_busan = (X[\"city\"]==\"busan\").astype(np.int32)\n",
        "    X[\"bus_lat\"] = X[\"lat\"]*is_busan\n",
        "    X[\"bus_long\"] = X[\"long\"]*is_busan\n",
        "    X[\"seoul_lat\"] = X[\"lat\"]*(1-is_busan)\n",
        "    X[\"seoul_long\"] = X[\"long\"]*(1-is_busan)\n",
        "\n",
        "    X = X.drop(['lat', 'long'], axis = 1)\n",
        "\n",
        "  X = X.drop(features_to_del, axis = 1)\n",
        "\n",
        "  return X\n",
        "\n",
        "\n",
        "def get_train_val_data( features_to_del = None, test_size=0.3, random_state=42):\n",
        "\n",
        "  df = train.copy()\n",
        "  y = df[\"PRICE\"]\n",
        "\n",
        "  adjust_to_inflation = False\n",
        "  \n",
        "  if adjust_to_inflation:\n",
        "    y = y * train[\"transaction_year\"].map(inflation_adjustion_dick)\n",
        "\n",
        "  df = df.drop('PRICE', axis = 1)\n",
        "  X = get_features_train(df, features_to_del)\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "  return X_train, X_valid, y_train, y_valid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw1XODNconrP"
      },
      "source": [
        "# train "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zezTvBAOsrZm"
      },
      "outputs": [],
      "source": [
        "# import optuna\n",
        "from catboost import CatBoostRegressor\n",
        "# from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XcpuG98oni8"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = get_train_val_data( test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW4UXeuTDVvN",
        "outputId": "35b31607-fb93-417b-8064-23c5e777c345"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1       2808\n",
              " 368      979\n",
              " 2707     872\n",
              " 4384     733\n",
              " 3966     710\n",
              "         ... \n",
              " 1899       5\n",
              " 2393       5\n",
              " 62         5\n",
              " 4242       3\n",
              " 2192       2\n",
              "Name: apartment_id, Length: 3857, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "X_train.apartment_id.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "j-CKZB9Tn0Bz",
        "outputId": "e4b2bfc2-948a-474e-f60c-2c1553fde671"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        apartment_id   city  dong  house_area  built_year  floor  \\\n",
              "266915          3610  seoul   179  100.965476        2011      6   \n",
              "315915          4254  busan   170  101.623270        1991      8   \n",
              "124041          1640  seoul   146   98.346258        1977     10   \n",
              "77457            988  seoul   198   53.245475        1992     13   \n",
              "255858          3463  seoul    61  106.993265        1991      8   \n",
              "...              ...    ...   ...         ...         ...    ...   \n",
              "119879          1584  busan   152   71.713474        2007      7   \n",
              "259178          3523  busan    78  121.943140        2008     10   \n",
              "131932          1753  busan    56   75.598528        1980      4   \n",
              "146867          1971  busan   195   71.436483        1996      1   \n",
              "121958          1626  busan   195   71.472362        1999     16   \n",
              "\n",
              "        transaction_year  transaction_month  transaction_day  \\\n",
              "266915              2022                 11                2   \n",
              "315915              2020                  2                2   \n",
              "124041              2022                  8                2   \n",
              "77457               2020                  4                2   \n",
              "255858              2021                  1                1   \n",
              "...                  ...                ...              ...   \n",
              "119879              2021                  4                1   \n",
              "259178              2021                  5                2   \n",
              "131932              2021                  5                2   \n",
              "146867              2022                  8                1   \n",
              "121958              2022                  4                1   \n",
              "\n",
              "        transaction_time  dong_mean_price  building_age  dong_park_num  \\\n",
              "266915       2022.833333     4.022878e+05     11.833333              9   \n",
              "315915       2020.083333     3.495621e+05     29.083333              6   \n",
              "124041       2022.583333     1.641677e+06     45.583333              4   \n",
              "77457        2020.250000     3.442521e+05     28.250000              1   \n",
              "255858       2021.000000     3.041110e+05     30.000000              4   \n",
              "...                  ...              ...           ...            ...   \n",
              "119879       2021.250000     2.701369e+05     14.250000             17   \n",
              "259178       2021.333333     3.809034e+05     13.333333             25   \n",
              "131932       2021.333333     1.492364e+05     41.333333             12   \n",
              "146867       2022.583333     2.832146e+05     26.583333             13   \n",
              "121958       2022.250000     2.832146e+05     23.250000             13   \n",
              "\n",
              "        dong_park_area  dong_dcc_num    bus_lat    bus_long  seoul_lat  \\\n",
              "266915      802.008495           283   0.000000    0.000000  37.592657   \n",
              "315915     1791.434934           143  35.119025  129.112653   0.000000   \n",
              "124041      265.890116           223   0.000000    0.000000  37.532109   \n",
              "77457      2309.603213           457   0.000000    0.000000  37.648020   \n",
              "255858      478.596386           168   0.000000    0.000000  37.468399   \n",
              "...                ...           ...        ...         ...        ...   \n",
              "119879      756.124304            97  35.183442  129.089934   0.000000   \n",
              "259178     1941.754346           997  35.107136  128.917942   0.000000   \n",
              "131932      936.064901           188  35.212214  129.011554   0.000000   \n",
              "146867      705.142308           223  35.178180  129.175997   0.000000   \n",
              "121958      705.142308           223  35.178180  129.175997   0.000000   \n",
              "\n",
              "        seoul_long  \n",
              "266915  126.921611  \n",
              "315915    0.000000  \n",
              "124041  127.030016  \n",
              "77457   127.076780  \n",
              "255858  126.897957  \n",
              "...            ...  \n",
              "119879    0.000000  \n",
              "259178    0.000000  \n",
              "131932    0.000000  \n",
              "146867    0.000000  \n",
              "121958    0.000000  \n",
              "\n",
              "[230783 rows x 19 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6f9bef2-e982-4b10-a696-af36d29c2859\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>apartment_id</th>\n",
              "      <th>city</th>\n",
              "      <th>dong</th>\n",
              "      <th>house_area</th>\n",
              "      <th>built_year</th>\n",
              "      <th>floor</th>\n",
              "      <th>transaction_year</th>\n",
              "      <th>transaction_month</th>\n",
              "      <th>transaction_day</th>\n",
              "      <th>transaction_time</th>\n",
              "      <th>dong_mean_price</th>\n",
              "      <th>building_age</th>\n",
              "      <th>dong_park_num</th>\n",
              "      <th>dong_park_area</th>\n",
              "      <th>dong_dcc_num</th>\n",
              "      <th>bus_lat</th>\n",
              "      <th>bus_long</th>\n",
              "      <th>seoul_lat</th>\n",
              "      <th>seoul_long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>266915</th>\n",
              "      <td>3610</td>\n",
              "      <td>seoul</td>\n",
              "      <td>179</td>\n",
              "      <td>100.965476</td>\n",
              "      <td>2011</td>\n",
              "      <td>6</td>\n",
              "      <td>2022</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>2022.833333</td>\n",
              "      <td>4.022878e+05</td>\n",
              "      <td>11.833333</td>\n",
              "      <td>9</td>\n",
              "      <td>802.008495</td>\n",
              "      <td>283</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.592657</td>\n",
              "      <td>126.921611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315915</th>\n",
              "      <td>4254</td>\n",
              "      <td>busan</td>\n",
              "      <td>170</td>\n",
              "      <td>101.623270</td>\n",
              "      <td>1991</td>\n",
              "      <td>8</td>\n",
              "      <td>2020</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2020.083333</td>\n",
              "      <td>3.495621e+05</td>\n",
              "      <td>29.083333</td>\n",
              "      <td>6</td>\n",
              "      <td>1791.434934</td>\n",
              "      <td>143</td>\n",
              "      <td>35.119025</td>\n",
              "      <td>129.112653</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124041</th>\n",
              "      <td>1640</td>\n",
              "      <td>seoul</td>\n",
              "      <td>146</td>\n",
              "      <td>98.346258</td>\n",
              "      <td>1977</td>\n",
              "      <td>10</td>\n",
              "      <td>2022</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2022.583333</td>\n",
              "      <td>1.641677e+06</td>\n",
              "      <td>45.583333</td>\n",
              "      <td>4</td>\n",
              "      <td>265.890116</td>\n",
              "      <td>223</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.532109</td>\n",
              "      <td>127.030016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77457</th>\n",
              "      <td>988</td>\n",
              "      <td>seoul</td>\n",
              "      <td>198</td>\n",
              "      <td>53.245475</td>\n",
              "      <td>1992</td>\n",
              "      <td>13</td>\n",
              "      <td>2020</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2020.250000</td>\n",
              "      <td>3.442521e+05</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>1</td>\n",
              "      <td>2309.603213</td>\n",
              "      <td>457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.648020</td>\n",
              "      <td>127.076780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255858</th>\n",
              "      <td>3463</td>\n",
              "      <td>seoul</td>\n",
              "      <td>61</td>\n",
              "      <td>106.993265</td>\n",
              "      <td>1991</td>\n",
              "      <td>8</td>\n",
              "      <td>2021</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2021.000000</td>\n",
              "      <td>3.041110e+05</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>478.596386</td>\n",
              "      <td>168</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.468399</td>\n",
              "      <td>126.897957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119879</th>\n",
              "      <td>1584</td>\n",
              "      <td>busan</td>\n",
              "      <td>152</td>\n",
              "      <td>71.713474</td>\n",
              "      <td>2007</td>\n",
              "      <td>7</td>\n",
              "      <td>2021</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2021.250000</td>\n",
              "      <td>2.701369e+05</td>\n",
              "      <td>14.250000</td>\n",
              "      <td>17</td>\n",
              "      <td>756.124304</td>\n",
              "      <td>97</td>\n",
              "      <td>35.183442</td>\n",
              "      <td>129.089934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259178</th>\n",
              "      <td>3523</td>\n",
              "      <td>busan</td>\n",
              "      <td>78</td>\n",
              "      <td>121.943140</td>\n",
              "      <td>2008</td>\n",
              "      <td>10</td>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2021.333333</td>\n",
              "      <td>3.809034e+05</td>\n",
              "      <td>13.333333</td>\n",
              "      <td>25</td>\n",
              "      <td>1941.754346</td>\n",
              "      <td>997</td>\n",
              "      <td>35.107136</td>\n",
              "      <td>128.917942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131932</th>\n",
              "      <td>1753</td>\n",
              "      <td>busan</td>\n",
              "      <td>56</td>\n",
              "      <td>75.598528</td>\n",
              "      <td>1980</td>\n",
              "      <td>4</td>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2021.333333</td>\n",
              "      <td>1.492364e+05</td>\n",
              "      <td>41.333333</td>\n",
              "      <td>12</td>\n",
              "      <td>936.064901</td>\n",
              "      <td>188</td>\n",
              "      <td>35.212214</td>\n",
              "      <td>129.011554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146867</th>\n",
              "      <td>1971</td>\n",
              "      <td>busan</td>\n",
              "      <td>195</td>\n",
              "      <td>71.436483</td>\n",
              "      <td>1996</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2022.583333</td>\n",
              "      <td>2.832146e+05</td>\n",
              "      <td>26.583333</td>\n",
              "      <td>13</td>\n",
              "      <td>705.142308</td>\n",
              "      <td>223</td>\n",
              "      <td>35.178180</td>\n",
              "      <td>129.175997</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121958</th>\n",
              "      <td>1626</td>\n",
              "      <td>busan</td>\n",
              "      <td>195</td>\n",
              "      <td>71.472362</td>\n",
              "      <td>1999</td>\n",
              "      <td>16</td>\n",
              "      <td>2022</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2022.250000</td>\n",
              "      <td>2.832146e+05</td>\n",
              "      <td>23.250000</td>\n",
              "      <td>13</td>\n",
              "      <td>705.142308</td>\n",
              "      <td>223</td>\n",
              "      <td>35.178180</td>\n",
              "      <td>129.175997</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230783 rows × 19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6f9bef2-e982-4b10-a696-af36d29c2859')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6f9bef2-e982-4b10-a696-af36d29c2859 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6f9bef2-e982-4b10-a696-af36d29c2859');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "# X_train, X_valid, y_train, y_valid = get_train_val_data( test_size=0.3, random_state=42)\n",
        "cat_features = [\"city\",\t\"dong\", \"apartment_id\"]\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5oKEn5rm8-e",
        "outputId": "38733d94-127b-4434-85ba-2fac4b1b9893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because MAE is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 199264.1179983\ttest: 198886.1714540\tbest: 198886.1714540 (0)\ttotal: 124ms\tremaining: 3m 6s\n",
            "1:\ttotal: 230ms\tremaining: 2m 52s\n",
            "2:\ttotal: 310ms\tremaining: 2m 34s\n",
            "3:\ttotal: 380ms\tremaining: 2m 22s\n",
            "4:\ttotal: 440ms\tremaining: 2m 11s\n",
            "5:\tlearn: 128792.7577335\ttest: 127625.7953835\tbest: 127625.7953835 (5)\ttotal: 512ms\tremaining: 2m 7s\n",
            "6:\ttotal: 570ms\tremaining: 2m 1s\n",
            "7:\ttotal: 632ms\tremaining: 1m 57s\n",
            "8:\ttotal: 698ms\tremaining: 1m 55s\n",
            "9:\ttotal: 746ms\tremaining: 1m 51s\n",
            "10:\tlearn: 88701.3050355\ttest: 87481.0963835\tbest: 87481.0963835 (10)\ttotal: 806ms\tremaining: 1m 49s\n",
            "11:\ttotal: 873ms\tremaining: 1m 48s\n",
            "12:\ttotal: 933ms\tremaining: 1m 46s\n",
            "13:\ttotal: 997ms\tremaining: 1m 45s\n",
            "14:\ttotal: 1.06s\tremaining: 1m 45s\n",
            "15:\tlearn: 67880.1787653\ttest: 66725.8990365\tbest: 66725.8990365 (15)\ttotal: 1.08s\tremaining: 1m 40s\n",
            "16:\ttotal: 1.15s\tremaining: 1m 39s\n",
            "17:\ttotal: 1.23s\tremaining: 1m 41s\n",
            "18:\ttotal: 1.29s\tremaining: 1m 40s\n",
            "19:\ttotal: 1.35s\tremaining: 1m 40s\n",
            "20:\tlearn: 55997.9486877\ttest: 54986.8908773\tbest: 54986.8908773 (20)\ttotal: 1.42s\tremaining: 1m 40s\n",
            "21:\ttotal: 1.48s\tremaining: 1m 39s\n",
            "22:\ttotal: 1.54s\tremaining: 1m 39s\n",
            "23:\ttotal: 1.61s\tremaining: 1m 38s\n",
            "24:\ttotal: 1.67s\tremaining: 1m 38s\n",
            "25:\tlearn: 49799.3594329\ttest: 48970.4521621\tbest: 48970.4521621 (25)\ttotal: 1.7s\tremaining: 1m 36s\n",
            "26:\ttotal: 1.76s\tremaining: 1m 36s\n",
            "27:\ttotal: 1.83s\tremaining: 1m 36s\n",
            "28:\ttotal: 1.9s\tremaining: 1m 36s\n",
            "29:\ttotal: 1.97s\tremaining: 1m 36s\n",
            "30:\tlearn: 46431.7175529\ttest: 45738.5941945\tbest: 45738.5941945 (30)\ttotal: 2.02s\tremaining: 1m 35s\n",
            "31:\ttotal: 2.08s\tremaining: 1m 35s\n",
            "32:\ttotal: 2.15s\tremaining: 1m 35s\n",
            "33:\ttotal: 2.22s\tremaining: 1m 35s\n",
            "34:\ttotal: 2.29s\tremaining: 1m 35s\n",
            "35:\tlearn: 43792.3672368\ttest: 43261.7093633\tbest: 43261.7093633 (35)\ttotal: 2.36s\tremaining: 1m 35s\n",
            "36:\ttotal: 2.42s\tremaining: 1m 35s\n",
            "37:\ttotal: 2.49s\tremaining: 1m 35s\n",
            "38:\ttotal: 2.55s\tremaining: 1m 35s\n",
            "39:\ttotal: 2.61s\tremaining: 1m 35s\n",
            "40:\tlearn: 41878.9568382\ttest: 41446.4613425\tbest: 41446.4613425 (40)\ttotal: 2.68s\tremaining: 1m 35s\n",
            "41:\ttotal: 2.75s\tremaining: 1m 35s\n",
            "42:\ttotal: 2.81s\tremaining: 1m 35s\n",
            "43:\ttotal: 2.88s\tremaining: 1m 35s\n",
            "44:\ttotal: 2.94s\tremaining: 1m 34s\n",
            "45:\tlearn: 40158.5056785\ttest: 39750.1478358\tbest: 39750.1478358 (45)\ttotal: 3.01s\tremaining: 1m 35s\n",
            "46:\ttotal: 3.06s\tremaining: 1m 34s\n",
            "47:\ttotal: 3.14s\tremaining: 1m 34s\n",
            "48:\ttotal: 3.2s\tremaining: 1m 34s\n",
            "49:\ttotal: 3.27s\tremaining: 1m 34s\n",
            "50:\tlearn: 38968.7944779\ttest: 38605.0598643\tbest: 38605.0598643 (50)\ttotal: 3.33s\tremaining: 1m 34s\n",
            "51:\ttotal: 3.4s\tremaining: 1m 34s\n",
            "52:\ttotal: 3.47s\tremaining: 1m 34s\n",
            "53:\ttotal: 3.53s\tremaining: 1m 34s\n",
            "54:\ttotal: 3.59s\tremaining: 1m 34s\n",
            "55:\tlearn: 37880.7986377\ttest: 37579.4033587\tbest: 37579.4033587 (55)\ttotal: 3.65s\tremaining: 1m 34s\n",
            "56:\ttotal: 3.71s\tremaining: 1m 33s\n",
            "57:\ttotal: 3.78s\tremaining: 1m 34s\n",
            "58:\ttotal: 3.85s\tremaining: 1m 34s\n",
            "59:\ttotal: 3.91s\tremaining: 1m 33s\n",
            "60:\tlearn: 37082.2838077\ttest: 36852.0679831\tbest: 36852.0679831 (60)\ttotal: 4.02s\tremaining: 1m 34s\n",
            "61:\ttotal: 4.09s\tremaining: 1m 34s\n",
            "62:\ttotal: 4.1s\tremaining: 1m 33s\n",
            "63:\ttotal: 4.17s\tremaining: 1m 33s\n",
            "64:\ttotal: 4.25s\tremaining: 1m 33s\n",
            "65:\tlearn: 36394.2719524\ttest: 36185.8939003\tbest: 36185.8939003 (65)\ttotal: 4.32s\tremaining: 1m 33s\n",
            "66:\ttotal: 4.38s\tremaining: 1m 33s\n",
            "67:\ttotal: 4.45s\tremaining: 1m 33s\n",
            "68:\ttotal: 4.52s\tremaining: 1m 33s\n",
            "69:\ttotal: 4.58s\tremaining: 1m 33s\n",
            "70:\tlearn: 35904.6835859\ttest: 35721.1534876\tbest: 35721.1534876 (70)\ttotal: 4.65s\tremaining: 1m 33s\n",
            "71:\ttotal: 4.72s\tremaining: 1m 33s\n",
            "72:\ttotal: 4.78s\tremaining: 1m 33s\n",
            "73:\ttotal: 4.84s\tremaining: 1m 33s\n",
            "74:\ttotal: 4.92s\tremaining: 1m 33s\n",
            "75:\tlearn: 35042.6898342\ttest: 34883.6708423\tbest: 34883.6708423 (75)\ttotal: 4.99s\tremaining: 1m 33s\n",
            "76:\ttotal: 5.06s\tremaining: 1m 33s\n",
            "77:\ttotal: 5.13s\tremaining: 1m 33s\n",
            "78:\ttotal: 5.2s\tremaining: 1m 33s\n",
            "79:\ttotal: 5.24s\tremaining: 1m 32s\n",
            "80:\tlearn: 34677.1552497\ttest: 34565.4224676\tbest: 34565.4224676 (80)\ttotal: 5.31s\tremaining: 1m 33s\n",
            "81:\ttotal: 5.37s\tremaining: 1m 32s\n",
            "82:\ttotal: 5.44s\tremaining: 1m 32s\n",
            "83:\ttotal: 5.5s\tremaining: 1m 32s\n",
            "84:\ttotal: 5.56s\tremaining: 1m 32s\n",
            "85:\tlearn: 34051.0981831\ttest: 33993.2343717\tbest: 33993.2343717 (85)\ttotal: 5.63s\tremaining: 1m 32s\n",
            "86:\ttotal: 5.7s\tremaining: 1m 32s\n",
            "87:\ttotal: 5.77s\tremaining: 1m 32s\n",
            "88:\ttotal: 5.83s\tremaining: 1m 32s\n",
            "89:\ttotal: 5.89s\tremaining: 1m 32s\n",
            "90:\tlearn: 33634.2555561\ttest: 33610.6592253\tbest: 33610.6592253 (90)\ttotal: 5.97s\tremaining: 1m 32s\n",
            "91:\ttotal: 6.04s\tremaining: 1m 32s\n",
            "92:\ttotal: 6.09s\tremaining: 1m 32s\n",
            "93:\ttotal: 6.15s\tremaining: 1m 32s\n",
            "94:\ttotal: 6.22s\tremaining: 1m 31s\n",
            "95:\tlearn: 32926.3722891\ttest: 32929.3436461\tbest: 32929.3436461 (95)\ttotal: 6.28s\tremaining: 1m 31s\n",
            "96:\ttotal: 6.35s\tremaining: 1m 31s\n",
            "97:\ttotal: 6.49s\tremaining: 1m 32s\n",
            "98:\ttotal: 6.7s\tremaining: 1m 34s\n",
            "99:\ttotal: 6.91s\tremaining: 1m 36s\n",
            "100:\tlearn: 32394.9933921\ttest: 32441.1326195\tbest: 32441.1326195 (100)\ttotal: 7.12s\tremaining: 1m 38s\n",
            "101:\ttotal: 7.34s\tremaining: 1m 40s\n",
            "102:\ttotal: 7.55s\tremaining: 1m 42s\n",
            "103:\ttotal: 7.76s\tremaining: 1m 44s\n",
            "104:\ttotal: 7.97s\tremaining: 1m 45s\n",
            "105:\tlearn: 32053.4189780\ttest: 32140.6632089\tbest: 32140.6632089 (105)\ttotal: 8.19s\tremaining: 1m 47s\n",
            "106:\ttotal: 8.42s\tremaining: 1m 49s\n",
            "107:\ttotal: 8.66s\tremaining: 1m 51s\n",
            "108:\ttotal: 8.94s\tremaining: 1m 54s\n",
            "109:\ttotal: 9.21s\tremaining: 1m 56s\n",
            "110:\tlearn: 31562.0757508\ttest: 31677.3282376\tbest: 31677.3282376 (110)\ttotal: 9.47s\tremaining: 1m 58s\n",
            "111:\ttotal: 9.73s\tremaining: 2m\n",
            "112:\ttotal: 10s\tremaining: 2m 2s\n",
            "113:\ttotal: 10.3s\tremaining: 2m 4s\n",
            "114:\ttotal: 10.4s\tremaining: 2m 4s\n",
            "115:\tlearn: 31162.8261007\ttest: 31328.8279293\tbest: 31328.8279293 (115)\ttotal: 10.4s\tremaining: 2m 4s\n",
            "116:\ttotal: 10.5s\tremaining: 2m 4s\n",
            "117:\ttotal: 10.5s\tremaining: 2m 3s\n",
            "118:\ttotal: 10.6s\tremaining: 2m 3s\n",
            "119:\ttotal: 10.7s\tremaining: 2m 2s\n",
            "120:\tlearn: 30858.6850158\ttest: 31073.7234776\tbest: 31073.7234776 (120)\ttotal: 10.7s\tremaining: 2m 2s\n",
            "121:\ttotal: 10.8s\tremaining: 2m 1s\n",
            "122:\ttotal: 10.9s\tremaining: 2m 1s\n",
            "123:\ttotal: 10.9s\tremaining: 2m 1s\n",
            "124:\ttotal: 11s\tremaining: 2m\n",
            "125:\tlearn: 30576.7625518\ttest: 30828.3458198\tbest: 30828.3458198 (125)\ttotal: 11.1s\tremaining: 2m\n",
            "126:\ttotal: 11.1s\tremaining: 2m\n",
            "127:\ttotal: 11.2s\tremaining: 1m 59s\n",
            "128:\ttotal: 11.2s\tremaining: 1m 59s\n",
            "129:\ttotal: 11.3s\tremaining: 1m 59s\n",
            "130:\tlearn: 30197.4331212\ttest: 30476.0407251\tbest: 30476.0407251 (130)\ttotal: 11.4s\tremaining: 1m 58s\n",
            "131:\ttotal: 11.4s\tremaining: 1m 58s\n",
            "132:\ttotal: 11.5s\tremaining: 1m 58s\n",
            "133:\ttotal: 11.6s\tremaining: 1m 57s\n",
            "134:\ttotal: 11.6s\tremaining: 1m 57s\n",
            "135:\tlearn: 29795.9804665\ttest: 30103.1173122\tbest: 30103.1173122 (135)\ttotal: 11.7s\tremaining: 1m 57s\n",
            "136:\ttotal: 11.8s\tremaining: 1m 57s\n",
            "137:\ttotal: 11.8s\tremaining: 1m 56s\n",
            "138:\ttotal: 11.9s\tremaining: 1m 56s\n",
            "139:\ttotal: 12s\tremaining: 1m 56s\n",
            "140:\tlearn: 29500.7246114\ttest: 29846.2528234\tbest: 29846.2528234 (140)\ttotal: 12s\tremaining: 1m 55s\n",
            "141:\ttotal: 12.1s\tremaining: 1m 55s\n",
            "142:\ttotal: 12.1s\tremaining: 1m 55s\n",
            "143:\ttotal: 12.2s\tremaining: 1m 54s\n",
            "144:\ttotal: 12.3s\tremaining: 1m 54s\n",
            "145:\tlearn: 29084.7250274\ttest: 29459.7486528\tbest: 29459.7486528 (145)\ttotal: 12.3s\tremaining: 1m 54s\n",
            "146:\ttotal: 12.4s\tremaining: 1m 54s\n",
            "147:\ttotal: 12.5s\tremaining: 1m 54s\n",
            "148:\ttotal: 12.6s\tremaining: 1m 53s\n",
            "149:\ttotal: 12.6s\tremaining: 1m 53s\n",
            "150:\tlearn: 28741.5044609\ttest: 29147.4352270\tbest: 29147.4352270 (150)\ttotal: 12.7s\tremaining: 1m 53s\n",
            "151:\ttotal: 12.8s\tremaining: 1m 53s\n",
            "152:\ttotal: 12.8s\tremaining: 1m 52s\n",
            "153:\ttotal: 12.9s\tremaining: 1m 52s\n",
            "154:\ttotal: 12.9s\tremaining: 1m 52s\n",
            "155:\tlearn: 28331.4993565\ttest: 28763.3847554\tbest: 28763.3847554 (155)\ttotal: 13s\tremaining: 1m 52s\n",
            "156:\ttotal: 13.1s\tremaining: 1m 51s\n",
            "157:\ttotal: 13.1s\tremaining: 1m 51s\n",
            "158:\ttotal: 13.2s\tremaining: 1m 51s\n",
            "159:\ttotal: 13.3s\tremaining: 1m 51s\n",
            "160:\tlearn: 28060.8381727\ttest: 28516.1331756\tbest: 28516.1331756 (160)\ttotal: 13.3s\tremaining: 1m 50s\n",
            "161:\ttotal: 13.4s\tremaining: 1m 50s\n",
            "162:\ttotal: 13.5s\tremaining: 1m 50s\n",
            "163:\ttotal: 13.5s\tremaining: 1m 50s\n",
            "164:\ttotal: 13.6s\tremaining: 1m 50s\n",
            "165:\tlearn: 27773.3627174\ttest: 28245.0020322\tbest: 28245.0020322 (165)\ttotal: 13.7s\tremaining: 1m 49s\n",
            "166:\ttotal: 13.7s\tremaining: 1m 49s\n",
            "167:\ttotal: 13.8s\tremaining: 1m 49s\n",
            "168:\ttotal: 13.9s\tremaining: 1m 49s\n",
            "169:\ttotal: 13.9s\tremaining: 1m 48s\n",
            "170:\tlearn: 27553.1975925\ttest: 28050.9889897\tbest: 28050.9889897 (170)\ttotal: 14s\tremaining: 1m 48s\n",
            "171:\ttotal: 14.1s\tremaining: 1m 48s\n",
            "172:\ttotal: 14.1s\tremaining: 1m 48s\n",
            "173:\ttotal: 14.2s\tremaining: 1m 48s\n",
            "174:\ttotal: 14.2s\tremaining: 1m 47s\n",
            "175:\tlearn: 27303.2064927\ttest: 27830.3864438\tbest: 27830.3864438 (175)\ttotal: 14.3s\tremaining: 1m 47s\n",
            "176:\ttotal: 14.4s\tremaining: 1m 47s\n",
            "177:\ttotal: 14.5s\tremaining: 1m 47s\n",
            "178:\ttotal: 14.5s\tremaining: 1m 47s\n",
            "179:\ttotal: 14.6s\tremaining: 1m 47s\n",
            "180:\tlearn: 27062.9902549\ttest: 27624.0816120\tbest: 27624.0816120 (180)\ttotal: 14.7s\tremaining: 1m 46s\n",
            "181:\ttotal: 14.8s\tremaining: 1m 46s\n",
            "182:\ttotal: 14.9s\tremaining: 1m 47s\n",
            "183:\ttotal: 15s\tremaining: 1m 47s\n",
            "184:\ttotal: 15.1s\tremaining: 1m 47s\n",
            "185:\tlearn: 26824.6619898\ttest: 27415.6906185\tbest: 27415.6906185 (185)\ttotal: 15.2s\tremaining: 1m 47s\n",
            "186:\ttotal: 15.2s\tremaining: 1m 46s\n",
            "187:\ttotal: 15.3s\tremaining: 1m 46s\n",
            "188:\ttotal: 15.3s\tremaining: 1m 46s\n",
            "189:\ttotal: 15.4s\tremaining: 1m 46s\n",
            "190:\tlearn: 26576.6254187\ttest: 27199.5295783\tbest: 27199.5295783 (190)\ttotal: 15.5s\tremaining: 1m 46s\n",
            "191:\ttotal: 15.6s\tremaining: 1m 46s\n",
            "192:\ttotal: 15.6s\tremaining: 1m 45s\n",
            "193:\ttotal: 15.7s\tremaining: 1m 45s\n",
            "194:\ttotal: 15.8s\tremaining: 1m 45s\n",
            "195:\tlearn: 26327.0514033\ttest: 26974.1723437\tbest: 26974.1723437 (195)\ttotal: 15.8s\tremaining: 1m 45s\n",
            "196:\ttotal: 15.9s\tremaining: 1m 45s\n",
            "197:\ttotal: 16s\tremaining: 1m 44s\n",
            "198:\ttotal: 16s\tremaining: 1m 44s\n",
            "199:\ttotal: 16.1s\tremaining: 1m 44s\n",
            "200:\tlearn: 26045.7368177\ttest: 26713.0112934\tbest: 26713.0112934 (200)\ttotal: 16.2s\tremaining: 1m 44s\n",
            "201:\ttotal: 16.2s\tremaining: 1m 44s\n",
            "202:\ttotal: 16.3s\tremaining: 1m 44s\n",
            "203:\ttotal: 16.4s\tremaining: 1m 43s\n",
            "204:\ttotal: 16.4s\tremaining: 1m 43s\n",
            "205:\tlearn: 25785.1744019\ttest: 26481.3282781\tbest: 26481.3282781 (205)\ttotal: 16.6s\tremaining: 1m 44s\n",
            "206:\ttotal: 16.7s\tremaining: 1m 44s\n",
            "207:\ttotal: 16.9s\tremaining: 1m 44s\n",
            "208:\ttotal: 17s\tremaining: 1m 45s\n",
            "209:\ttotal: 17.3s\tremaining: 1m 46s\n",
            "210:\tlearn: 25610.3383698\ttest: 26332.9674947\tbest: 26332.9674947 (210)\ttotal: 17.6s\tremaining: 1m 47s\n",
            "211:\ttotal: 17.8s\tremaining: 1m 48s\n",
            "212:\ttotal: 18s\tremaining: 1m 48s\n",
            "213:\ttotal: 18.3s\tremaining: 1m 49s\n",
            "214:\ttotal: 18.4s\tremaining: 1m 49s\n",
            "215:\tlearn: 25487.2607428\ttest: 26229.9431992\tbest: 26229.9431992 (215)\ttotal: 18.6s\tremaining: 1m 50s\n",
            "216:\ttotal: 18.9s\tremaining: 1m 51s\n",
            "217:\ttotal: 19.1s\tremaining: 1m 52s\n",
            "218:\ttotal: 19.3s\tremaining: 1m 52s\n",
            "219:\ttotal: 19.5s\tremaining: 1m 53s\n",
            "220:\tlearn: 25374.6102269\ttest: 26132.5173749\tbest: 26132.5173749 (220)\ttotal: 19.7s\tremaining: 1m 54s\n",
            "221:\ttotal: 19.8s\tremaining: 1m 53s\n",
            "222:\ttotal: 19.9s\tremaining: 1m 53s\n",
            "223:\ttotal: 19.9s\tremaining: 1m 53s\n",
            "224:\ttotal: 20s\tremaining: 1m 53s\n",
            "225:\tlearn: 25108.0577686\ttest: 25887.6185508\tbest: 25887.6185508 (225)\ttotal: 20s\tremaining: 1m 53s\n",
            "226:\ttotal: 20.1s\tremaining: 1m 52s\n",
            "227:\ttotal: 20.2s\tremaining: 1m 52s\n",
            "228:\ttotal: 20.3s\tremaining: 1m 52s\n",
            "229:\ttotal: 20.5s\tremaining: 1m 53s\n",
            "230:\tlearn: 24932.4474680\ttest: 25722.0766983\tbest: 25722.0766983 (230)\ttotal: 20.7s\tremaining: 1m 53s\n",
            "231:\ttotal: 20.9s\tremaining: 1m 54s\n",
            "232:\ttotal: 21.1s\tremaining: 1m 54s\n",
            "233:\ttotal: 21.3s\tremaining: 1m 55s\n",
            "234:\ttotal: 21.5s\tremaining: 1m 55s\n",
            "235:\tlearn: 24767.7967268\ttest: 25575.3905386\tbest: 25575.3905386 (235)\ttotal: 21.7s\tremaining: 1m 56s\n",
            "236:\ttotal: 21.9s\tremaining: 1m 56s\n",
            "237:\ttotal: 22.2s\tremaining: 1m 57s\n",
            "238:\ttotal: 22.4s\tremaining: 1m 58s\n",
            "239:\ttotal: 22.6s\tremaining: 1m 58s\n",
            "240:\tlearn: 24582.4936499\ttest: 25414.8958517\tbest: 25414.8958517 (240)\ttotal: 22.9s\tremaining: 1m 59s\n",
            "241:\ttotal: 23.2s\tremaining: 2m\n",
            "242:\ttotal: 23.4s\tremaining: 2m 1s\n",
            "243:\ttotal: 23.7s\tremaining: 2m 1s\n",
            "244:\ttotal: 23.9s\tremaining: 2m 2s\n",
            "245:\tlearn: 24369.3834641\ttest: 25216.2264754\tbest: 25216.2264754 (245)\ttotal: 24.1s\tremaining: 2m 2s\n",
            "246:\ttotal: 24.1s\tremaining: 2m 2s\n",
            "247:\ttotal: 24.2s\tremaining: 2m 2s\n",
            "248:\ttotal: 24.3s\tremaining: 2m 1s\n",
            "249:\ttotal: 24.3s\tremaining: 2m 1s\n",
            "250:\tlearn: 24263.7590464\ttest: 25125.4499682\tbest: 25125.4499682 (250)\ttotal: 24.4s\tremaining: 2m 1s\n",
            "251:\ttotal: 24.5s\tremaining: 2m 1s\n",
            "252:\ttotal: 24.5s\tremaining: 2m\n",
            "253:\ttotal: 24.6s\tremaining: 2m\n",
            "254:\ttotal: 24.7s\tremaining: 2m\n",
            "255:\tlearn: 24102.0234593\ttest: 24981.1062109\tbest: 24981.1062109 (255)\ttotal: 24.8s\tremaining: 2m\n",
            "256:\ttotal: 24.8s\tremaining: 2m\n",
            "257:\ttotal: 24.9s\tremaining: 1m 59s\n",
            "258:\ttotal: 24.9s\tremaining: 1m 59s\n",
            "259:\ttotal: 25s\tremaining: 1m 59s\n",
            "260:\tlearn: 23915.6577044\ttest: 24832.9783736\tbest: 24832.9783736 (260)\ttotal: 25.1s\tremaining: 1m 59s\n",
            "261:\ttotal: 25.1s\tremaining: 1m 58s\n",
            "262:\ttotal: 25.2s\tremaining: 1m 58s\n",
            "263:\ttotal: 25.3s\tremaining: 1m 58s\n",
            "264:\ttotal: 25.3s\tremaining: 1m 58s\n",
            "265:\tlearn: 23766.8184572\ttest: 24704.6147189\tbest: 24704.6147189 (265)\ttotal: 25.4s\tremaining: 1m 57s\n",
            "266:\ttotal: 25.5s\tremaining: 1m 57s\n",
            "267:\ttotal: 25.5s\tremaining: 1m 57s\n",
            "268:\ttotal: 25.6s\tremaining: 1m 57s\n",
            "269:\ttotal: 25.6s\tremaining: 1m 56s\n",
            "270:\tlearn: 23609.5665279\ttest: 24561.0966261\tbest: 24561.0966261 (270)\ttotal: 25.7s\tremaining: 1m 56s\n",
            "271:\ttotal: 25.8s\tremaining: 1m 56s\n",
            "272:\ttotal: 25.9s\tremaining: 1m 56s\n",
            "273:\ttotal: 25.9s\tremaining: 1m 56s\n",
            "274:\ttotal: 26s\tremaining: 1m 55s\n",
            "275:\tlearn: 23483.1877218\ttest: 24449.3756761\tbest: 24449.3756761 (275)\ttotal: 26s\tremaining: 1m 55s\n",
            "276:\ttotal: 26.1s\tremaining: 1m 55s\n",
            "277:\ttotal: 26.2s\tremaining: 1m 55s\n",
            "278:\ttotal: 26.3s\tremaining: 1m 54s\n",
            "279:\ttotal: 26.3s\tremaining: 1m 54s\n",
            "280:\tlearn: 23403.9616436\ttest: 24392.6196528\tbest: 24392.6196528 (280)\ttotal: 26.4s\tremaining: 1m 54s\n",
            "281:\ttotal: 26.4s\tremaining: 1m 54s\n",
            "282:\ttotal: 26.5s\tremaining: 1m 53s\n",
            "283:\ttotal: 26.6s\tremaining: 1m 53s\n",
            "284:\ttotal: 26.6s\tremaining: 1m 53s\n",
            "285:\tlearn: 23250.7052946\ttest: 24291.3864539\tbest: 24291.3864539 (285)\ttotal: 26.7s\tremaining: 1m 53s\n",
            "286:\ttotal: 26.8s\tremaining: 1m 53s\n",
            "287:\ttotal: 26.8s\tremaining: 1m 52s\n",
            "288:\ttotal: 26.9s\tremaining: 1m 52s\n",
            "289:\ttotal: 27s\tremaining: 1m 52s\n",
            "290:\tlearn: 23140.5084777\ttest: 24212.1278373\tbest: 24212.1278373 (290)\ttotal: 27s\tremaining: 1m 52s\n",
            "291:\ttotal: 27.1s\tremaining: 1m 52s\n",
            "292:\ttotal: 27.2s\tremaining: 1m 51s\n",
            "293:\ttotal: 27.2s\tremaining: 1m 51s\n",
            "294:\ttotal: 27.3s\tremaining: 1m 51s\n",
            "295:\tlearn: 23010.2095215\ttest: 24107.3538577\tbest: 24107.3538577 (295)\ttotal: 27.4s\tremaining: 1m 51s\n",
            "296:\ttotal: 27.4s\tremaining: 1m 51s\n",
            "297:\ttotal: 27.5s\tremaining: 1m 50s\n",
            "298:\ttotal: 27.6s\tremaining: 1m 50s\n",
            "299:\ttotal: 27.6s\tremaining: 1m 50s\n",
            "300:\tlearn: 22845.2836821\ttest: 23956.3090176\tbest: 23956.3090176 (300)\ttotal: 27.7s\tremaining: 1m 50s\n",
            "301:\ttotal: 27.8s\tremaining: 1m 50s\n",
            "302:\ttotal: 27.8s\tremaining: 1m 49s\n",
            "303:\ttotal: 27.9s\tremaining: 1m 49s\n",
            "304:\ttotal: 28s\tremaining: 1m 49s\n",
            "305:\tlearn: 22749.2566437\ttest: 23896.4703408\tbest: 23896.4703408 (305)\ttotal: 28s\tremaining: 1m 49s\n",
            "306:\ttotal: 28.1s\tremaining: 1m 49s\n",
            "307:\ttotal: 28.1s\tremaining: 1m 48s\n",
            "308:\ttotal: 28.2s\tremaining: 1m 48s\n",
            "309:\ttotal: 28.3s\tremaining: 1m 48s\n",
            "310:\tlearn: 22628.1202342\ttest: 23808.9524907\tbest: 23808.9524907 (310)\ttotal: 28.4s\tremaining: 1m 48s\n",
            "311:\ttotal: 28.4s\tremaining: 1m 48s\n",
            "312:\ttotal: 28.5s\tremaining: 1m 48s\n",
            "313:\ttotal: 28.5s\tremaining: 1m 47s\n",
            "314:\ttotal: 28.6s\tremaining: 1m 47s\n",
            "315:\tlearn: 22527.6361604\ttest: 23720.5157977\tbest: 23720.5157977 (315)\ttotal: 28.7s\tremaining: 1m 47s\n",
            "316:\ttotal: 28.7s\tremaining: 1m 47s\n",
            "317:\ttotal: 28.8s\tremaining: 1m 47s\n",
            "318:\ttotal: 28.9s\tremaining: 1m 46s\n",
            "319:\ttotal: 28.9s\tremaining: 1m 46s\n",
            "320:\tlearn: 22401.9783780\ttest: 23626.3175711\tbest: 23626.3175711 (320)\ttotal: 29s\tremaining: 1m 46s\n",
            "321:\ttotal: 29.1s\tremaining: 1m 46s\n",
            "322:\ttotal: 29.1s\tremaining: 1m 46s\n",
            "323:\ttotal: 29.2s\tremaining: 1m 45s\n",
            "324:\ttotal: 29.3s\tremaining: 1m 45s\n",
            "325:\tlearn: 22293.2968200\ttest: 23536.5064960\tbest: 23536.5064960 (325)\ttotal: 29.3s\tremaining: 1m 45s\n",
            "326:\ttotal: 29.4s\tremaining: 1m 45s\n",
            "327:\ttotal: 29.4s\tremaining: 1m 45s\n",
            "328:\ttotal: 29.5s\tremaining: 1m 45s\n",
            "329:\ttotal: 29.6s\tremaining: 1m 44s\n",
            "330:\tlearn: 22190.8848399\ttest: 23455.2212482\tbest: 23455.2212482 (330)\ttotal: 29.6s\tremaining: 1m 44s\n",
            "331:\ttotal: 29.7s\tremaining: 1m 44s\n",
            "332:\ttotal: 29.8s\tremaining: 1m 44s\n",
            "333:\ttotal: 29.9s\tremaining: 1m 44s\n",
            "334:\ttotal: 29.9s\tremaining: 1m 44s\n",
            "335:\tlearn: 22115.2550058\ttest: 23407.8400113\tbest: 23406.9677576 (332)\ttotal: 30s\tremaining: 1m 43s\n",
            "336:\ttotal: 30s\tremaining: 1m 43s\n",
            "337:\ttotal: 30.1s\tremaining: 1m 43s\n",
            "338:\ttotal: 30.2s\tremaining: 1m 43s\n",
            "339:\ttotal: 30.2s\tremaining: 1m 43s\n",
            "340:\tlearn: 21985.4618754\ttest: 23300.7598653\tbest: 23300.7598653 (340)\ttotal: 30.3s\tremaining: 1m 43s\n",
            "341:\ttotal: 30.4s\tremaining: 1m 42s\n",
            "342:\ttotal: 30.4s\tremaining: 1m 42s\n",
            "343:\ttotal: 30.5s\tremaining: 1m 42s\n",
            "344:\ttotal: 30.6s\tremaining: 1m 42s\n",
            "345:\tlearn: 21913.6190794\ttest: 23253.1870949\tbest: 23253.1870949 (345)\ttotal: 30.6s\tremaining: 1m 42s\n",
            "346:\ttotal: 30.7s\tremaining: 1m 41s\n",
            "347:\ttotal: 30.8s\tremaining: 1m 41s\n",
            "348:\ttotal: 30.8s\tremaining: 1m 41s\n",
            "349:\ttotal: 30.9s\tremaining: 1m 41s\n",
            "350:\tlearn: 21773.9201934\ttest: 23141.7767398\tbest: 23141.7767398 (350)\ttotal: 31s\tremaining: 1m 41s\n",
            "351:\ttotal: 31s\tremaining: 1m 41s\n",
            "352:\ttotal: 31.1s\tremaining: 1m 40s\n",
            "353:\ttotal: 31.1s\tremaining: 1m 40s\n",
            "354:\ttotal: 31.2s\tremaining: 1m 40s\n",
            "355:\tlearn: 21681.3851280\ttest: 23079.3497730\tbest: 23079.3497730 (355)\ttotal: 31.3s\tremaining: 1m 40s\n",
            "356:\ttotal: 31.3s\tremaining: 1m 40s\n",
            "357:\ttotal: 31.4s\tremaining: 1m 40s\n",
            "358:\ttotal: 31.5s\tremaining: 1m 40s\n",
            "359:\ttotal: 31.5s\tremaining: 1m 39s\n",
            "360:\tlearn: 21542.6357747\ttest: 22966.6090368\tbest: 22966.6090368 (360)\ttotal: 31.6s\tremaining: 1m 39s\n",
            "361:\ttotal: 31.7s\tremaining: 1m 39s\n",
            "362:\ttotal: 31.7s\tremaining: 1m 39s\n",
            "363:\ttotal: 31.8s\tremaining: 1m 39s\n",
            "364:\ttotal: 31.9s\tremaining: 1m 39s\n",
            "365:\tlearn: 21422.7129035\ttest: 22868.9062655\tbest: 22868.9062655 (365)\ttotal: 31.9s\tremaining: 1m 38s\n",
            "366:\ttotal: 32s\tremaining: 1m 38s\n",
            "367:\ttotal: 32s\tremaining: 1m 38s\n",
            "368:\ttotal: 32.1s\tremaining: 1m 38s\n",
            "369:\ttotal: 32.2s\tremaining: 1m 38s\n",
            "370:\tlearn: 21328.9931407\ttest: 22797.7593497\tbest: 22797.7593497 (370)\ttotal: 32.2s\tremaining: 1m 38s\n",
            "371:\ttotal: 32.3s\tremaining: 1m 37s\n",
            "372:\ttotal: 32.4s\tremaining: 1m 37s\n",
            "373:\ttotal: 32.4s\tremaining: 1m 37s\n",
            "374:\ttotal: 32.5s\tremaining: 1m 37s\n",
            "375:\tlearn: 21267.6639787\ttest: 22771.5787962\tbest: 22771.5787962 (375)\ttotal: 32.5s\tremaining: 1m 37s\n",
            "376:\ttotal: 32.6s\tremaining: 1m 37s\n",
            "377:\ttotal: 32.7s\tremaining: 1m 36s\n",
            "378:\ttotal: 32.7s\tremaining: 1m 36s\n",
            "379:\ttotal: 32.8s\tremaining: 1m 36s\n",
            "380:\tlearn: 21213.7779992\ttest: 22738.3477408\tbest: 22738.3477408 (380)\ttotal: 32.9s\tremaining: 1m 36s\n",
            "381:\ttotal: 32.9s\tremaining: 1m 36s\n",
            "382:\ttotal: 33s\tremaining: 1m 36s\n",
            "383:\ttotal: 33.1s\tremaining: 1m 36s\n",
            "384:\ttotal: 33.1s\tremaining: 1m 35s\n",
            "385:\tlearn: 21141.9352032\ttest: 22690.7257929\tbest: 22690.6119082 (384)\ttotal: 33.2s\tremaining: 1m 35s\n",
            "386:\ttotal: 33.2s\tremaining: 1m 35s\n",
            "387:\ttotal: 33.3s\tremaining: 1m 35s\n",
            "388:\ttotal: 33.4s\tremaining: 1m 35s\n",
            "389:\ttotal: 33.4s\tremaining: 1m 35s\n",
            "390:\tlearn: 21061.8793932\ttest: 22617.9327247\tbest: 22617.9327247 (390)\ttotal: 33.5s\tremaining: 1m 35s\n",
            "391:\ttotal: 33.6s\tremaining: 1m 34s\n",
            "392:\ttotal: 33.6s\tremaining: 1m 34s\n",
            "393:\ttotal: 33.7s\tremaining: 1m 34s\n",
            "394:\ttotal: 33.8s\tremaining: 1m 34s\n",
            "395:\tlearn: 20969.4863140\ttest: 22544.8471797\tbest: 22544.8471797 (395)\ttotal: 33.8s\tremaining: 1m 34s\n",
            "396:\ttotal: 33.9s\tremaining: 1m 34s\n",
            "397:\ttotal: 34s\tremaining: 1m 34s\n",
            "398:\ttotal: 34.1s\tremaining: 1m 34s\n",
            "399:\ttotal: 34.2s\tremaining: 1m 34s\n",
            "400:\tlearn: 20881.3727874\ttest: 22471.5959841\tbest: 22471.5959841 (400)\ttotal: 34.3s\tremaining: 1m 33s\n",
            "401:\ttotal: 34.4s\tremaining: 1m 34s\n",
            "402:\ttotal: 34.6s\tremaining: 1m 34s\n",
            "403:\ttotal: 34.8s\tremaining: 1m 34s\n",
            "404:\ttotal: 35s\tremaining: 1m 34s\n",
            "405:\tlearn: 20819.0164440\ttest: 22422.9723680\tbest: 22422.9723680 (405)\ttotal: 35s\tremaining: 1m 34s\n",
            "406:\ttotal: 35.1s\tremaining: 1m 34s\n",
            "407:\ttotal: 35.2s\tremaining: 1m 34s\n",
            "408:\ttotal: 35.3s\tremaining: 1m 34s\n",
            "409:\ttotal: 35.4s\tremaining: 1m 34s\n",
            "410:\tlearn: 20770.3373645\ttest: 22396.7089893\tbest: 22396.7089893 (410)\ttotal: 35.5s\tremaining: 1m 34s\n",
            "411:\ttotal: 35.7s\tremaining: 1m 34s\n",
            "412:\ttotal: 35.8s\tremaining: 1m 34s\n",
            "413:\ttotal: 36s\tremaining: 1m 34s\n",
            "414:\ttotal: 36.2s\tremaining: 1m 34s\n",
            "415:\tlearn: 20666.2082389\ttest: 22309.4111438\tbest: 22309.4111438 (415)\ttotal: 36.4s\tremaining: 1m 34s\n",
            "416:\ttotal: 36.5s\tremaining: 1m 34s\n",
            "417:\ttotal: 36.7s\tremaining: 1m 34s\n",
            "418:\ttotal: 36.9s\tremaining: 1m 35s\n",
            "419:\ttotal: 37.1s\tremaining: 1m 35s\n",
            "420:\tlearn: 20621.9573539\ttest: 22280.6837130\tbest: 22280.6837130 (420)\ttotal: 37.3s\tremaining: 1m 35s\n",
            "421:\ttotal: 37.5s\tremaining: 1m 35s\n",
            "422:\ttotal: 37.7s\tremaining: 1m 36s\n",
            "423:\ttotal: 37.9s\tremaining: 1m 36s\n",
            "424:\ttotal: 38s\tremaining: 1m 36s\n",
            "425:\tlearn: 20528.3263325\ttest: 22225.8689072\tbest: 22225.8689072 (425)\ttotal: 38.1s\tremaining: 1m 36s\n",
            "426:\ttotal: 38.1s\tremaining: 1m 35s\n",
            "427:\ttotal: 38.2s\tremaining: 1m 35s\n",
            "428:\ttotal: 38.3s\tremaining: 1m 35s\n",
            "429:\ttotal: 38.3s\tremaining: 1m 35s\n",
            "430:\tlearn: 20462.6865584\ttest: 22174.7682975\tbest: 22174.7682975 (430)\ttotal: 38.4s\tremaining: 1m 35s\n",
            "431:\ttotal: 38.5s\tremaining: 1m 35s\n",
            "432:\ttotal: 38.5s\tremaining: 1m 34s\n",
            "433:\ttotal: 38.6s\tremaining: 1m 34s\n",
            "434:\ttotal: 38.7s\tremaining: 1m 34s\n",
            "435:\tlearn: 20421.7124485\ttest: 22146.9105321\tbest: 22146.9105321 (435)\ttotal: 38.7s\tremaining: 1m 34s\n",
            "436:\ttotal: 38.8s\tremaining: 1m 34s\n",
            "437:\ttotal: 38.8s\tremaining: 1m 34s\n",
            "438:\ttotal: 38.9s\tremaining: 1m 34s\n",
            "439:\ttotal: 39s\tremaining: 1m 33s\n",
            "440:\tlearn: 20348.6782995\ttest: 22092.3338490\tbest: 22092.3338490 (440)\ttotal: 39s\tremaining: 1m 33s\n",
            "441:\ttotal: 39.1s\tremaining: 1m 33s\n",
            "442:\ttotal: 39.2s\tremaining: 1m 33s\n",
            "443:\ttotal: 39.2s\tremaining: 1m 33s\n",
            "444:\ttotal: 39.3s\tremaining: 1m 33s\n",
            "445:\tlearn: 20292.2365686\ttest: 22059.4056235\tbest: 22059.4056235 (445)\ttotal: 39.4s\tremaining: 1m 33s\n",
            "446:\ttotal: 39.4s\tremaining: 1m 32s\n",
            "447:\ttotal: 39.5s\tremaining: 1m 32s\n",
            "448:\ttotal: 39.6s\tremaining: 1m 32s\n",
            "449:\ttotal: 39.6s\tremaining: 1m 32s\n",
            "450:\tlearn: 20263.7439326\ttest: 22055.3704793\tbest: 22055.2773009 (447)\ttotal: 39.7s\tremaining: 1m 32s\n",
            "451:\ttotal: 39.8s\tremaining: 1m 32s\n",
            "452:\ttotal: 39.8s\tremaining: 1m 32s\n",
            "453:\ttotal: 39.9s\tremaining: 1m 31s\n",
            "454:\ttotal: 40s\tremaining: 1m 31s\n",
            "455:\tlearn: 20179.0136709\ttest: 21982.0934009\tbest: 21982.0934009 (455)\ttotal: 40s\tremaining: 1m 31s\n",
            "456:\ttotal: 40.1s\tremaining: 1m 31s\n",
            "457:\ttotal: 40.1s\tremaining: 1m 31s\n",
            "458:\ttotal: 40.2s\tremaining: 1m 31s\n",
            "459:\ttotal: 40.3s\tremaining: 1m 31s\n",
            "460:\tlearn: 20148.3424342\ttest: 21966.0900037\tbest: 21966.0900037 (460)\ttotal: 40.3s\tremaining: 1m 30s\n",
            "461:\ttotal: 40.4s\tremaining: 1m 30s\n",
            "462:\ttotal: 40.4s\tremaining: 1m 30s\n",
            "463:\ttotal: 40.5s\tremaining: 1m 30s\n",
            "464:\ttotal: 40.6s\tremaining: 1m 30s\n",
            "465:\tlearn: 20055.4146883\ttest: 21888.8787245\tbest: 21888.8787245 (465)\ttotal: 40.6s\tremaining: 1m 30s\n",
            "466:\ttotal: 40.7s\tremaining: 1m 30s\n",
            "467:\ttotal: 40.8s\tremaining: 1m 29s\n",
            "468:\ttotal: 40.8s\tremaining: 1m 29s\n",
            "469:\ttotal: 40.9s\tremaining: 1m 29s\n",
            "470:\tlearn: 19994.0256258\ttest: 21842.8278686\tbest: 21842.8278686 (470)\ttotal: 41s\tremaining: 1m 29s\n",
            "471:\ttotal: 41s\tremaining: 1m 29s\n",
            "472:\ttotal: 41.1s\tremaining: 1m 29s\n",
            "473:\ttotal: 41.2s\tremaining: 1m 29s\n",
            "474:\ttotal: 41.2s\tremaining: 1m 28s\n",
            "475:\tlearn: 19958.5956331\ttest: 21826.3275198\tbest: 21826.3275198 (475)\ttotal: 41.3s\tremaining: 1m 28s\n",
            "476:\ttotal: 41.4s\tremaining: 1m 28s\n",
            "477:\ttotal: 41.4s\tremaining: 1m 28s\n",
            "478:\ttotal: 41.5s\tremaining: 1m 28s\n",
            "479:\ttotal: 41.6s\tremaining: 1m 28s\n",
            "480:\tlearn: 19876.3678780\ttest: 21761.5245028\tbest: 21761.5245028 (480)\ttotal: 41.6s\tremaining: 1m 28s\n",
            "481:\ttotal: 41.7s\tremaining: 1m 28s\n",
            "482:\ttotal: 41.8s\tremaining: 1m 27s\n",
            "483:\ttotal: 41.8s\tremaining: 1m 27s\n",
            "484:\ttotal: 41.9s\tremaining: 1m 27s\n",
            "485:\tlearn: 19799.4268902\ttest: 21719.1697251\tbest: 21719.1697251 (485)\ttotal: 42s\tremaining: 1m 27s\n",
            "486:\ttotal: 42s\tremaining: 1m 27s\n",
            "487:\ttotal: 42.1s\tremaining: 1m 27s\n",
            "488:\ttotal: 42.2s\tremaining: 1m 27s\n",
            "489:\ttotal: 42.2s\tremaining: 1m 27s\n",
            "490:\tlearn: 19733.2790717\ttest: 21676.6065900\tbest: 21676.6065900 (490)\ttotal: 42.3s\tremaining: 1m 26s\n",
            "491:\ttotal: 42.4s\tremaining: 1m 26s\n",
            "492:\ttotal: 42.4s\tremaining: 1m 26s\n",
            "493:\ttotal: 42.5s\tremaining: 1m 26s\n",
            "494:\ttotal: 42.6s\tremaining: 1m 26s\n",
            "495:\tlearn: 19658.1683746\ttest: 21622.3767377\tbest: 21622.3767377 (495)\ttotal: 42.6s\tremaining: 1m 26s\n",
            "496:\ttotal: 42.7s\tremaining: 1m 26s\n",
            "497:\ttotal: 42.8s\tremaining: 1m 26s\n",
            "498:\ttotal: 42.8s\tremaining: 1m 25s\n",
            "499:\ttotal: 42.9s\tremaining: 1m 25s\n",
            "500:\tlearn: 19604.6351421\ttest: 21591.7339723\tbest: 21591.7339723 (500)\ttotal: 43s\tremaining: 1m 25s\n",
            "501:\ttotal: 43s\tremaining: 1m 25s\n",
            "502:\ttotal: 43.1s\tremaining: 1m 25s\n",
            "503:\ttotal: 43.1s\tremaining: 1m 25s\n",
            "504:\ttotal: 43.2s\tremaining: 1m 25s\n",
            "505:\tlearn: 19526.0590945\ttest: 21534.0267524\tbest: 21534.0267524 (505)\ttotal: 43.3s\tremaining: 1m 25s\n",
            "506:\ttotal: 43.4s\tremaining: 1m 24s\n",
            "507:\ttotal: 43.4s\tremaining: 1m 24s\n",
            "508:\ttotal: 43.5s\tremaining: 1m 24s\n",
            "509:\ttotal: 43.5s\tremaining: 1m 24s\n",
            "510:\tlearn: 19428.3725924\ttest: 21452.2225525\tbest: 21452.2225525 (510)\ttotal: 43.6s\tremaining: 1m 24s\n",
            "511:\ttotal: 43.7s\tremaining: 1m 24s\n",
            "512:\ttotal: 43.7s\tremaining: 1m 24s\n",
            "513:\ttotal: 43.8s\tremaining: 1m 24s\n",
            "514:\ttotal: 43.9s\tremaining: 1m 23s\n",
            "515:\tlearn: 19379.1255682\ttest: 21428.6328773\tbest: 21428.6328773 (515)\ttotal: 43.9s\tremaining: 1m 23s\n",
            "516:\ttotal: 44s\tremaining: 1m 23s\n",
            "517:\ttotal: 44.1s\tremaining: 1m 23s\n",
            "518:\ttotal: 44.1s\tremaining: 1m 23s\n",
            "519:\ttotal: 44.2s\tremaining: 1m 23s\n",
            "520:\tlearn: 19325.7431960\ttest: 21398.3964735\tbest: 21398.3964735 (520)\ttotal: 44.3s\tremaining: 1m 23s\n",
            "521:\ttotal: 44.3s\tremaining: 1m 23s\n",
            "522:\ttotal: 44.4s\tremaining: 1m 22s\n",
            "523:\ttotal: 44.5s\tremaining: 1m 22s\n",
            "524:\ttotal: 44.6s\tremaining: 1m 22s\n",
            "525:\tlearn: 19259.2936568\ttest: 21339.9515909\tbest: 21339.9515909 (525)\ttotal: 44.6s\tremaining: 1m 22s\n",
            "526:\ttotal: 44.7s\tremaining: 1m 22s\n",
            "527:\ttotal: 44.7s\tremaining: 1m 22s\n",
            "528:\ttotal: 44.8s\tremaining: 1m 22s\n",
            "529:\ttotal: 44.9s\tremaining: 1m 22s\n",
            "530:\tlearn: 19172.8573422\ttest: 21263.7691569\tbest: 21263.7691569 (530)\ttotal: 44.9s\tremaining: 1m 21s\n",
            "531:\ttotal: 45s\tremaining: 1m 21s\n",
            "532:\ttotal: 45.1s\tremaining: 1m 21s\n",
            "533:\ttotal: 45.1s\tremaining: 1m 21s\n",
            "534:\ttotal: 45.2s\tremaining: 1m 21s\n",
            "535:\tlearn: 19123.5260136\ttest: 21237.0735337\tbest: 21237.0735337 (535)\ttotal: 45.3s\tremaining: 1m 21s\n",
            "536:\ttotal: 45.3s\tremaining: 1m 21s\n",
            "537:\ttotal: 45.4s\tremaining: 1m 21s\n",
            "538:\ttotal: 45.5s\tremaining: 1m 21s\n",
            "539:\ttotal: 45.5s\tremaining: 1m 20s\n",
            "540:\tlearn: 19062.5606912\ttest: 21188.5314487\tbest: 21188.5314487 (540)\ttotal: 45.6s\tremaining: 1m 20s\n",
            "541:\ttotal: 45.7s\tremaining: 1m 20s\n",
            "542:\ttotal: 45.7s\tremaining: 1m 20s\n",
            "543:\ttotal: 45.8s\tremaining: 1m 20s\n",
            "544:\ttotal: 45.9s\tremaining: 1m 20s\n",
            "545:\tlearn: 19002.8155627\ttest: 21149.8002770\tbest: 21149.8002770 (545)\ttotal: 45.9s\tremaining: 1m 20s\n",
            "546:\ttotal: 46s\tremaining: 1m 20s\n",
            "547:\ttotal: 46s\tremaining: 1m 19s\n",
            "548:\ttotal: 46.1s\tremaining: 1m 19s\n",
            "549:\ttotal: 46.2s\tremaining: 1m 19s\n",
            "550:\tlearn: 18959.5741107\ttest: 21118.4444377\tbest: 21118.4444377 (550)\ttotal: 46.2s\tremaining: 1m 19s\n",
            "551:\ttotal: 46.3s\tremaining: 1m 19s\n",
            "552:\ttotal: 46.4s\tremaining: 1m 19s\n",
            "553:\ttotal: 46.4s\tremaining: 1m 19s\n",
            "554:\ttotal: 46.5s\tremaining: 1m 19s\n",
            "555:\tlearn: 18908.9626879\ttest: 21096.8412751\tbest: 21096.8412751 (555)\ttotal: 46.6s\tremaining: 1m 19s\n",
            "556:\ttotal: 46.6s\tremaining: 1m 18s\n",
            "557:\ttotal: 46.7s\tremaining: 1m 18s\n",
            "558:\ttotal: 46.8s\tremaining: 1m 18s\n",
            "559:\ttotal: 46.8s\tremaining: 1m 18s\n",
            "560:\tlearn: 18866.5931199\ttest: 21074.7981033\tbest: 21074.7981033 (560)\ttotal: 46.9s\tremaining: 1m 18s\n",
            "561:\ttotal: 47s\tremaining: 1m 18s\n",
            "562:\ttotal: 47.1s\tremaining: 1m 18s\n",
            "563:\ttotal: 47.1s\tremaining: 1m 18s\n",
            "564:\ttotal: 47.2s\tremaining: 1m 18s\n",
            "565:\tlearn: 18802.4575120\ttest: 21029.6906791\tbest: 21029.6906791 (565)\ttotal: 47.2s\tremaining: 1m 17s\n",
            "566:\ttotal: 47.3s\tremaining: 1m 17s\n",
            "567:\ttotal: 47.4s\tremaining: 1m 17s\n",
            "568:\ttotal: 47.4s\tremaining: 1m 17s\n",
            "569:\ttotal: 47.5s\tremaining: 1m 17s\n",
            "570:\tlearn: 18767.5067228\ttest: 21013.8076375\tbest: 21013.8076375 (570)\ttotal: 47.6s\tremaining: 1m 17s\n",
            "571:\ttotal: 47.6s\tremaining: 1m 17s\n",
            "572:\ttotal: 47.7s\tremaining: 1m 17s\n",
            "573:\ttotal: 47.8s\tremaining: 1m 17s\n",
            "574:\ttotal: 47.8s\tremaining: 1m 16s\n",
            "575:\tlearn: 18738.3662748\ttest: 21003.2448259\tbest: 21003.2448259 (575)\ttotal: 47.9s\tremaining: 1m 16s\n",
            "576:\ttotal: 48.1s\tremaining: 1m 16s\n",
            "577:\ttotal: 48.3s\tremaining: 1m 17s\n",
            "578:\ttotal: 48.5s\tremaining: 1m 17s\n",
            "579:\ttotal: 48.7s\tremaining: 1m 17s\n",
            "580:\tlearn: 18716.6468241\ttest: 20991.2416310\tbest: 20991.2416310 (580)\ttotal: 48.9s\tremaining: 1m 17s\n",
            "581:\ttotal: 49.2s\tremaining: 1m 17s\n",
            "582:\ttotal: 49.4s\tremaining: 1m 17s\n",
            "583:\ttotal: 49.6s\tremaining: 1m 17s\n",
            "584:\ttotal: 49.9s\tremaining: 1m 17s\n",
            "585:\tlearn: 18647.6992153\ttest: 20942.3216557\tbest: 20942.3216557 (585)\ttotal: 50.1s\tremaining: 1m 18s\n",
            "586:\ttotal: 50.3s\tremaining: 1m 18s\n",
            "587:\ttotal: 50.6s\tremaining: 1m 18s\n",
            "588:\ttotal: 50.9s\tremaining: 1m 18s\n",
            "589:\ttotal: 51.1s\tremaining: 1m 18s\n",
            "590:\tlearn: 18555.4669798\ttest: 20874.0490360\tbest: 20874.0490360 (590)\ttotal: 51.4s\tremaining: 1m 19s\n",
            "591:\ttotal: 51.7s\tremaining: 1m 19s\n",
            "592:\ttotal: 51.8s\tremaining: 1m 19s\n",
            "593:\ttotal: 51.9s\tremaining: 1m 19s\n",
            "594:\ttotal: 51.9s\tremaining: 1m 19s\n",
            "595:\tlearn: 18505.2759692\ttest: 20844.5826483\tbest: 20844.5826483 (595)\ttotal: 52s\tremaining: 1m 18s\n",
            "596:\ttotal: 52.1s\tremaining: 1m 18s\n",
            "597:\ttotal: 52.2s\tremaining: 1m 18s\n",
            "598:\ttotal: 52.2s\tremaining: 1m 18s\n",
            "599:\ttotal: 52.3s\tremaining: 1m 18s\n",
            "600:\tlearn: 18449.3921996\ttest: 20815.9755730\tbest: 20815.9755730 (600)\ttotal: 52.4s\tremaining: 1m 18s\n",
            "601:\ttotal: 52.4s\tremaining: 1m 18s\n",
            "602:\ttotal: 52.5s\tremaining: 1m 18s\n",
            "603:\ttotal: 52.5s\tremaining: 1m 17s\n",
            "604:\ttotal: 52.6s\tremaining: 1m 17s\n",
            "605:\tlearn: 18420.5723299\ttest: 20803.2204798\tbest: 20803.2204798 (605)\ttotal: 52.7s\tremaining: 1m 17s\n",
            "606:\ttotal: 52.7s\tremaining: 1m 17s\n",
            "607:\ttotal: 52.8s\tremaining: 1m 17s\n",
            "608:\ttotal: 52.8s\tremaining: 1m 17s\n",
            "609:\ttotal: 52.9s\tremaining: 1m 17s\n",
            "610:\tlearn: 18369.7268516\ttest: 20767.4723124\tbest: 20767.4723124 (610)\ttotal: 53s\tremaining: 1m 17s\n",
            "611:\ttotal: 53.1s\tremaining: 1m 16s\n",
            "612:\ttotal: 53.1s\tremaining: 1m 16s\n",
            "613:\ttotal: 53.2s\tremaining: 1m 16s\n",
            "614:\ttotal: 53.3s\tremaining: 1m 16s\n",
            "615:\tlearn: 18316.5164159\ttest: 20727.9827717\tbest: 20727.9827717 (615)\ttotal: 53.3s\tremaining: 1m 16s\n",
            "616:\ttotal: 53.4s\tremaining: 1m 16s\n",
            "617:\ttotal: 53.5s\tremaining: 1m 16s\n",
            "618:\ttotal: 53.5s\tremaining: 1m 16s\n",
            "619:\ttotal: 53.6s\tremaining: 1m 16s\n",
            "620:\tlearn: 18261.0807902\ttest: 20694.4475922\tbest: 20694.4475922 (620)\ttotal: 53.6s\tremaining: 1m 15s\n",
            "621:\ttotal: 53.7s\tremaining: 1m 15s\n",
            "622:\ttotal: 53.8s\tremaining: 1m 15s\n",
            "623:\ttotal: 53.8s\tremaining: 1m 15s\n",
            "624:\ttotal: 53.9s\tremaining: 1m 15s\n",
            "625:\tlearn: 18230.4372852\ttest: 20675.4236606\tbest: 20675.4236606 (625)\ttotal: 54s\tremaining: 1m 15s\n",
            "626:\ttotal: 54s\tremaining: 1m 15s\n",
            "627:\ttotal: 54.1s\tremaining: 1m 15s\n",
            "628:\ttotal: 54.2s\tremaining: 1m 15s\n",
            "629:\ttotal: 54.3s\tremaining: 1m 14s\n",
            "630:\tlearn: 18198.4404744\ttest: 20670.7582679\tbest: 20670.7582679 (630)\ttotal: 54.3s\tremaining: 1m 14s\n",
            "631:\ttotal: 54.4s\tremaining: 1m 14s\n",
            "632:\ttotal: 54.4s\tremaining: 1m 14s\n",
            "633:\ttotal: 54.5s\tremaining: 1m 14s\n",
            "634:\ttotal: 54.6s\tremaining: 1m 14s\n",
            "635:\tlearn: 18143.2000797\ttest: 20637.5582719\tbest: 20637.4133277 (634)\ttotal: 54.6s\tremaining: 1m 14s\n",
            "636:\ttotal: 54.7s\tremaining: 1m 14s\n",
            "637:\ttotal: 54.8s\tremaining: 1m 13s\n",
            "638:\ttotal: 54.8s\tremaining: 1m 13s\n",
            "639:\ttotal: 54.9s\tremaining: 1m 13s\n",
            "640:\tlearn: 18108.1782974\ttest: 20625.8812015\tbest: 20625.8812015 (640)\ttotal: 55s\tremaining: 1m 13s\n",
            "641:\ttotal: 55s\tremaining: 1m 13s\n",
            "642:\ttotal: 55.1s\tremaining: 1m 13s\n",
            "643:\ttotal: 55.2s\tremaining: 1m 13s\n",
            "644:\ttotal: 55.2s\tremaining: 1m 13s\n",
            "645:\tlearn: 18073.5347751\ttest: 20615.6406321\tbest: 20615.6406321 (645)\ttotal: 55.3s\tremaining: 1m 13s\n",
            "646:\ttotal: 55.4s\tremaining: 1m 13s\n",
            "647:\ttotal: 55.4s\tremaining: 1m 12s\n",
            "648:\ttotal: 55.5s\tremaining: 1m 12s\n",
            "649:\ttotal: 55.6s\tremaining: 1m 12s\n",
            "650:\tlearn: 18033.5789725\ttest: 20594.7013659\tbest: 20594.7013659 (650)\ttotal: 55.6s\tremaining: 1m 12s\n",
            "651:\ttotal: 55.7s\tremaining: 1m 12s\n",
            "652:\ttotal: 55.8s\tremaining: 1m 12s\n",
            "653:\ttotal: 55.8s\tremaining: 1m 12s\n",
            "654:\ttotal: 55.9s\tremaining: 1m 12s\n",
            "655:\tlearn: 17985.7440453\ttest: 20564.7600271\tbest: 20564.7600271 (655)\ttotal: 56s\tremaining: 1m 11s\n",
            "656:\ttotal: 56s\tremaining: 1m 11s\n",
            "657:\ttotal: 56.1s\tremaining: 1m 11s\n",
            "658:\ttotal: 56.2s\tremaining: 1m 11s\n",
            "659:\ttotal: 56.2s\tremaining: 1m 11s\n",
            "660:\tlearn: 17928.8053626\ttest: 20532.6613485\tbest: 20532.6613485 (660)\ttotal: 56.3s\tremaining: 1m 11s\n",
            "661:\ttotal: 56.4s\tremaining: 1m 11s\n",
            "662:\ttotal: 56.4s\tremaining: 1m 11s\n",
            "663:\ttotal: 56.5s\tremaining: 1m 11s\n",
            "664:\ttotal: 56.5s\tremaining: 1m 10s\n",
            "665:\tlearn: 17860.5643570\ttest: 20484.3897399\tbest: 20484.3897399 (665)\ttotal: 56.6s\tremaining: 1m 10s\n",
            "666:\ttotal: 56.7s\tremaining: 1m 10s\n",
            "667:\ttotal: 56.7s\tremaining: 1m 10s\n",
            "668:\ttotal: 56.8s\tremaining: 1m 10s\n",
            "669:\ttotal: 56.9s\tremaining: 1m 10s\n",
            "670:\tlearn: 17814.1304342\ttest: 20455.1666515\tbest: 20455.1666515 (670)\ttotal: 56.9s\tremaining: 1m 10s\n",
            "671:\ttotal: 57s\tremaining: 1m 10s\n",
            "672:\ttotal: 57.1s\tremaining: 1m 10s\n",
            "673:\ttotal: 57.1s\tremaining: 1m 10s\n",
            "674:\ttotal: 57.2s\tremaining: 1m 9s\n",
            "675:\tlearn: 17777.1996031\ttest: 20431.2547342\tbest: 20431.2547342 (675)\ttotal: 57.3s\tremaining: 1m 9s\n",
            "676:\ttotal: 57.4s\tremaining: 1m 9s\n",
            "677:\ttotal: 57.4s\tremaining: 1m 9s\n",
            "678:\ttotal: 57.5s\tremaining: 1m 9s\n",
            "679:\ttotal: 57.6s\tremaining: 1m 9s\n",
            "680:\tlearn: 17736.5660382\ttest: 20414.1577846\tbest: 20414.1577846 (680)\ttotal: 57.6s\tremaining: 1m 9s\n",
            "681:\ttotal: 57.7s\tremaining: 1m 9s\n",
            "682:\ttotal: 57.8s\tremaining: 1m 9s\n",
            "683:\ttotal: 57.8s\tremaining: 1m 9s\n",
            "684:\ttotal: 57.9s\tremaining: 1m 8s\n",
            "685:\tlearn: 17687.0483528\ttest: 20383.4101934\tbest: 20383.4101934 (685)\ttotal: 58s\tremaining: 1m 8s\n",
            "686:\ttotal: 58s\tremaining: 1m 8s\n",
            "687:\ttotal: 58.1s\tremaining: 1m 8s\n",
            "688:\ttotal: 58.2s\tremaining: 1m 8s\n",
            "689:\ttotal: 58.3s\tremaining: 1m 8s\n",
            "690:\tlearn: 17643.1402313\ttest: 20351.7295237\tbest: 20351.5897560 (689)\ttotal: 58.3s\tremaining: 1m 8s\n",
            "691:\ttotal: 58.4s\tremaining: 1m 8s\n",
            "692:\ttotal: 58.4s\tremaining: 1m 8s\n",
            "693:\ttotal: 58.5s\tremaining: 1m 7s\n",
            "694:\ttotal: 58.6s\tremaining: 1m 7s\n",
            "695:\tlearn: 17614.9116009\ttest: 20343.4340542\tbest: 20343.1338126 (694)\ttotal: 58.6s\tremaining: 1m 7s\n",
            "696:\ttotal: 58.7s\tremaining: 1m 7s\n",
            "697:\ttotal: 58.8s\tremaining: 1m 7s\n",
            "698:\ttotal: 58.8s\tremaining: 1m 7s\n",
            "699:\ttotal: 58.9s\tremaining: 1m 7s\n",
            "700:\tlearn: 17561.5602882\ttest: 20307.4451758\tbest: 20307.4451758 (700)\ttotal: 59s\tremaining: 1m 7s\n",
            "701:\ttotal: 59s\tremaining: 1m 7s\n",
            "702:\ttotal: 59.1s\tremaining: 1m 7s\n",
            "703:\ttotal: 59.2s\tremaining: 1m 6s\n",
            "704:\ttotal: 59.2s\tremaining: 1m 6s\n",
            "705:\tlearn: 17498.4818119\ttest: 20270.8480492\tbest: 20270.8480492 (705)\ttotal: 59.3s\tremaining: 1m 6s\n",
            "706:\ttotal: 59.4s\tremaining: 1m 6s\n",
            "707:\ttotal: 59.4s\tremaining: 1m 6s\n",
            "708:\ttotal: 59.5s\tremaining: 1m 6s\n",
            "709:\ttotal: 59.6s\tremaining: 1m 6s\n",
            "710:\tlearn: 17454.1166724\ttest: 20241.5240175\tbest: 20241.5240175 (710)\ttotal: 59.6s\tremaining: 1m 6s\n",
            "711:\ttotal: 59.7s\tremaining: 1m 6s\n",
            "712:\ttotal: 59.8s\tremaining: 1m 5s\n",
            "713:\ttotal: 59.8s\tremaining: 1m 5s\n",
            "714:\ttotal: 59.9s\tremaining: 1m 5s\n",
            "715:\tlearn: 17416.4004801\ttest: 20224.8968425\tbest: 20224.8968425 (715)\ttotal: 60s\tremaining: 1m 5s\n",
            "716:\ttotal: 1m\tremaining: 1m 5s\n",
            "717:\ttotal: 1m\tremaining: 1m 5s\n",
            "718:\ttotal: 1m\tremaining: 1m 5s\n",
            "719:\ttotal: 1m\tremaining: 1m 5s\n",
            "720:\tlearn: 17356.9559630\ttest: 20185.0319998\tbest: 20185.0319998 (720)\ttotal: 1m\tremaining: 1m 5s\n",
            "721:\ttotal: 1m\tremaining: 1m 5s\n",
            "722:\ttotal: 1m\tremaining: 1m 4s\n",
            "723:\ttotal: 1m\tremaining: 1m 4s\n",
            "724:\ttotal: 1m\tremaining: 1m 4s\n",
            "725:\tlearn: 17310.1349060\ttest: 20165.6120598\tbest: 20165.6120598 (725)\ttotal: 1m\tremaining: 1m 4s\n",
            "726:\ttotal: 1m\tremaining: 1m 4s\n",
            "727:\ttotal: 1m\tremaining: 1m 4s\n",
            "728:\ttotal: 1m\tremaining: 1m 4s\n",
            "729:\ttotal: 1m\tremaining: 1m 4s\n",
            "730:\tlearn: 17274.6117348\ttest: 20138.9837322\tbest: 20138.9837322 (730)\ttotal: 1m\tremaining: 1m 4s\n",
            "731:\ttotal: 1m\tremaining: 1m 3s\n",
            "732:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "733:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "734:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "735:\tlearn: 17235.3126183\ttest: 20113.2742475\tbest: 20113.2742475 (735)\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "736:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "737:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "738:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "739:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "740:\tlearn: 17208.6757863\ttest: 20106.7245797\tbest: 20106.7245797 (740)\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "741:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "742:\ttotal: 1m 1s\tremaining: 1m 3s\n",
            "743:\ttotal: 1m 2s\tremaining: 1m 3s\n",
            "744:\ttotal: 1m 2s\tremaining: 1m 3s\n",
            "745:\tlearn: 17146.3360819\ttest: 20071.5587774\tbest: 20071.5587774 (745)\ttotal: 1m 2s\tremaining: 1m 3s\n",
            "746:\ttotal: 1m 2s\tremaining: 1m 3s\n",
            "747:\ttotal: 1m 2s\tremaining: 1m 3s\n",
            "748:\ttotal: 1m 2s\tremaining: 1m 3s\n",
            "749:\ttotal: 1m 3s\tremaining: 1m 3s\n",
            "750:\tlearn: 17084.3302670\ttest: 20028.6371642\tbest: 20028.6371642 (750)\ttotal: 1m 3s\tremaining: 1m 3s\n",
            "751:\ttotal: 1m 3s\tremaining: 1m 3s\n",
            "752:\ttotal: 1m 3s\tremaining: 1m 3s\n",
            "753:\ttotal: 1m 3s\tremaining: 1m 3s\n",
            "754:\ttotal: 1m 3s\tremaining: 1m 3s\n",
            "755:\tlearn: 17050.5553009\ttest: 20002.3621382\tbest: 20002.3621382 (755)\ttotal: 1m 4s\tremaining: 1m 3s\n",
            "756:\ttotal: 1m 4s\tremaining: 1m 3s\n",
            "757:\ttotal: 1m 4s\tremaining: 1m 3s\n",
            "758:\ttotal: 1m 4s\tremaining: 1m 3s\n",
            "759:\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "760:\tlearn: 16998.8212997\ttest: 19974.2235433\tbest: 19974.2235433 (760)\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "761:\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "762:\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "763:\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "764:\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "765:\tlearn: 16968.3952111\ttest: 19960.3179553\tbest: 19960.3179553 (765)\ttotal: 1m 5s\tremaining: 1m 3s\n",
            "766:\ttotal: 1m 5s\tremaining: 1m 2s\n",
            "767:\ttotal: 1m 5s\tremaining: 1m 2s\n",
            "768:\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "769:\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "770:\tlearn: 16925.2291893\ttest: 19931.3718139\tbest: 19931.3718139 (770)\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "771:\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "772:\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "773:\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "774:\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "775:\tlearn: 16897.6838675\ttest: 19917.4765790\tbest: 19917.4765790 (775)\ttotal: 1m 6s\tremaining: 1m 2s\n",
            "776:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "777:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "778:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "779:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "780:\tlearn: 16853.2810129\ttest: 19892.3533420\tbest: 19892.3533420 (780)\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "781:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "782:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "783:\ttotal: 1m 6s\tremaining: 1m 1s\n",
            "784:\ttotal: 1m 7s\tremaining: 1m 1s\n",
            "785:\tlearn: 16816.7339882\ttest: 19881.4721708\tbest: 19881.4721708 (785)\ttotal: 1m 7s\tremaining: 1m\n",
            "786:\ttotal: 1m 7s\tremaining: 1m\n",
            "787:\ttotal: 1m 7s\tremaining: 1m\n",
            "788:\ttotal: 1m 7s\tremaining: 1m\n",
            "789:\ttotal: 1m 7s\tremaining: 1m\n",
            "790:\tlearn: 16782.9823167\ttest: 19865.1413550\tbest: 19865.1413550 (790)\ttotal: 1m 7s\tremaining: 1m\n",
            "791:\ttotal: 1m 7s\tremaining: 1m\n",
            "792:\ttotal: 1m 7s\tremaining: 1m\n",
            "793:\ttotal: 1m 7s\tremaining: 1m\n",
            "794:\ttotal: 1m 7s\tremaining: 1m\n",
            "795:\tlearn: 16732.7214223\ttest: 19840.3610665\tbest: 19840.3610665 (795)\ttotal: 1m 7s\tremaining: 60s\n",
            "796:\ttotal: 1m 7s\tremaining: 59.9s\n",
            "797:\ttotal: 1m 7s\tremaining: 59.8s\n",
            "798:\ttotal: 1m 8s\tremaining: 59.7s\n",
            "799:\ttotal: 1m 8s\tremaining: 59.6s\n",
            "800:\tlearn: 16679.9158690\ttest: 19804.8186680\tbest: 19804.8186680 (800)\ttotal: 1m 8s\tremaining: 59.5s\n",
            "801:\ttotal: 1m 8s\tremaining: 59.3s\n",
            "802:\ttotal: 1m 8s\tremaining: 59.2s\n",
            "803:\ttotal: 1m 8s\tremaining: 59.1s\n",
            "804:\ttotal: 1m 8s\tremaining: 59s\n",
            "805:\tlearn: 16637.5507381\ttest: 19784.7154195\tbest: 19784.7154195 (805)\ttotal: 1m 8s\tremaining: 58.9s\n",
            "806:\ttotal: 1m 8s\tremaining: 58.8s\n",
            "807:\ttotal: 1m 8s\tremaining: 58.7s\n",
            "808:\ttotal: 1m 8s\tremaining: 58.6s\n",
            "809:\ttotal: 1m 8s\tremaining: 58.5s\n",
            "810:\tlearn: 16612.1751429\ttest: 19773.1496456\tbest: 19773.1496456 (810)\ttotal: 1m 8s\tremaining: 58.4s\n",
            "811:\ttotal: 1m 8s\tremaining: 58.3s\n",
            "812:\ttotal: 1m 8s\tremaining: 58.2s\n",
            "813:\ttotal: 1m 8s\tremaining: 58.1s\n",
            "814:\ttotal: 1m 9s\tremaining: 58s\n",
            "815:\tlearn: 16579.5992946\ttest: 19762.3241227\tbest: 19762.3241227 (815)\ttotal: 1m 9s\tremaining: 57.9s\n",
            "816:\ttotal: 1m 9s\tremaining: 57.8s\n",
            "817:\ttotal: 1m 9s\tremaining: 57.7s\n",
            "818:\ttotal: 1m 9s\tremaining: 57.6s\n",
            "819:\ttotal: 1m 9s\tremaining: 57.6s\n",
            "820:\tlearn: 16542.4943085\ttest: 19752.6348590\tbest: 19752.6348590 (820)\ttotal: 1m 9s\tremaining: 57.5s\n",
            "821:\ttotal: 1m 9s\tremaining: 57.3s\n",
            "822:\ttotal: 1m 9s\tremaining: 57.2s\n",
            "823:\ttotal: 1m 9s\tremaining: 57.1s\n",
            "824:\ttotal: 1m 9s\tremaining: 57s\n",
            "825:\tlearn: 16520.0039171\ttest: 19749.7204445\tbest: 19749.7204445 (825)\ttotal: 1m 9s\tremaining: 57s\n",
            "826:\ttotal: 1m 9s\tremaining: 56.9s\n",
            "827:\ttotal: 1m 9s\tremaining: 56.8s\n",
            "828:\ttotal: 1m 9s\tremaining: 56.7s\n",
            "829:\ttotal: 1m 10s\tremaining: 56.6s\n",
            "830:\tlearn: 16478.6981363\ttest: 19721.8613849\tbest: 19721.3670215 (829)\ttotal: 1m 10s\tremaining: 56.5s\n",
            "831:\ttotal: 1m 10s\tremaining: 56.4s\n",
            "832:\ttotal: 1m 10s\tremaining: 56.3s\n",
            "833:\ttotal: 1m 10s\tremaining: 56.2s\n",
            "834:\ttotal: 1m 10s\tremaining: 56.1s\n",
            "835:\tlearn: 16458.3819085\ttest: 19720.8157158\tbest: 19720.8157158 (835)\ttotal: 1m 10s\tremaining: 56s\n",
            "836:\ttotal: 1m 10s\tremaining: 55.9s\n",
            "837:\ttotal: 1m 10s\tremaining: 55.8s\n",
            "838:\ttotal: 1m 10s\tremaining: 55.7s\n",
            "839:\ttotal: 1m 10s\tremaining: 55.6s\n",
            "840:\tlearn: 16430.7888883\ttest: 19715.0490056\tbest: 19715.0490056 (840)\ttotal: 1m 10s\tremaining: 55.5s\n",
            "841:\ttotal: 1m 10s\tremaining: 55.4s\n",
            "842:\ttotal: 1m 10s\tremaining: 55.3s\n",
            "843:\ttotal: 1m 11s\tremaining: 55.2s\n",
            "844:\ttotal: 1m 11s\tremaining: 55.1s\n",
            "845:\tlearn: 16415.8193281\ttest: 19710.0574883\tbest: 19710.0574883 (845)\ttotal: 1m 11s\tremaining: 55s\n",
            "846:\ttotal: 1m 11s\tremaining: 54.9s\n",
            "847:\ttotal: 1m 11s\tremaining: 54.8s\n",
            "848:\ttotal: 1m 11s\tremaining: 54.7s\n",
            "849:\ttotal: 1m 11s\tremaining: 54.6s\n",
            "850:\tlearn: 16386.1919119\ttest: 19702.3508548\tbest: 19702.3508548 (850)\ttotal: 1m 11s\tremaining: 54.5s\n",
            "851:\ttotal: 1m 11s\tremaining: 54.4s\n",
            "852:\ttotal: 1m 11s\tremaining: 54.3s\n",
            "853:\ttotal: 1m 11s\tremaining: 54.2s\n",
            "854:\ttotal: 1m 11s\tremaining: 54.1s\n",
            "855:\tlearn: 16347.0924635\ttest: 19682.0767792\tbest: 19682.0767792 (855)\ttotal: 1m 11s\tremaining: 54s\n",
            "856:\ttotal: 1m 11s\tremaining: 53.9s\n",
            "857:\ttotal: 1m 11s\tremaining: 53.8s\n",
            "858:\ttotal: 1m 12s\tremaining: 53.7s\n",
            "859:\ttotal: 1m 12s\tremaining: 53.6s\n",
            "860:\tlearn: 16308.9292366\ttest: 19672.5324598\tbest: 19672.5324598 (860)\ttotal: 1m 12s\tremaining: 53.5s\n",
            "861:\ttotal: 1m 12s\tremaining: 53.4s\n",
            "862:\ttotal: 1m 12s\tremaining: 53.4s\n",
            "863:\ttotal: 1m 12s\tremaining: 53.3s\n",
            "864:\ttotal: 1m 12s\tremaining: 53.2s\n",
            "865:\tlearn: 16260.2168791\ttest: 19647.9877056\tbest: 19647.9877056 (865)\ttotal: 1m 12s\tremaining: 53.1s\n",
            "866:\ttotal: 1m 12s\tremaining: 53s\n",
            "867:\ttotal: 1m 12s\tremaining: 52.9s\n",
            "868:\ttotal: 1m 12s\tremaining: 52.8s\n",
            "869:\ttotal: 1m 12s\tremaining: 52.7s\n",
            "870:\tlearn: 16216.8545170\ttest: 19623.0857674\tbest: 19622.8023497 (869)\ttotal: 1m 12s\tremaining: 52.6s\n",
            "871:\ttotal: 1m 12s\tremaining: 52.5s\n",
            "872:\ttotal: 1m 12s\tremaining: 52.4s\n",
            "873:\ttotal: 1m 13s\tremaining: 52.3s\n",
            "874:\ttotal: 1m 13s\tremaining: 52.2s\n",
            "875:\tlearn: 16169.4555318\ttest: 19597.7839384\tbest: 19597.7839384 (875)\ttotal: 1m 13s\tremaining: 52.1s\n",
            "876:\ttotal: 1m 13s\tremaining: 52s\n",
            "877:\ttotal: 1m 13s\tremaining: 51.9s\n",
            "878:\ttotal: 1m 13s\tremaining: 51.8s\n",
            "879:\ttotal: 1m 13s\tremaining: 51.7s\n",
            "880:\tlearn: 16128.4647829\ttest: 19580.6805181\tbest: 19580.6805181 (880)\ttotal: 1m 13s\tremaining: 51.6s\n",
            "881:\ttotal: 1m 13s\tremaining: 51.5s\n",
            "882:\ttotal: 1m 13s\tremaining: 51.4s\n",
            "883:\ttotal: 1m 13s\tremaining: 51.3s\n",
            "884:\ttotal: 1m 13s\tremaining: 51.2s\n",
            "885:\tlearn: 16079.5671778\ttest: 19548.0227689\tbest: 19548.0227689 (885)\ttotal: 1m 13s\tremaining: 51.1s\n",
            "886:\ttotal: 1m 13s\tremaining: 51.1s\n",
            "887:\ttotal: 1m 13s\tremaining: 51s\n",
            "888:\ttotal: 1m 14s\tremaining: 50.9s\n",
            "889:\ttotal: 1m 14s\tremaining: 50.8s\n",
            "890:\tlearn: 16044.7794508\ttest: 19522.9305914\tbest: 19522.9305914 (890)\ttotal: 1m 14s\tremaining: 50.7s\n",
            "891:\ttotal: 1m 14s\tremaining: 50.6s\n",
            "892:\ttotal: 1m 14s\tremaining: 50.5s\n",
            "893:\ttotal: 1m 14s\tremaining: 50.4s\n",
            "894:\ttotal: 1m 14s\tremaining: 50.3s\n",
            "895:\tlearn: 16013.8198048\ttest: 19503.7500682\tbest: 19503.7500682 (895)\ttotal: 1m 14s\tremaining: 50.2s\n",
            "896:\ttotal: 1m 14s\tremaining: 50.1s\n",
            "897:\ttotal: 1m 14s\tremaining: 50s\n",
            "898:\ttotal: 1m 14s\tremaining: 49.9s\n",
            "899:\ttotal: 1m 14s\tremaining: 49.8s\n",
            "900:\tlearn: 15978.7281386\ttest: 19490.5860253\tbest: 19490.5860253 (900)\ttotal: 1m 14s\tremaining: 49.7s\n",
            "901:\ttotal: 1m 14s\tremaining: 49.6s\n",
            "902:\ttotal: 1m 14s\tremaining: 49.5s\n",
            "903:\ttotal: 1m 14s\tremaining: 49.4s\n",
            "904:\ttotal: 1m 15s\tremaining: 49.3s\n",
            "905:\tlearn: 15944.3763535\ttest: 19471.2282043\tbest: 19471.2282043 (905)\ttotal: 1m 15s\tremaining: 49.2s\n",
            "906:\ttotal: 1m 15s\tremaining: 49.1s\n",
            "907:\ttotal: 1m 15s\tremaining: 49s\n",
            "908:\ttotal: 1m 15s\tremaining: 49s\n",
            "909:\ttotal: 1m 15s\tremaining: 48.9s\n",
            "910:\tlearn: 15903.3190486\ttest: 19437.8470280\tbest: 19437.8470280 (910)\ttotal: 1m 15s\tremaining: 48.8s\n",
            "911:\ttotal: 1m 15s\tremaining: 48.7s\n",
            "912:\ttotal: 1m 15s\tremaining: 48.7s\n",
            "913:\ttotal: 1m 15s\tremaining: 48.6s\n",
            "914:\ttotal: 1m 15s\tremaining: 48.6s\n",
            "915:\tlearn: 15861.8890300\ttest: 19417.9184891\tbest: 19417.9184891 (915)\ttotal: 1m 16s\tremaining: 48.6s\n",
            "916:\ttotal: 1m 16s\tremaining: 48.6s\n",
            "917:\ttotal: 1m 16s\tremaining: 48.6s\n",
            "918:\ttotal: 1m 16s\tremaining: 48.6s\n",
            "919:\ttotal: 1m 17s\tremaining: 48.6s\n",
            "920:\tlearn: 15823.3963507\ttest: 19398.0688930\tbest: 19398.0688930 (920)\ttotal: 1m 17s\tremaining: 48.5s\n",
            "921:\ttotal: 1m 17s\tremaining: 48.4s\n",
            "922:\ttotal: 1m 17s\tremaining: 48.4s\n",
            "923:\ttotal: 1m 17s\tremaining: 48.4s\n",
            "924:\ttotal: 1m 17s\tremaining: 48.4s\n",
            "925:\tlearn: 15794.0872941\ttest: 19387.6743203\tbest: 19387.3520782 (923)\ttotal: 1m 17s\tremaining: 48.3s\n",
            "926:\ttotal: 1m 18s\tremaining: 48.3s\n",
            "927:\ttotal: 1m 18s\tremaining: 48.2s\n",
            "928:\ttotal: 1m 18s\tremaining: 48.1s\n",
            "929:\ttotal: 1m 18s\tremaining: 48.1s\n",
            "930:\tlearn: 15782.0317788\ttest: 19381.2514787\tbest: 19381.2048894 (929)\ttotal: 1m 18s\tremaining: 48.1s\n",
            "931:\ttotal: 1m 18s\tremaining: 48.1s\n",
            "932:\ttotal: 1m 19s\tremaining: 48.1s\n",
            "933:\ttotal: 1m 19s\tremaining: 48.1s\n",
            "934:\ttotal: 1m 19s\tremaining: 48.1s\n",
            "935:\tlearn: 15762.3090089\ttest: 19373.9615598\tbest: 19373.9615598 (935)\ttotal: 1m 19s\tremaining: 48s\n",
            "936:\ttotal: 1m 19s\tremaining: 47.9s\n",
            "937:\ttotal: 1m 19s\tremaining: 47.8s\n",
            "938:\ttotal: 1m 19s\tremaining: 47.7s\n",
            "939:\ttotal: 1m 19s\tremaining: 47.6s\n",
            "940:\tlearn: 15738.8735219\ttest: 19363.1619198\tbest: 19363.1619198 (940)\ttotal: 1m 19s\tremaining: 47.5s\n",
            "941:\ttotal: 1m 19s\tremaining: 47.4s\n",
            "942:\ttotal: 1m 20s\tremaining: 47.3s\n",
            "943:\ttotal: 1m 20s\tremaining: 47.2s\n",
            "944:\ttotal: 1m 20s\tremaining: 47.1s\n",
            "945:\tlearn: 15702.9199551\ttest: 19348.2585459\tbest: 19348.1123075 (943)\ttotal: 1m 20s\tremaining: 47s\n",
            "946:\ttotal: 1m 20s\tremaining: 46.9s\n",
            "947:\ttotal: 1m 20s\tremaining: 46.8s\n",
            "948:\ttotal: 1m 20s\tremaining: 46.7s\n",
            "949:\ttotal: 1m 20s\tremaining: 46.6s\n",
            "950:\tlearn: 15661.2115104\ttest: 19335.9318147\tbest: 19335.9318147 (950)\ttotal: 1m 20s\tremaining: 46.5s\n",
            "951:\ttotal: 1m 20s\tremaining: 46.4s\n",
            "952:\ttotal: 1m 20s\tremaining: 46.3s\n",
            "953:\ttotal: 1m 20s\tremaining: 46.2s\n",
            "954:\ttotal: 1m 20s\tremaining: 46.2s\n",
            "955:\tlearn: 15625.7293821\ttest: 19318.1412034\tbest: 19318.1412034 (955)\ttotal: 1m 20s\tremaining: 46.1s\n",
            "956:\ttotal: 1m 21s\tremaining: 46s\n",
            "957:\ttotal: 1m 21s\tremaining: 45.9s\n",
            "958:\ttotal: 1m 21s\tremaining: 45.8s\n",
            "959:\ttotal: 1m 21s\tremaining: 45.7s\n",
            "960:\tlearn: 15604.4991182\ttest: 19316.9337661\tbest: 19316.9337661 (960)\ttotal: 1m 21s\tremaining: 45.6s\n",
            "961:\ttotal: 1m 21s\tremaining: 45.5s\n",
            "962:\ttotal: 1m 21s\tremaining: 45.4s\n",
            "963:\ttotal: 1m 21s\tremaining: 45.3s\n",
            "964:\ttotal: 1m 21s\tremaining: 45.2s\n",
            "965:\tlearn: 15567.1201432\ttest: 19293.0542024\tbest: 19293.0542024 (965)\ttotal: 1m 21s\tremaining: 45.1s\n",
            "966:\ttotal: 1m 21s\tremaining: 45s\n",
            "967:\ttotal: 1m 21s\tremaining: 44.9s\n",
            "968:\ttotal: 1m 21s\tremaining: 44.8s\n",
            "969:\ttotal: 1m 21s\tremaining: 44.7s\n",
            "970:\tlearn: 15535.7289922\ttest: 19281.2606590\tbest: 19281.2606590 (970)\ttotal: 1m 21s\tremaining: 44.6s\n",
            "971:\ttotal: 1m 22s\tremaining: 44.6s\n",
            "972:\ttotal: 1m 22s\tremaining: 44.5s\n",
            "973:\ttotal: 1m 22s\tremaining: 44.4s\n",
            "974:\ttotal: 1m 22s\tremaining: 44.3s\n",
            "975:\tlearn: 15511.9762548\ttest: 19279.8849829\tbest: 19279.8849829 (975)\ttotal: 1m 22s\tremaining: 44.2s\n",
            "976:\ttotal: 1m 22s\tremaining: 44.1s\n",
            "977:\ttotal: 1m 22s\tremaining: 44s\n",
            "978:\ttotal: 1m 22s\tremaining: 43.9s\n",
            "979:\ttotal: 1m 22s\tremaining: 43.8s\n",
            "980:\tlearn: 15479.8651894\ttest: 19261.1444690\tbest: 19261.1444690 (980)\ttotal: 1m 22s\tremaining: 43.7s\n",
            "981:\ttotal: 1m 22s\tremaining: 43.6s\n",
            "982:\ttotal: 1m 22s\tremaining: 43.5s\n",
            "983:\ttotal: 1m 22s\tremaining: 43.4s\n",
            "984:\ttotal: 1m 22s\tremaining: 43.3s\n",
            "985:\tlearn: 15437.9659507\ttest: 19232.5231581\tbest: 19232.5231581 (985)\ttotal: 1m 22s\tremaining: 43.2s\n",
            "986:\ttotal: 1m 22s\tremaining: 43.1s\n",
            "987:\ttotal: 1m 23s\tremaining: 43s\n",
            "988:\ttotal: 1m 23s\tremaining: 42.9s\n",
            "989:\ttotal: 1m 23s\tremaining: 42.8s\n",
            "990:\tlearn: 15402.8288045\ttest: 19212.7059157\tbest: 19212.7059157 (990)\ttotal: 1m 23s\tremaining: 42.8s\n",
            "991:\ttotal: 1m 23s\tremaining: 42.7s\n",
            "992:\ttotal: 1m 23s\tremaining: 42.6s\n",
            "993:\ttotal: 1m 23s\tremaining: 42.5s\n",
            "994:\ttotal: 1m 23s\tremaining: 42.4s\n",
            "995:\tlearn: 15370.5923920\ttest: 19197.0661733\tbest: 19197.0661733 (995)\ttotal: 1m 23s\tremaining: 42.3s\n",
            "996:\ttotal: 1m 23s\tremaining: 42.2s\n",
            "997:\ttotal: 1m 23s\tremaining: 42.1s\n",
            "998:\ttotal: 1m 23s\tremaining: 42s\n",
            "999:\ttotal: 1m 23s\tremaining: 41.9s\n",
            "1000:\tlearn: 15342.8973191\ttest: 19182.9897581\tbest: 19182.9897581 (1000)\ttotal: 1m 23s\tremaining: 41.8s\n",
            "1001:\ttotal: 1m 23s\tremaining: 41.7s\n",
            "1002:\ttotal: 1m 24s\tremaining: 41.7s\n",
            "1003:\ttotal: 1m 24s\tremaining: 41.6s\n",
            "1004:\ttotal: 1m 24s\tremaining: 41.5s\n",
            "1005:\tlearn: 15307.1633873\ttest: 19166.7948275\tbest: 19166.7948275 (1005)\ttotal: 1m 24s\tremaining: 41.4s\n",
            "1006:\ttotal: 1m 24s\tremaining: 41.3s\n",
            "1007:\ttotal: 1m 24s\tremaining: 41.2s\n",
            "1008:\ttotal: 1m 24s\tremaining: 41.1s\n",
            "1009:\ttotal: 1m 24s\tremaining: 41s\n",
            "1010:\tlearn: 15279.6568898\ttest: 19153.2554824\tbest: 19153.2554824 (1010)\ttotal: 1m 24s\tremaining: 40.9s\n",
            "1011:\ttotal: 1m 24s\tremaining: 40.8s\n",
            "1012:\ttotal: 1m 24s\tremaining: 40.7s\n",
            "1013:\ttotal: 1m 24s\tremaining: 40.6s\n",
            "1014:\ttotal: 1m 24s\tremaining: 40.6s\n",
            "1015:\tlearn: 15260.8570302\ttest: 19144.6727128\tbest: 19144.6727128 (1015)\ttotal: 1m 24s\tremaining: 40.5s\n",
            "1016:\ttotal: 1m 25s\tremaining: 40.4s\n",
            "1017:\ttotal: 1m 25s\tremaining: 40.3s\n",
            "1018:\ttotal: 1m 25s\tremaining: 40.2s\n",
            "1019:\ttotal: 1m 25s\tremaining: 40.1s\n",
            "1020:\tlearn: 15231.2495808\ttest: 19137.8719807\tbest: 19137.8719807 (1020)\ttotal: 1m 25s\tremaining: 40s\n",
            "1021:\ttotal: 1m 25s\tremaining: 39.9s\n",
            "1022:\ttotal: 1m 25s\tremaining: 39.8s\n",
            "1023:\ttotal: 1m 25s\tremaining: 39.7s\n",
            "1024:\ttotal: 1m 25s\tremaining: 39.6s\n",
            "1025:\tlearn: 15205.7342179\ttest: 19129.9841669\tbest: 19129.9841669 (1025)\ttotal: 1m 25s\tremaining: 39.6s\n",
            "1026:\ttotal: 1m 25s\tremaining: 39.5s\n",
            "1027:\ttotal: 1m 25s\tremaining: 39.4s\n",
            "1028:\ttotal: 1m 25s\tremaining: 39.3s\n",
            "1029:\ttotal: 1m 25s\tremaining: 39.2s\n",
            "1030:\tlearn: 15168.6858044\ttest: 19106.4462576\tbest: 19106.4462576 (1030)\ttotal: 1m 25s\tremaining: 39.1s\n",
            "1031:\ttotal: 1m 26s\tremaining: 39s\n",
            "1032:\ttotal: 1m 26s\tremaining: 38.9s\n",
            "1033:\ttotal: 1m 26s\tremaining: 38.8s\n",
            "1034:\ttotal: 1m 26s\tremaining: 38.7s\n",
            "1035:\tlearn: 15140.5858490\ttest: 19096.5641663\tbest: 19096.1914526 (1033)\ttotal: 1m 26s\tremaining: 38.6s\n",
            "1036:\ttotal: 1m 26s\tremaining: 38.6s\n",
            "1037:\ttotal: 1m 26s\tremaining: 38.5s\n",
            "1038:\ttotal: 1m 26s\tremaining: 38.4s\n",
            "1039:\ttotal: 1m 26s\tremaining: 38.3s\n",
            "1040:\tlearn: 15120.7821027\ttest: 19087.8157461\tbest: 19087.8157461 (1040)\ttotal: 1m 26s\tremaining: 38.2s\n",
            "1041:\ttotal: 1m 26s\tremaining: 38.1s\n",
            "1042:\ttotal: 1m 26s\tremaining: 38s\n",
            "1043:\ttotal: 1m 26s\tremaining: 37.9s\n",
            "1044:\ttotal: 1m 26s\tremaining: 37.8s\n",
            "1045:\tlearn: 15094.8596387\ttest: 19072.1371794\tbest: 19072.1371794 (1045)\ttotal: 1m 26s\tremaining: 37.7s\n",
            "1046:\ttotal: 1m 27s\tremaining: 37.7s\n",
            "1047:\ttotal: 1m 27s\tremaining: 37.6s\n",
            "1048:\ttotal: 1m 27s\tremaining: 37.5s\n",
            "1049:\ttotal: 1m 27s\tremaining: 37.4s\n",
            "1050:\tlearn: 15050.5133567\ttest: 19055.7080085\tbest: 19055.1398789 (1049)\ttotal: 1m 27s\tremaining: 37.3s\n",
            "1051:\ttotal: 1m 27s\tremaining: 37.2s\n",
            "1052:\ttotal: 1m 27s\tremaining: 37.1s\n",
            "1053:\ttotal: 1m 27s\tremaining: 37s\n",
            "1054:\ttotal: 1m 27s\tremaining: 36.9s\n",
            "1055:\tlearn: 15008.7206077\ttest: 19038.0662238\tbest: 19038.0662238 (1055)\ttotal: 1m 27s\tremaining: 36.9s\n",
            "1056:\ttotal: 1m 27s\tremaining: 36.8s\n",
            "1057:\ttotal: 1m 27s\tremaining: 36.7s\n",
            "1058:\ttotal: 1m 27s\tremaining: 36.6s\n",
            "1059:\ttotal: 1m 27s\tremaining: 36.5s\n",
            "1060:\tlearn: 14981.7055156\ttest: 19019.8731333\tbest: 19019.8731333 (1060)\ttotal: 1m 27s\tremaining: 36.4s\n",
            "1061:\ttotal: 1m 28s\tremaining: 36.3s\n",
            "1062:\ttotal: 1m 28s\tremaining: 36.2s\n",
            "1063:\ttotal: 1m 28s\tremaining: 36.1s\n",
            "1064:\ttotal: 1m 28s\tremaining: 36s\n",
            "1065:\tlearn: 14943.9926511\ttest: 19000.7844945\tbest: 19000.7844945 (1065)\ttotal: 1m 28s\tremaining: 36s\n",
            "1066:\ttotal: 1m 28s\tremaining: 35.9s\n",
            "1067:\ttotal: 1m 28s\tremaining: 35.8s\n",
            "1068:\ttotal: 1m 28s\tremaining: 35.7s\n",
            "1069:\ttotal: 1m 28s\tremaining: 35.6s\n",
            "1070:\tlearn: 14924.7834719\ttest: 18992.9419758\tbest: 18992.9419758 (1070)\ttotal: 1m 28s\tremaining: 35.5s\n",
            "1071:\ttotal: 1m 28s\tremaining: 35.4s\n",
            "1072:\ttotal: 1m 28s\tremaining: 35.3s\n",
            "1073:\ttotal: 1m 28s\tremaining: 35.2s\n",
            "1074:\ttotal: 1m 28s\tremaining: 35.2s\n",
            "1075:\tlearn: 14889.9291889\ttest: 18981.7605630\tbest: 18981.7605630 (1075)\ttotal: 1m 29s\tremaining: 35.1s\n",
            "1076:\ttotal: 1m 29s\tremaining: 35s\n",
            "1077:\ttotal: 1m 29s\tremaining: 34.9s\n",
            "1078:\ttotal: 1m 29s\tremaining: 34.8s\n",
            "1079:\ttotal: 1m 29s\tremaining: 34.7s\n",
            "1080:\tlearn: 14863.8248051\ttest: 18978.7568524\tbest: 18978.7568524 (1080)\ttotal: 1m 29s\tremaining: 34.6s\n",
            "1081:\ttotal: 1m 29s\tremaining: 34.6s\n",
            "1082:\ttotal: 1m 29s\tremaining: 34.6s\n",
            "1083:\ttotal: 1m 30s\tremaining: 34.5s\n",
            "1084:\ttotal: 1m 30s\tremaining: 34.5s\n",
            "1085:\tlearn: 14839.9533761\ttest: 18970.7059763\tbest: 18970.7059763 (1085)\ttotal: 1m 30s\tremaining: 34.5s\n",
            "1086:\ttotal: 1m 30s\tremaining: 34.5s\n",
            "1087:\ttotal: 1m 30s\tremaining: 34.4s\n",
            "1088:\ttotal: 1m 31s\tremaining: 34.4s\n",
            "1089:\ttotal: 1m 31s\tremaining: 34.3s\n",
            "1090:\tlearn: 14812.4990142\ttest: 18959.9296309\tbest: 18959.3382066 (1089)\ttotal: 1m 31s\tremaining: 34.3s\n",
            "1091:\ttotal: 1m 31s\tremaining: 34.3s\n",
            "1092:\ttotal: 1m 31s\tremaining: 34.2s\n",
            "1093:\ttotal: 1m 32s\tremaining: 34.2s\n",
            "1094:\ttotal: 1m 32s\tremaining: 34.2s\n",
            "1095:\tlearn: 14782.6419797\ttest: 18946.2996957\tbest: 18946.2996957 (1095)\ttotal: 1m 32s\tremaining: 34.2s\n",
            "1096:\ttotal: 1m 32s\tremaining: 34.1s\n",
            "1097:\ttotal: 1m 33s\tremaining: 34.1s\n",
            "1098:\ttotal: 1m 33s\tremaining: 34s\n",
            "1099:\ttotal: 1m 33s\tremaining: 34s\n",
            "1100:\tlearn: 14741.9396403\ttest: 18918.0355688\tbest: 18918.0355688 (1100)\ttotal: 1m 33s\tremaining: 33.9s\n",
            "1101:\ttotal: 1m 33s\tremaining: 33.8s\n",
            "1102:\ttotal: 1m 33s\tremaining: 33.7s\n",
            "1103:\ttotal: 1m 33s\tremaining: 33.6s\n",
            "1104:\ttotal: 1m 33s\tremaining: 33.5s\n",
            "1105:\tlearn: 14701.8773480\ttest: 18896.1684006\tbest: 18896.1684006 (1105)\ttotal: 1m 33s\tremaining: 33.4s\n",
            "1106:\ttotal: 1m 33s\tremaining: 33.3s\n",
            "1107:\ttotal: 1m 33s\tremaining: 33.2s\n",
            "1108:\ttotal: 1m 34s\tremaining: 33.1s\n",
            "1109:\ttotal: 1m 34s\tremaining: 33.1s\n",
            "1110:\tlearn: 14670.0757681\ttest: 18882.3948153\tbest: 18882.3948153 (1110)\ttotal: 1m 34s\tremaining: 33s\n",
            "1111:\ttotal: 1m 34s\tremaining: 32.9s\n",
            "1112:\ttotal: 1m 34s\tremaining: 32.8s\n",
            "1113:\ttotal: 1m 34s\tremaining: 32.7s\n",
            "1114:\ttotal: 1m 34s\tremaining: 32.6s\n",
            "1115:\tlearn: 14641.9436440\ttest: 18872.9061442\tbest: 18872.9061442 (1115)\ttotal: 1m 34s\tremaining: 32.5s\n",
            "1116:\ttotal: 1m 34s\tremaining: 32.4s\n",
            "1117:\ttotal: 1m 34s\tremaining: 32.3s\n",
            "1118:\ttotal: 1m 34s\tremaining: 32.2s\n",
            "1119:\ttotal: 1m 34s\tremaining: 32.1s\n",
            "1120:\tlearn: 14620.7255820\ttest: 18868.2782816\tbest: 18868.2782816 (1120)\ttotal: 1m 34s\tremaining: 32.1s\n",
            "1121:\ttotal: 1m 34s\tremaining: 32s\n",
            "1122:\ttotal: 1m 34s\tremaining: 31.9s\n",
            "1123:\ttotal: 1m 35s\tremaining: 31.8s\n",
            "1124:\ttotal: 1m 35s\tremaining: 31.7s\n",
            "1125:\tlearn: 14584.2684080\ttest: 18853.8589180\tbest: 18853.3619663 (1123)\ttotal: 1m 35s\tremaining: 31.6s\n",
            "1126:\ttotal: 1m 35s\tremaining: 31.5s\n",
            "1127:\ttotal: 1m 35s\tremaining: 31.4s\n",
            "1128:\ttotal: 1m 35s\tremaining: 31.3s\n",
            "1129:\ttotal: 1m 35s\tremaining: 31.2s\n",
            "1130:\tlearn: 14571.0548177\ttest: 18847.5111367\tbest: 18847.5111367 (1130)\ttotal: 1m 35s\tremaining: 31.2s\n",
            "1131:\ttotal: 1m 35s\tremaining: 31.1s\n",
            "1132:\ttotal: 1m 35s\tremaining: 31s\n",
            "1133:\ttotal: 1m 35s\tremaining: 30.9s\n",
            "1134:\ttotal: 1m 35s\tremaining: 30.8s\n",
            "1135:\tlearn: 14555.8445466\ttest: 18846.0241641\tbest: 18846.0241641 (1135)\ttotal: 1m 35s\tremaining: 30.7s\n",
            "1136:\ttotal: 1m 35s\tremaining: 30.6s\n",
            "1137:\ttotal: 1m 35s\tremaining: 30.5s\n",
            "1138:\ttotal: 1m 36s\tremaining: 30.4s\n",
            "1139:\ttotal: 1m 36s\tremaining: 30.3s\n",
            "1140:\tlearn: 14532.0230346\ttest: 18833.9342615\tbest: 18833.9342615 (1140)\ttotal: 1m 36s\tremaining: 30.3s\n",
            "1141:\ttotal: 1m 36s\tremaining: 30.2s\n",
            "1142:\ttotal: 1m 36s\tremaining: 30.1s\n",
            "1143:\ttotal: 1m 36s\tremaining: 30s\n",
            "1144:\ttotal: 1m 36s\tremaining: 29.9s\n",
            "1145:\tlearn: 14503.7023351\ttest: 18818.8018239\tbest: 18818.6646446 (1144)\ttotal: 1m 36s\tremaining: 29.8s\n",
            "1146:\ttotal: 1m 36s\tremaining: 29.7s\n",
            "1147:\ttotal: 1m 36s\tremaining: 29.6s\n",
            "1148:\ttotal: 1m 36s\tremaining: 29.5s\n",
            "1149:\ttotal: 1m 36s\tremaining: 29.4s\n",
            "1150:\tlearn: 14479.2729447\ttest: 18807.8210036\tbest: 18807.8210036 (1150)\ttotal: 1m 36s\tremaining: 29.4s\n",
            "1151:\ttotal: 1m 36s\tremaining: 29.3s\n",
            "1152:\ttotal: 1m 36s\tremaining: 29.2s\n",
            "1153:\ttotal: 1m 37s\tremaining: 29.1s\n",
            "1154:\ttotal: 1m 37s\tremaining: 29s\n",
            "1155:\tlearn: 14460.4786314\ttest: 18805.7879018\tbest: 18805.7879018 (1155)\ttotal: 1m 37s\tremaining: 28.9s\n",
            "1156:\ttotal: 1m 37s\tremaining: 28.8s\n",
            "1157:\ttotal: 1m 37s\tremaining: 28.7s\n",
            "1158:\ttotal: 1m 37s\tremaining: 28.7s\n",
            "1159:\ttotal: 1m 37s\tremaining: 28.6s\n",
            "1160:\tlearn: 14433.5167842\ttest: 18798.2042120\tbest: 18798.2042120 (1160)\ttotal: 1m 37s\tremaining: 28.5s\n",
            "1161:\ttotal: 1m 37s\tremaining: 28.4s\n",
            "1162:\ttotal: 1m 37s\tremaining: 28.3s\n",
            "1163:\ttotal: 1m 37s\tremaining: 28.2s\n",
            "1164:\ttotal: 1m 37s\tremaining: 28.1s\n",
            "1165:\tlearn: 14411.4745367\ttest: 18791.4099508\tbest: 18791.4099508 (1165)\ttotal: 1m 37s\tremaining: 28s\n",
            "1166:\ttotal: 1m 37s\tremaining: 27.9s\n",
            "1167:\ttotal: 1m 37s\tremaining: 27.8s\n",
            "1168:\ttotal: 1m 38s\tremaining: 27.8s\n",
            "1169:\ttotal: 1m 38s\tremaining: 27.7s\n",
            "1170:\tlearn: 14392.8033521\ttest: 18784.2235635\tbest: 18784.2235635 (1170)\ttotal: 1m 38s\tremaining: 27.6s\n",
            "1171:\ttotal: 1m 38s\tremaining: 27.5s\n",
            "1172:\ttotal: 1m 38s\tremaining: 27.4s\n",
            "1173:\ttotal: 1m 38s\tremaining: 27.3s\n",
            "1174:\ttotal: 1m 38s\tremaining: 27.2s\n",
            "1175:\tlearn: 14370.5137380\ttest: 18781.2806778\tbest: 18780.9946718 (1172)\ttotal: 1m 38s\tremaining: 27.1s\n",
            "1176:\ttotal: 1m 38s\tremaining: 27.1s\n",
            "1177:\ttotal: 1m 38s\tremaining: 27s\n",
            "1178:\ttotal: 1m 38s\tremaining: 26.9s\n",
            "1179:\ttotal: 1m 38s\tremaining: 26.8s\n",
            "1180:\tlearn: 14347.4054848\ttest: 18775.1800783\tbest: 18775.1800783 (1180)\ttotal: 1m 38s\tremaining: 26.7s\n",
            "1181:\ttotal: 1m 38s\tremaining: 26.6s\n",
            "1182:\ttotal: 1m 38s\tremaining: 26.5s\n",
            "1183:\ttotal: 1m 39s\tremaining: 26.4s\n",
            "1184:\ttotal: 1m 39s\tremaining: 26.4s\n",
            "1185:\tlearn: 14322.1907333\ttest: 18768.6899411\tbest: 18768.6899411 (1185)\ttotal: 1m 39s\tremaining: 26.3s\n",
            "1186:\ttotal: 1m 39s\tremaining: 26.2s\n",
            "1187:\ttotal: 1m 39s\tremaining: 26.1s\n",
            "1188:\ttotal: 1m 39s\tremaining: 26s\n",
            "1189:\ttotal: 1m 39s\tremaining: 25.9s\n",
            "1190:\tlearn: 14299.6171468\ttest: 18762.7601686\tbest: 18762.7601686 (1190)\ttotal: 1m 39s\tremaining: 25.8s\n",
            "1191:\ttotal: 1m 39s\tremaining: 25.7s\n",
            "1192:\ttotal: 1m 39s\tremaining: 25.7s\n",
            "1193:\ttotal: 1m 39s\tremaining: 25.6s\n",
            "1194:\ttotal: 1m 39s\tremaining: 25.5s\n",
            "1195:\tlearn: 14275.9653527\ttest: 18755.1751848\tbest: 18755.1751848 (1195)\ttotal: 1m 39s\tremaining: 25.4s\n",
            "1196:\ttotal: 1m 39s\tremaining: 25.3s\n",
            "1197:\ttotal: 1m 40s\tremaining: 25.2s\n",
            "1198:\ttotal: 1m 40s\tremaining: 25.1s\n",
            "1199:\ttotal: 1m 40s\tremaining: 25s\n",
            "1200:\tlearn: 14259.3851367\ttest: 18753.2249082\tbest: 18752.7098385 (1198)\ttotal: 1m 40s\tremaining: 25s\n",
            "1201:\ttotal: 1m 40s\tremaining: 24.9s\n",
            "1202:\ttotal: 1m 40s\tremaining: 24.8s\n",
            "1203:\ttotal: 1m 40s\tremaining: 24.7s\n",
            "1204:\ttotal: 1m 40s\tremaining: 24.6s\n",
            "1205:\tlearn: 14241.3861680\ttest: 18747.2187813\tbest: 18747.2187813 (1205)\ttotal: 1m 40s\tremaining: 24.5s\n",
            "1206:\ttotal: 1m 40s\tremaining: 24.4s\n",
            "1207:\ttotal: 1m 40s\tremaining: 24.3s\n",
            "1208:\ttotal: 1m 40s\tremaining: 24.3s\n",
            "1209:\ttotal: 1m 40s\tremaining: 24.2s\n",
            "1210:\tlearn: 14213.3938115\ttest: 18735.5494758\tbest: 18735.5494758 (1210)\ttotal: 1m 40s\tremaining: 24.1s\n",
            "1211:\ttotal: 1m 40s\tremaining: 24s\n",
            "1212:\ttotal: 1m 41s\tremaining: 23.9s\n",
            "1213:\ttotal: 1m 41s\tremaining: 23.8s\n",
            "1214:\ttotal: 1m 41s\tremaining: 23.7s\n",
            "1215:\tlearn: 14193.2328811\ttest: 18721.6387111\tbest: 18721.6387111 (1215)\ttotal: 1m 41s\tremaining: 23.6s\n",
            "1216:\ttotal: 1m 41s\tremaining: 23.6s\n",
            "1217:\ttotal: 1m 41s\tremaining: 23.5s\n",
            "1218:\ttotal: 1m 41s\tremaining: 23.4s\n",
            "1219:\ttotal: 1m 41s\tremaining: 23.3s\n",
            "1220:\tlearn: 14171.0941274\ttest: 18715.8435298\tbest: 18715.8435298 (1220)\ttotal: 1m 41s\tremaining: 23.2s\n",
            "1221:\ttotal: 1m 41s\tremaining: 23.1s\n",
            "1222:\ttotal: 1m 41s\tremaining: 23s\n",
            "1223:\ttotal: 1m 41s\tremaining: 23s\n",
            "1224:\ttotal: 1m 41s\tremaining: 22.9s\n",
            "1225:\tlearn: 14141.4545092\ttest: 18702.9240802\tbest: 18702.9240802 (1225)\ttotal: 1m 41s\tremaining: 22.8s\n",
            "1226:\ttotal: 1m 41s\tremaining: 22.7s\n",
            "1227:\ttotal: 1m 42s\tremaining: 22.6s\n",
            "1228:\ttotal: 1m 42s\tremaining: 22.5s\n",
            "1229:\ttotal: 1m 42s\tremaining: 22.4s\n",
            "1230:\tlearn: 14124.0190482\ttest: 18700.1649630\tbest: 18700.0601373 (1229)\ttotal: 1m 42s\tremaining: 22.3s\n",
            "1231:\ttotal: 1m 42s\tremaining: 22.3s\n",
            "1232:\ttotal: 1m 42s\tremaining: 22.2s\n",
            "1233:\ttotal: 1m 42s\tremaining: 22.1s\n",
            "1234:\ttotal: 1m 42s\tremaining: 22s\n",
            "1235:\tlearn: 14096.1464926\ttest: 18690.7914708\tbest: 18690.7914708 (1235)\ttotal: 1m 42s\tremaining: 21.9s\n",
            "1236:\ttotal: 1m 42s\tremaining: 21.8s\n",
            "1237:\ttotal: 1m 42s\tremaining: 21.7s\n",
            "1238:\ttotal: 1m 42s\tremaining: 21.7s\n",
            "1239:\ttotal: 1m 42s\tremaining: 21.6s\n",
            "1240:\tlearn: 14072.0476638\ttest: 18680.7566704\tbest: 18680.7566704 (1240)\ttotal: 1m 42s\tremaining: 21.5s\n",
            "1241:\ttotal: 1m 43s\tremaining: 21.4s\n",
            "1242:\ttotal: 1m 43s\tremaining: 21.3s\n",
            "1243:\ttotal: 1m 43s\tremaining: 21.2s\n",
            "1244:\ttotal: 1m 43s\tremaining: 21.1s\n",
            "1245:\tlearn: 14045.9998527\ttest: 18669.2957222\tbest: 18669.2957222 (1245)\ttotal: 1m 43s\tremaining: 21.1s\n",
            "1246:\ttotal: 1m 43s\tremaining: 21s\n",
            "1247:\ttotal: 1m 43s\tremaining: 21s\n",
            "1248:\ttotal: 1m 44s\tremaining: 20.9s\n",
            "1249:\ttotal: 1m 44s\tremaining: 20.9s\n",
            "1250:\tlearn: 14017.9087714\ttest: 18662.0860404\tbest: 18662.0860404 (1250)\ttotal: 1m 44s\tremaining: 20.8s\n",
            "1251:\ttotal: 1m 44s\tremaining: 20.7s\n",
            "1252:\ttotal: 1m 44s\tremaining: 20.7s\n",
            "1253:\ttotal: 1m 45s\tremaining: 20.6s\n",
            "1254:\ttotal: 1m 45s\tremaining: 20.6s\n",
            "1255:\tlearn: 13988.3268352\ttest: 18654.8232987\tbest: 18654.8232987 (1255)\ttotal: 1m 45s\tremaining: 20.5s\n",
            "1256:\ttotal: 1m 46s\tremaining: 20.5s\n",
            "1257:\ttotal: 1m 46s\tremaining: 20.4s\n",
            "1258:\ttotal: 1m 46s\tremaining: 20.4s\n",
            "1259:\ttotal: 1m 46s\tremaining: 20.3s\n",
            "1260:\tlearn: 13961.3871732\ttest: 18644.4119021\tbest: 18644.4119021 (1260)\ttotal: 1m 47s\tremaining: 20.3s\n",
            "1261:\ttotal: 1m 47s\tremaining: 20.2s\n",
            "1262:\ttotal: 1m 47s\tremaining: 20.1s\n",
            "1263:\ttotal: 1m 47s\tremaining: 20s\n",
            "1264:\ttotal: 1m 47s\tremaining: 19.9s\n",
            "1265:\tlearn: 13945.0909296\ttest: 18640.2447552\tbest: 18640.2447552 (1265)\ttotal: 1m 47s\tremaining: 19.8s\n",
            "1266:\ttotal: 1m 47s\tremaining: 19.8s\n",
            "1267:\ttotal: 1m 47s\tremaining: 19.7s\n",
            "1268:\ttotal: 1m 47s\tremaining: 19.6s\n",
            "1269:\ttotal: 1m 47s\tremaining: 19.5s\n",
            "1270:\tlearn: 13920.2045211\ttest: 18630.3898410\tbest: 18630.3898410 (1270)\ttotal: 1m 47s\tremaining: 19.4s\n",
            "1271:\ttotal: 1m 47s\tremaining: 19.3s\n",
            "1272:\ttotal: 1m 47s\tremaining: 19.2s\n",
            "1273:\ttotal: 1m 47s\tremaining: 19.1s\n",
            "1274:\ttotal: 1m 47s\tremaining: 19.1s\n",
            "1275:\tlearn: 13893.3791137\ttest: 18615.8307097\tbest: 18615.7491785 (1273)\ttotal: 1m 48s\tremaining: 19s\n",
            "1276:\ttotal: 1m 48s\tremaining: 18.9s\n",
            "1277:\ttotal: 1m 48s\tremaining: 18.8s\n",
            "1278:\ttotal: 1m 48s\tremaining: 18.7s\n",
            "1279:\ttotal: 1m 48s\tremaining: 18.6s\n",
            "1280:\tlearn: 13877.2082172\ttest: 18607.8186579\tbest: 18607.8186579 (1280)\ttotal: 1m 48s\tremaining: 18.5s\n",
            "1281:\ttotal: 1m 48s\tremaining: 18.4s\n",
            "1282:\ttotal: 1m 48s\tremaining: 18.4s\n",
            "1283:\ttotal: 1m 48s\tremaining: 18.3s\n",
            "1284:\ttotal: 1m 48s\tremaining: 18.2s\n",
            "1285:\tlearn: 13864.0656201\ttest: 18604.6117666\tbest: 18604.6117666 (1285)\ttotal: 1m 48s\tremaining: 18.1s\n",
            "1286:\ttotal: 1m 48s\tremaining: 18s\n",
            "1287:\ttotal: 1m 48s\tremaining: 17.9s\n",
            "1288:\ttotal: 1m 48s\tremaining: 17.8s\n",
            "1289:\ttotal: 1m 49s\tremaining: 17.7s\n",
            "1290:\tlearn: 13843.6939289\ttest: 18598.2639854\tbest: 18598.2639854 (1290)\ttotal: 1m 49s\tremaining: 17.7s\n",
            "1291:\ttotal: 1m 49s\tremaining: 17.6s\n",
            "1292:\ttotal: 1m 49s\tremaining: 17.5s\n",
            "1293:\ttotal: 1m 49s\tremaining: 17.4s\n",
            "1294:\ttotal: 1m 49s\tremaining: 17.3s\n",
            "1295:\tlearn: 13822.2739803\ttest: 18589.6825098\tbest: 18589.6825098 (1295)\ttotal: 1m 49s\tremaining: 17.2s\n",
            "1296:\ttotal: 1m 49s\tremaining: 17.1s\n",
            "1297:\ttotal: 1m 49s\tremaining: 17.1s\n",
            "1298:\ttotal: 1m 49s\tremaining: 17s\n",
            "1299:\ttotal: 1m 49s\tremaining: 16.9s\n",
            "1300:\tlearn: 13803.6405108\ttest: 18586.5726794\tbest: 18586.5726794 (1300)\ttotal: 1m 49s\tremaining: 16.8s\n",
            "1301:\ttotal: 1m 49s\tremaining: 16.7s\n",
            "1302:\ttotal: 1m 49s\tremaining: 16.6s\n",
            "1303:\ttotal: 1m 49s\tremaining: 16.5s\n",
            "1304:\ttotal: 1m 50s\tremaining: 16.4s\n",
            "1305:\tlearn: 13787.0370001\ttest: 18581.6018684\tbest: 18581.6018684 (1305)\ttotal: 1m 50s\tremaining: 16.4s\n",
            "1306:\ttotal: 1m 50s\tremaining: 16.3s\n",
            "1307:\ttotal: 1m 50s\tremaining: 16.2s\n",
            "1308:\ttotal: 1m 50s\tremaining: 16.1s\n",
            "1309:\ttotal: 1m 50s\tremaining: 16.1s\n",
            "1310:\tlearn: 13783.0680423\ttest: 18581.1424469\tbest: 18581.1049167 (1309)\ttotal: 1m 50s\tremaining: 16s\n",
            "1311:\ttotal: 1m 50s\tremaining: 15.9s\n",
            "1312:\ttotal: 1m 51s\tremaining: 15.8s\n",
            "1313:\ttotal: 1m 51s\tremaining: 15.7s\n",
            "1314:\ttotal: 1m 51s\tremaining: 15.6s\n",
            "1315:\tlearn: 13769.2177327\ttest: 18578.8996937\tbest: 18578.7638084 (1314)\ttotal: 1m 51s\tremaining: 15.5s\n",
            "1316:\ttotal: 1m 51s\tremaining: 15.5s\n",
            "1317:\ttotal: 1m 51s\tremaining: 15.4s\n",
            "1318:\ttotal: 1m 51s\tremaining: 15.3s\n",
            "1319:\ttotal: 1m 51s\tremaining: 15.2s\n",
            "1320:\tlearn: 13748.5953471\ttest: 18571.2525908\tbest: 18571.2525908 (1320)\ttotal: 1m 51s\tremaining: 15.2s\n",
            "1321:\ttotal: 1m 51s\tremaining: 15.1s\n",
            "1322:\ttotal: 1m 52s\tremaining: 15s\n",
            "1323:\ttotal: 1m 52s\tremaining: 14.9s\n",
            "1324:\ttotal: 1m 52s\tremaining: 14.8s\n",
            "1325:\tlearn: 13731.9774160\ttest: 18569.5430253\tbest: 18569.5430253 (1325)\ttotal: 1m 52s\tremaining: 14.7s\n",
            "1326:\ttotal: 1m 52s\tremaining: 14.6s\n",
            "1327:\ttotal: 1m 52s\tremaining: 14.6s\n",
            "1328:\ttotal: 1m 52s\tremaining: 14.5s\n",
            "1329:\ttotal: 1m 52s\tremaining: 14.4s\n",
            "1330:\tlearn: 13706.3355966\ttest: 18557.5488894\tbest: 18557.5488894 (1330)\ttotal: 1m 52s\tremaining: 14.3s\n",
            "1331:\ttotal: 1m 52s\tremaining: 14.2s\n",
            "1332:\ttotal: 1m 52s\tremaining: 14.1s\n",
            "1333:\ttotal: 1m 52s\tremaining: 14s\n",
            "1334:\ttotal: 1m 52s\tremaining: 13.9s\n",
            "1335:\tlearn: 13695.8108700\ttest: 18555.7875580\tbest: 18555.7875580 (1335)\ttotal: 1m 52s\tremaining: 13.9s\n",
            "1336:\ttotal: 1m 52s\tremaining: 13.8s\n",
            "1337:\ttotal: 1m 53s\tremaining: 13.7s\n",
            "1338:\ttotal: 1m 53s\tremaining: 13.6s\n",
            "1339:\ttotal: 1m 53s\tremaining: 13.5s\n",
            "1340:\tlearn: 13684.3887461\ttest: 18552.2713660\tbest: 18552.2713660 (1340)\ttotal: 1m 53s\tremaining: 13.4s\n",
            "1341:\ttotal: 1m 53s\tremaining: 13.3s\n",
            "1342:\ttotal: 1m 53s\tremaining: 13.3s\n",
            "1343:\ttotal: 1m 53s\tremaining: 13.2s\n",
            "1344:\ttotal: 1m 53s\tremaining: 13.1s\n",
            "1345:\tlearn: 13663.5888779\ttest: 18540.9010080\tbest: 18540.9010080 (1345)\ttotal: 1m 53s\tremaining: 13s\n",
            "1346:\ttotal: 1m 53s\tremaining: 12.9s\n",
            "1347:\ttotal: 1m 53s\tremaining: 12.8s\n",
            "1348:\ttotal: 1m 53s\tremaining: 12.7s\n",
            "1349:\ttotal: 1m 53s\tremaining: 12.6s\n",
            "1350:\tlearn: 13644.2687720\ttest: 18539.4166237\tbest: 18539.4166237 (1350)\ttotal: 1m 53s\tremaining: 12.6s\n",
            "1351:\ttotal: 1m 53s\tremaining: 12.5s\n",
            "1352:\ttotal: 1m 54s\tremaining: 12.4s\n",
            "1353:\ttotal: 1m 54s\tremaining: 12.3s\n",
            "1354:\ttotal: 1m 54s\tremaining: 12.2s\n",
            "1355:\tlearn: 13624.3496618\ttest: 18535.3646557\tbest: 18535.3012426 (1354)\ttotal: 1m 54s\tremaining: 12.1s\n",
            "1356:\ttotal: 1m 54s\tremaining: 12s\n",
            "1357:\ttotal: 1m 54s\tremaining: 12s\n",
            "1358:\ttotal: 1m 54s\tremaining: 11.9s\n",
            "1359:\ttotal: 1m 54s\tremaining: 11.8s\n",
            "1360:\tlearn: 13614.8731926\ttest: 18534.7745256\tbest: 18534.7745256 (1360)\ttotal: 1m 54s\tremaining: 11.7s\n",
            "1361:\ttotal: 1m 54s\tremaining: 11.6s\n",
            "1362:\ttotal: 1m 54s\tremaining: 11.5s\n",
            "1363:\ttotal: 1m 54s\tremaining: 11.4s\n",
            "1364:\ttotal: 1m 54s\tremaining: 11.4s\n",
            "1365:\tlearn: 13605.1549031\ttest: 18532.2625901\tbest: 18532.0141143 (1364)\ttotal: 1m 54s\tremaining: 11.3s\n",
            "1366:\ttotal: 1m 54s\tremaining: 11.2s\n",
            "1367:\ttotal: 1m 55s\tremaining: 11.1s\n",
            "1368:\ttotal: 1m 55s\tremaining: 11s\n",
            "1369:\ttotal: 1m 55s\tremaining: 10.9s\n",
            "1370:\tlearn: 13595.5730535\ttest: 18531.5301040\tbest: 18530.9723275 (1368)\ttotal: 1m 55s\tremaining: 10.8s\n",
            "1371:\ttotal: 1m 55s\tremaining: 10.8s\n",
            "1372:\ttotal: 1m 55s\tremaining: 10.7s\n",
            "1373:\ttotal: 1m 55s\tremaining: 10.6s\n",
            "1374:\ttotal: 1m 55s\tremaining: 10.5s\n",
            "1375:\tlearn: 13568.4381605\ttest: 18522.3675574\tbest: 18522.3675574 (1375)\ttotal: 1m 55s\tremaining: 10.4s\n",
            "1376:\ttotal: 1m 55s\tremaining: 10.3s\n",
            "1377:\ttotal: 1m 55s\tremaining: 10.2s\n",
            "1378:\ttotal: 1m 55s\tremaining: 10.2s\n",
            "1379:\ttotal: 1m 55s\tremaining: 10.1s\n",
            "1380:\tlearn: 13538.3171204\ttest: 18511.1757914\tbest: 18511.1757914 (1380)\ttotal: 1m 55s\tremaining: 9.99s\n",
            "1381:\ttotal: 1m 55s\tremaining: 9.9s\n",
            "1382:\ttotal: 1m 56s\tremaining: 9.82s\n",
            "1383:\ttotal: 1m 56s\tremaining: 9.73s\n",
            "1384:\ttotal: 1m 56s\tremaining: 9.65s\n",
            "1385:\tlearn: 13520.8694575\ttest: 18505.1644879\tbest: 18505.1644879 (1385)\ttotal: 1m 56s\tremaining: 9.56s\n",
            "1386:\ttotal: 1m 56s\tremaining: 9.48s\n",
            "1387:\ttotal: 1m 56s\tremaining: 9.39s\n",
            "1388:\ttotal: 1m 56s\tremaining: 9.31s\n",
            "1389:\ttotal: 1m 56s\tremaining: 9.22s\n",
            "1390:\tlearn: 13509.4318039\ttest: 18499.1661258\tbest: 18499.1661258 (1390)\ttotal: 1m 56s\tremaining: 9.13s\n",
            "1391:\ttotal: 1m 56s\tremaining: 9.05s\n",
            "1392:\ttotal: 1m 56s\tremaining: 8.96s\n",
            "1393:\ttotal: 1m 56s\tremaining: 8.88s\n",
            "1394:\ttotal: 1m 56s\tremaining: 8.79s\n",
            "1395:\tlearn: 13490.1283370\ttest: 18493.0849384\tbest: 18493.0849384 (1395)\ttotal: 1m 56s\tremaining: 8.71s\n",
            "1396:\ttotal: 1m 57s\tremaining: 8.63s\n",
            "1397:\ttotal: 1m 57s\tremaining: 8.55s\n",
            "1398:\ttotal: 1m 57s\tremaining: 8.47s\n",
            "1399:\ttotal: 1m 57s\tremaining: 8.4s\n",
            "1400:\tlearn: 13469.4382862\ttest: 18485.8027844\tbest: 18485.8027844 (1400)\ttotal: 1m 57s\tremaining: 8.32s\n",
            "1401:\ttotal: 1m 57s\tremaining: 8.24s\n",
            "1402:\ttotal: 1m 58s\tremaining: 8.16s\n",
            "1403:\ttotal: 1m 58s\tremaining: 8.09s\n",
            "1404:\ttotal: 1m 58s\tremaining: 8.01s\n",
            "1405:\tlearn: 13448.4553888\ttest: 18476.8964785\tbest: 18476.8964785 (1405)\ttotal: 1m 58s\tremaining: 7.93s\n",
            "1406:\ttotal: 1m 58s\tremaining: 7.86s\n",
            "1407:\ttotal: 1m 59s\tremaining: 7.78s\n",
            "1408:\ttotal: 1m 59s\tremaining: 7.71s\n",
            "1409:\ttotal: 1m 59s\tremaining: 7.64s\n",
            "1410:\tlearn: 13424.4453014\ttest: 18473.0269849\tbest: 18472.6775658 (1409)\ttotal: 1m 59s\tremaining: 7.57s\n",
            "1411:\ttotal: 2m\tremaining: 7.49s\n",
            "1412:\ttotal: 2m\tremaining: 7.42s\n",
            "1413:\ttotal: 2m\tremaining: 7.34s\n",
            "1414:\ttotal: 2m\tremaining: 7.26s\n",
            "1415:\tlearn: 13398.7014295\ttest: 18467.7080490\tbest: 18467.7080490 (1415)\ttotal: 2m\tremaining: 7.17s\n",
            "1416:\ttotal: 2m 1s\tremaining: 7.09s\n",
            "1417:\ttotal: 2m 1s\tremaining: 7s\n",
            "1418:\ttotal: 2m 1s\tremaining: 6.92s\n",
            "1419:\ttotal: 2m 1s\tremaining: 6.83s\n",
            "1420:\tlearn: 13377.0840313\ttest: 18460.7364898\tbest: 18460.7364898 (1420)\ttotal: 2m 1s\tremaining: 6.74s\n",
            "1421:\ttotal: 2m 1s\tremaining: 6.66s\n",
            "1422:\ttotal: 2m 1s\tremaining: 6.57s\n",
            "1423:\ttotal: 2m 1s\tremaining: 6.48s\n",
            "1424:\ttotal: 2m 1s\tremaining: 6.4s\n",
            "1425:\tlearn: 13359.3091346\ttest: 18457.0015873\tbest: 18457.0015873 (1425)\ttotal: 2m 1s\tremaining: 6.31s\n",
            "1426:\ttotal: 2m 1s\tremaining: 6.22s\n",
            "1427:\ttotal: 2m 1s\tremaining: 6.14s\n",
            "1428:\ttotal: 2m 1s\tremaining: 6.05s\n",
            "1429:\ttotal: 2m 1s\tremaining: 5.97s\n",
            "1430:\tlearn: 13333.8226126\ttest: 18450.0403814\tbest: 18450.0403814 (1430)\ttotal: 2m 1s\tremaining: 5.88s\n",
            "1431:\ttotal: 2m 2s\tremaining: 5.79s\n",
            "1432:\ttotal: 2m 2s\tremaining: 5.71s\n",
            "1433:\ttotal: 2m 2s\tremaining: 5.62s\n",
            "1434:\ttotal: 2m 2s\tremaining: 5.54s\n",
            "1435:\tlearn: 13318.7243774\ttest: 18445.2093381\tbest: 18445.2093381 (1435)\ttotal: 2m 2s\tremaining: 5.45s\n",
            "1436:\ttotal: 2m 2s\tremaining: 5.36s\n",
            "1437:\ttotal: 2m 2s\tremaining: 5.28s\n",
            "1438:\ttotal: 2m 2s\tremaining: 5.19s\n",
            "1439:\ttotal: 2m 2s\tremaining: 5.11s\n",
            "1440:\tlearn: 13296.4680414\ttest: 18436.6822166\tbest: 18436.6822166 (1440)\ttotal: 2m 2s\tremaining: 5.02s\n",
            "1441:\ttotal: 2m 2s\tremaining: 4.93s\n",
            "1442:\ttotal: 2m 2s\tremaining: 4.85s\n",
            "1443:\ttotal: 2m 2s\tremaining: 4.76s\n",
            "1444:\ttotal: 2m 2s\tremaining: 4.68s\n",
            "1445:\tlearn: 13273.6326679\ttest: 18428.7995794\tbest: 18428.7995794 (1445)\ttotal: 2m 2s\tremaining: 4.59s\n",
            "1446:\ttotal: 2m 3s\tremaining: 4.51s\n",
            "1447:\ttotal: 2m 3s\tremaining: 4.42s\n",
            "1448:\ttotal: 2m 3s\tremaining: 4.33s\n",
            "1449:\ttotal: 2m 3s\tremaining: 4.25s\n",
            "1450:\tlearn: 13257.2665404\ttest: 18422.3443841\tbest: 18422.3443841 (1450)\ttotal: 2m 3s\tremaining: 4.16s\n",
            "1451:\ttotal: 2m 3s\tremaining: 4.08s\n",
            "1452:\ttotal: 2m 3s\tremaining: 3.99s\n",
            "1453:\ttotal: 2m 3s\tremaining: 3.91s\n",
            "1454:\ttotal: 2m 3s\tremaining: 3.82s\n",
            "1455:\tlearn: 13235.4650039\ttest: 18419.5309129\tbest: 18419.5309129 (1455)\ttotal: 2m 3s\tremaining: 3.74s\n",
            "1456:\ttotal: 2m 3s\tremaining: 3.65s\n",
            "1457:\ttotal: 2m 3s\tremaining: 3.57s\n",
            "1458:\ttotal: 2m 3s\tremaining: 3.48s\n",
            "1459:\ttotal: 2m 3s\tremaining: 3.4s\n",
            "1460:\tlearn: 13215.1232630\ttest: 18412.2707594\tbest: 18412.2707594 (1460)\ttotal: 2m 3s\tremaining: 3.31s\n",
            "1461:\ttotal: 2m 4s\tremaining: 3.22s\n",
            "1462:\ttotal: 2m 4s\tremaining: 3.14s\n",
            "1463:\ttotal: 2m 4s\tremaining: 3.05s\n",
            "1464:\ttotal: 2m 4s\tremaining: 2.97s\n",
            "1465:\tlearn: 13200.5552575\ttest: 18404.3622393\tbest: 18404.3622393 (1465)\ttotal: 2m 4s\tremaining: 2.88s\n",
            "1466:\ttotal: 2m 4s\tremaining: 2.8s\n",
            "1467:\ttotal: 2m 4s\tremaining: 2.71s\n",
            "1468:\ttotal: 2m 4s\tremaining: 2.63s\n",
            "1469:\ttotal: 2m 4s\tremaining: 2.54s\n",
            "1470:\tlearn: 13174.2035072\ttest: 18392.7201108\tbest: 18392.7201108 (1470)\ttotal: 2m 4s\tremaining: 2.46s\n",
            "1471:\ttotal: 2m 4s\tremaining: 2.37s\n",
            "1472:\ttotal: 2m 4s\tremaining: 2.29s\n",
            "1473:\ttotal: 2m 4s\tremaining: 2.2s\n",
            "1474:\ttotal: 2m 4s\tremaining: 2.12s\n",
            "1475:\tlearn: 13153.4114038\ttest: 18385.0859494\tbest: 18385.0859494 (1475)\ttotal: 2m 4s\tremaining: 2.03s\n",
            "1476:\ttotal: 2m 5s\tremaining: 1.95s\n",
            "1477:\ttotal: 2m 5s\tremaining: 1.86s\n",
            "1478:\ttotal: 2m 5s\tremaining: 1.78s\n",
            "1479:\ttotal: 2m 5s\tremaining: 1.69s\n",
            "1480:\tlearn: 13132.8777596\ttest: 18379.1380590\tbest: 18379.1380590 (1480)\ttotal: 2m 5s\tremaining: 1.61s\n",
            "1481:\ttotal: 2m 5s\tremaining: 1.52s\n",
            "1482:\ttotal: 2m 5s\tremaining: 1.44s\n",
            "1483:\ttotal: 2m 5s\tremaining: 1.35s\n",
            "1484:\ttotal: 2m 5s\tremaining: 1.27s\n",
            "1485:\tlearn: 13112.0035705\ttest: 18373.3066416\tbest: 18373.3066416 (1485)\ttotal: 2m 5s\tremaining: 1.18s\n",
            "1486:\ttotal: 2m 5s\tremaining: 1.1s\n",
            "1487:\ttotal: 2m 5s\tremaining: 1.01s\n",
            "1488:\ttotal: 2m 5s\tremaining: 930ms\n",
            "1489:\ttotal: 2m 5s\tremaining: 845ms\n",
            "1490:\tlearn: 13091.2658211\ttest: 18365.7721294\tbest: 18365.7721294 (1490)\ttotal: 2m 5s\tremaining: 760ms\n",
            "1491:\ttotal: 2m 6s\tremaining: 676ms\n",
            "1492:\ttotal: 2m 6s\tremaining: 591ms\n",
            "1493:\ttotal: 2m 6s\tremaining: 507ms\n",
            "1494:\ttotal: 2m 6s\tremaining: 422ms\n",
            "1495:\tlearn: 13077.3323165\ttest: 18363.2485466\tbest: 18363.2485466 (1495)\ttotal: 2m 6s\tremaining: 338ms\n",
            "1496:\ttotal: 2m 6s\tremaining: 253ms\n",
            "1497:\ttotal: 2m 6s\tremaining: 169ms\n",
            "1498:\ttotal: 2m 6s\tremaining: 84.4ms\n",
            "1499:\tlearn: 13062.3505544\ttest: 18361.7771037\tbest: 18360.6291971 (1496)\ttotal: 2m 6s\tremaining: 0us\n",
            "bestTest = 18360.6292\n",
            "bestIteration = 1496\n",
            "Shrink model to first 1497 iterations.\n",
            "18360.626298242572\n"
          ]
        }
      ],
      "source": [
        "param = {'iterations': 282, 'depth': 10, 'learning_rate': 0.09954501286322029, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "\n",
        "# param = {'iterations': 300, 'depth': 10, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 20112.9456712 ( app_id_count_less_then = 100 )\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 19323.3915496 (app_id_count_less_then = 50)\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 18463.0983 (app_id_count_less_then = 20)\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 18403.9105827 (app_id_count_less_then = 10)\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 18496.6296 (app_id_count_less_then = 0)\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 18534.48723  (app_id_count_less_then = 10) + building age\n",
        "\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 18534.48723  (app_id_count_less_then = 10) + building age\n",
        "\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 18465.83672 (app_id_count_less_then = 10) + building age + split_long_lat by city \n",
        "\n",
        "\n",
        "\n",
        "# param = {'iterations': 2500, 'depth': 10, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 20032.6684259  learn: 15917.7006972\t (app_id_count_less_then = 100)\n",
        "\n",
        "\n",
        "# param = {'iterations': 2500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 19374.3950984  learn: 11988.5400224\t (app_id_count_less_then = 100)\n",
        "\n",
        "# param = {'iterations': 2500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 17939.50018 (app_id_count_less_then = 10)\n",
        "\n",
        "\n",
        "param = {'iterations': 2500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 17958.30022 (app_id_count_less_then = 10) + building age + split_long_lat by city \n",
        "\n",
        "param = {'iterations': 2500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# test: 20490.40669429518 (app_id_count_less_then = 10) + building age + split_long_lat by city + inflation_adjust = True\n",
        "\n",
        "\n",
        "param = {'iterations': 2500, 'depth': 11, 'learning_rate': 0.0955, 'random_strength': 0, 'bagging_temperature': 0.0821, 'l2_leaf_reg': 0.7328, 'min_data_in_leaf': 43}\n",
        "# 2499:\tlearn: 10645.0477895\ttest: 17702.2251610\tbest: 17702.1992781 (2498)\ttotal: 2m 31s\tremaining: 0us\n",
        "# test: 17702.19928 (app_id_count_less_then = 10) + include everything + inflation_adjust = False\n",
        "\n",
        "param = {'iterations': 2500, 'depth': 11, 'learning_rate': 0.0995, 'random_strength': 0, 'bagging_temperature': 0.0821, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 2499:\tlearn: 11836.9464649\ttest: 17740.5875014\tbest: 17740.5875014 (2499)\ttotal: 2m 38s\tremaining: 0us\n",
        "# test: 17740.5875 (app_id_count_less_then = 10) + include everything + inflation_adjust = False\n",
        "\n",
        "param = {'iterations': 2500, 'depth': 8, 'learning_rate': 0.0995, 'random_strength': 0, 'bagging_temperature': 0.0821, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 2499:\tlearn: 17555.0244862\ttest: 19545.6143650\tbest: 19545.6143650 (2499)\ttotal: 1m 16s\tremaining: 0us\n",
        "# test: 19545.61437 (app_id_count_less_then = 10) + include everything + inflation_adjust = False\n",
        "\n",
        "\n",
        "param = {'iterations': 5000, 'depth': 8, 'learning_rate': 0.0995, 'random_strength': 0, 'bagging_temperature': 0.0821, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 4999:\tlearn: 14438.2444807\ttest: 17945.9243127\tbest: 17945.9243127 (4999)\ttotal: 2m 32s\tremaining: 0us\n",
        "# test: 17945.9243127 (app_id_count_less_then = 10) + include everything + inflation_adjust = False\n",
        "\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 1499:\tlearn: 13249.8743842\ttest: 18507.7333657\tbest: 18507.4991254 (1497)\ttotal: 2m 13s\tremaining: 0us\n",
        "# bestTest = 18507.49913\n",
        "\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 1499:\tlearn: 13249.8743842\ttest: 18507.7333657\tbest: 18507.4991254 (1497)\ttotal: 2m 13s\tremaining: 0us\n",
        "# bestTest = 18507.49913\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 1499:\tlearn: 13062.3505544\ttest: 18361.7771037\tbest: 18360.6291971 (1496)\ttotal: 2m 6s\tremaining: 0us\n",
        "# bestTest = 18360.6292 same + dong_mean_price\n",
        "\n",
        "\n",
        "model = CatBoostRegressor(**param, \n",
        "                          loss_function='RMSE', \n",
        "                          eval_metric='MAE', \n",
        "                          task_type='GPU', \n",
        "                          verbose=True)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          eval_set=(X_valid, y_valid), \n",
        "          cat_features = cat_features,\n",
        "          early_stopping_rounds=100)\n",
        "\n",
        "preds = model.predict(X_valid)\n",
        "mae = mean_absolute_error(y_valid, preds)\n",
        "print (mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"With inflation adjustment: \")\n",
        "dict(zip(X_train.columns, model.feature_importances_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgFMmopwu7BO",
        "outputId": "fb812307-15d4-4a6f-c1fb-2c7bcb73ac47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With inflation adjustment: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apartment_id': 11.797710922195014,\n",
              " 'city': 1.0221931147283352,\n",
              " 'dong': 6.2054907003061235,\n",
              " 'house_area': 29.88271193460841,\n",
              " 'built_year': 9.592707044059871,\n",
              " 'floor': 2.401163222215178,\n",
              " 'transaction_year': 4.730570131115344,\n",
              " 'transaction_month': 1.2061294513941967,\n",
              " 'transaction_day': 0.3338869088785687,\n",
              " 'dong_mean_price': 25.444356728765797,\n",
              " 'bus_lat': 0.17410130725377926,\n",
              " 'bus_long': 0.3495736607036237,\n",
              " 'seoul_lat': 3.32061318509147,\n",
              " 'seoul_long': 3.538791688684494}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QqWr5F4xb31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqhOoUZYjVBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a712487-6cea-4c4b-91d3-320488a02d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With inflation adjustment: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apartment_id': 12.554740561250904,\n",
              " 'city': 0.0,\n",
              " 'dong': 9.188655259329316,\n",
              " 'house_area': 28.252993310240647,\n",
              " 'built_year': 6.361748668906994,\n",
              " 'floor': 2.4153046451454427,\n",
              " 'transaction_year': 0.11548015426250761,\n",
              " 'transaction_month': 0.1443368600633374,\n",
              " 'transaction_day': 0.07880298224503553,\n",
              " 'transaction_time': 3.2964751759273145,\n",
              " 'building_age': 1.5176965353400445,\n",
              " 'dong_park_num': 1.2643614029197505,\n",
              " 'dong_park_area': 1.6303725710190748,\n",
              " 'dong_dcc_num': 5.650016760034021,\n",
              " 'bus_lat': 0.10217947757146853,\n",
              " 'bus_long': 0.1711439510831546,\n",
              " 'seoul_lat': 9.798234260860745,\n",
              " 'seoul_long': 17.457457423800438}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "print (\"With inflation adjustment: \")\n",
        "dict(zip(X_train.columns, model.feature_importances_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EiwDzFJIxTE",
        "outputId": "002e5feb-3c99-4fb5-93bd-0693886c872f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With inflation adjustment: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 11.670930104124329,\n",
              " 'city': 0.5542908840114052,\n",
              " 'dong': 11.069440598945443,\n",
              " 'house_area': 31.000385904459787,\n",
              " 'built_year': 6.590527397218096,\n",
              " 'floor': 3.448211703228526,\n",
              " 'transaction_year': 0.8573325165734635,\n",
              " 'transaction_month': 0.8960270229586732,\n",
              " 'transaction_day': 0.43358087492779523,\n",
              " 'transaction_time': 0.9682784886746584,\n",
              " 'building_age': 2.0172154664799304,\n",
              " 'bus_lat': 0.21773967538870106,\n",
              " 'bus_long': 0.5165064753113341,\n",
              " 'seoul_lat': 11.054393398440352,\n",
              " 'seoul_long': 18.70513948925739}"
            ]
          },
          "execution_count": 288,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print (\"With inflation adjustment: \")\n",
        "dict(zip(X_train.columns, model.feature_importances_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFPg8HGaJtgR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BazMmU8mz9tU",
        "outputId": "da882dc8-ea95-49fe-cf4e-3fd78cf2104e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 11.831978814822897,\n",
              " 'city': 0.5351234007611589,\n",
              " 'dong': 12.007148940982646,\n",
              " 'house_area': 31.22199146149635,\n",
              " 'built_year': 6.251133563492678,\n",
              " 'floor': 3.100829265026328,\n",
              " 'transaction_year': 2.499376625415747,\n",
              " 'transaction_month': 0.6874095207129042,\n",
              " 'transaction_day': 0.32865797920593454,\n",
              " 'transaction_time': 1.7826188172977009,\n",
              " 'building_age': 1.7148819255278687,\n",
              " 'bus_lat': 0.1345831395452858,\n",
              " 'bus_long': 0.42867313404817053,\n",
              " 'seoul_lat': 9.01820062601347,\n",
              " 'seoul_long': 18.45739278565083}"
            ]
          },
          "execution_count": 267,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(zip(X_train.columns, model.feature_importances_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model features select"
      ],
      "metadata": {
        "id": "gDqSMC_Oxvfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = get_train_val_data( test_size=0.3, random_state=None)\n",
        "cat_features = [\"city\",\t\"dong\", \"apartment_id\"]\n",
        "\n",
        "\n",
        "\n",
        "param = {'iterations': 1500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# 1499:\tlearn: 13062.3505544\ttest: 18361.7771037\tbest: 18360.6291971 (1496)\ttotal: 2m 6s\tremaining: 0us\n",
        "# bestTest = 18360.6292 same + dong_mean_price\n",
        "\n",
        "\n",
        "model = CatBoostRegressor(**param, \n",
        "                          loss_function='RMSE', \n",
        "                          eval_metric='MAE', \n",
        "                          task_type='GPU', \n",
        "                          cat_features = cat_features,\n",
        "                          # early_stopping_rounds=100,\n",
        "                          verbose=True, )\n",
        "\n"
      ],
      "metadata": {
        "id": "2Jb1ewmTxvVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.select_features(X_train, y_train, \n",
        "          eval_set=(X_valid, y_valid), \n",
        "          features_for_select = X_train.columns,\n",
        "          steps = 20,\n",
        "          num_features_to_select = 15,\n",
        "          )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYvToRy2xvTT",
        "outputId": "7feb084e-46a2-45cd-c529-e0396c5699b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Default metric period is 5 because MAE is/are not implemented for GPU\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step #1 out of 1\n",
            "0:\tlearn: 200274.7047400\ttest: 199890.6350410\tbest: 199890.6350410 (0)\ttotal: 123ms\tremaining: 3m 3s\n",
            "1:\ttotal: 225ms\tremaining: 2m 48s\n",
            "2:\ttotal: 300ms\tremaining: 2m 29s\n",
            "3:\ttotal: 368ms\tremaining: 2m 17s\n",
            "4:\ttotal: 426ms\tremaining: 2m 7s\n",
            "5:\tlearn: 129269.5917464\ttest: 128682.0040644\tbest: 128682.0040644 (5)\ttotal: 496ms\tremaining: 2m 3s\n",
            "6:\ttotal: 555ms\tremaining: 1m 58s\n",
            "7:\ttotal: 616ms\tremaining: 1m 54s\n",
            "8:\ttotal: 684ms\tremaining: 1m 53s\n",
            "9:\ttotal: 746ms\tremaining: 1m 51s\n",
            "10:\tlearn: 88458.6506285\ttest: 87651.3955130\tbest: 87651.3955130 (10)\ttotal: 806ms\tremaining: 1m 49s\n",
            "11:\ttotal: 882ms\tremaining: 1m 49s\n",
            "12:\ttotal: 944ms\tremaining: 1m 47s\n",
            "13:\ttotal: 1s\tremaining: 1m 46s\n",
            "14:\ttotal: 1.09s\tremaining: 1m 47s\n",
            "15:\tlearn: 66518.9659897\ttest: 65657.3856249\tbest: 65657.3856249 (15)\ttotal: 1.15s\tremaining: 1m 46s\n",
            "16:\ttotal: 1.18s\tremaining: 1m 43s\n",
            "17:\ttotal: 1.24s\tremaining: 1m 42s\n",
            "18:\ttotal: 1.31s\tremaining: 1m 42s\n",
            "19:\ttotal: 1.37s\tremaining: 1m 41s\n",
            "20:\tlearn: 55211.0434477\ttest: 54410.4217497\tbest: 54410.4217497 (20)\ttotal: 1.44s\tremaining: 1m 41s\n",
            "21:\ttotal: 1.5s\tremaining: 1m 41s\n",
            "22:\ttotal: 1.57s\tremaining: 1m 40s\n",
            "23:\ttotal: 1.64s\tremaining: 1m 40s\n",
            "24:\ttotal: 1.7s\tremaining: 1m 40s\n",
            "25:\tlearn: 48565.5570818\ttest: 47782.6185811\tbest: 47782.6185811 (25)\ttotal: 1.76s\tremaining: 1m 39s\n",
            "26:\ttotal: 1.84s\tremaining: 1m 40s\n",
            "27:\ttotal: 1.9s\tremaining: 1m 40s\n",
            "28:\ttotal: 1.97s\tremaining: 1m 40s\n",
            "29:\ttotal: 2.04s\tremaining: 1m 39s\n",
            "30:\tlearn: 44629.1939354\ttest: 43957.7367830\tbest: 43957.7367830 (30)\ttotal: 2.11s\tremaining: 1m 39s\n",
            "31:\ttotal: 2.17s\tremaining: 1m 39s\n",
            "32:\ttotal: 2.23s\tremaining: 1m 39s\n",
            "33:\ttotal: 2.29s\tremaining: 1m 38s\n",
            "34:\ttotal: 2.36s\tremaining: 1m 38s\n",
            "35:\tlearn: 42321.0574782\ttest: 41730.8652370\tbest: 41730.8652370 (35)\ttotal: 2.42s\tremaining: 1m 38s\n",
            "36:\ttotal: 2.44s\tremaining: 1m 36s\n",
            "37:\ttotal: 2.5s\tremaining: 1m 36s\n",
            "38:\ttotal: 2.57s\tremaining: 1m 36s\n",
            "39:\ttotal: 2.63s\tremaining: 1m 35s\n",
            "40:\tlearn: 40596.8259361\ttest: 40066.4756994\tbest: 40066.4756994 (40)\ttotal: 2.7s\tremaining: 1m 36s\n",
            "41:\ttotal: 2.76s\tremaining: 1m 35s\n",
            "42:\ttotal: 2.83s\tremaining: 1m 35s\n",
            "43:\ttotal: 2.89s\tremaining: 1m 35s\n",
            "44:\ttotal: 2.95s\tremaining: 1m 35s\n",
            "45:\tlearn: 39305.2440778\ttest: 38821.4693803\tbest: 38821.4693803 (45)\ttotal: 3.02s\tremaining: 1m 35s\n",
            "46:\ttotal: 3.11s\tremaining: 1m 36s\n",
            "47:\ttotal: 3.17s\tremaining: 1m 35s\n",
            "48:\ttotal: 3.24s\tremaining: 1m 35s\n",
            "49:\ttotal: 3.3s\tremaining: 1m 35s\n",
            "50:\tlearn: 37903.6096073\ttest: 37408.0042060\tbest: 37408.0042060 (50)\ttotal: 3.37s\tremaining: 1m 35s\n",
            "51:\ttotal: 3.44s\tremaining: 1m 35s\n",
            "52:\ttotal: 3.51s\tremaining: 1m 35s\n",
            "53:\ttotal: 3.57s\tremaining: 1m 35s\n",
            "54:\ttotal: 3.65s\tremaining: 1m 35s\n",
            "55:\tlearn: 36770.1604364\ttest: 36326.2180432\tbest: 36326.2180432 (55)\ttotal: 3.72s\tremaining: 1m 36s\n",
            "56:\ttotal: 3.79s\tremaining: 1m 35s\n",
            "57:\ttotal: 3.86s\tremaining: 1m 36s\n",
            "58:\ttotal: 3.93s\tremaining: 1m 36s\n",
            "59:\ttotal: 4s\tremaining: 1m 36s\n",
            "60:\tlearn: 36185.2904590\ttest: 35797.5960448\tbest: 35797.5960448 (60)\ttotal: 4.07s\tremaining: 1m 36s\n",
            "61:\ttotal: 4.13s\tremaining: 1m 35s\n",
            "62:\ttotal: 4.2s\tremaining: 1m 35s\n",
            "63:\ttotal: 4.26s\tremaining: 1m 35s\n",
            "64:\ttotal: 4.33s\tremaining: 1m 35s\n",
            "65:\tlearn: 35351.5896751\ttest: 34989.7700264\tbest: 34989.7700264 (65)\ttotal: 4.4s\tremaining: 1m 35s\n",
            "66:\ttotal: 4.46s\tremaining: 1m 35s\n",
            "67:\ttotal: 4.53s\tremaining: 1m 35s\n",
            "68:\ttotal: 4.6s\tremaining: 1m 35s\n",
            "69:\ttotal: 4.66s\tremaining: 1m 35s\n",
            "70:\tlearn: 34671.0187839\ttest: 34338.0153073\tbest: 34338.0153073 (70)\ttotal: 4.73s\tremaining: 1m 35s\n",
            "71:\ttotal: 4.79s\tremaining: 1m 34s\n",
            "72:\ttotal: 4.87s\tremaining: 1m 35s\n",
            "73:\ttotal: 4.94s\tremaining: 1m 35s\n",
            "74:\ttotal: 5s\tremaining: 1m 34s\n",
            "75:\tlearn: 34093.3035796\ttest: 33794.4355809\tbest: 33794.4355809 (75)\ttotal: 5.07s\tremaining: 1m 35s\n",
            "76:\ttotal: 5.14s\tremaining: 1m 34s\n",
            "77:\ttotal: 5.21s\tremaining: 1m 34s\n",
            "78:\ttotal: 5.26s\tremaining: 1m 34s\n",
            "79:\ttotal: 5.32s\tremaining: 1m 34s\n",
            "80:\tlearn: 33581.9580472\ttest: 33344.8056053\tbest: 33344.8056053 (80)\ttotal: 5.39s\tremaining: 1m 34s\n",
            "81:\ttotal: 5.46s\tremaining: 1m 34s\n",
            "82:\ttotal: 5.53s\tremaining: 1m 34s\n",
            "83:\ttotal: 5.6s\tremaining: 1m 34s\n",
            "84:\ttotal: 5.66s\tremaining: 1m 34s\n",
            "85:\tlearn: 33085.8028191\ttest: 32910.8451374\tbest: 32910.8451374 (85)\ttotal: 5.73s\tremaining: 1m 34s\n",
            "86:\ttotal: 5.81s\tremaining: 1m 34s\n",
            "87:\ttotal: 5.88s\tremaining: 1m 34s\n",
            "88:\ttotal: 5.95s\tremaining: 1m 34s\n",
            "89:\ttotal: 6.01s\tremaining: 1m 34s\n",
            "90:\tlearn: 32580.0014039\ttest: 32443.6820852\tbest: 32443.6820852 (90)\ttotal: 6.09s\tremaining: 1m 34s\n",
            "91:\ttotal: 6.16s\tremaining: 1m 34s\n",
            "92:\ttotal: 6.17s\tremaining: 1m 33s\n",
            "93:\ttotal: 6.23s\tremaining: 1m 33s\n",
            "94:\ttotal: 6.31s\tremaining: 1m 33s\n",
            "95:\tlearn: 32227.6160722\ttest: 32124.6442820\tbest: 32124.6442820 (95)\ttotal: 6.39s\tremaining: 1m 33s\n",
            "96:\ttotal: 6.45s\tremaining: 1m 33s\n",
            "97:\ttotal: 6.51s\tremaining: 1m 33s\n",
            "98:\ttotal: 6.58s\tremaining: 1m 33s\n",
            "99:\ttotal: 6.64s\tremaining: 1m 32s\n",
            "100:\tlearn: 31899.7632928\ttest: 31830.4178268\tbest: 31830.4178268 (100)\ttotal: 6.71s\tremaining: 1m 32s\n",
            "101:\ttotal: 6.78s\tremaining: 1m 32s\n",
            "102:\ttotal: 6.86s\tremaining: 1m 32s\n",
            "103:\ttotal: 6.94s\tremaining: 1m 33s\n",
            "104:\ttotal: 7.05s\tremaining: 1m 33s\n",
            "105:\tlearn: 31470.0842263\ttest: 31435.1755892\tbest: 31435.1755892 (105)\ttotal: 7.19s\tremaining: 1m 34s\n",
            "106:\ttotal: 7.41s\tremaining: 1m 36s\n",
            "107:\ttotal: 7.63s\tremaining: 1m 38s\n",
            "108:\ttotal: 7.86s\tremaining: 1m 40s\n",
            "109:\ttotal: 8.09s\tremaining: 1m 42s\n",
            "110:\tlearn: 30982.8208837\ttest: 30990.1579464\tbest: 30990.1579464 (110)\ttotal: 8.32s\tremaining: 1m 44s\n",
            "111:\ttotal: 8.53s\tremaining: 1m 45s\n",
            "112:\ttotal: 8.8s\tremaining: 1m 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CatBoostRegressor(**param, \n",
        "                          loss_function='RMSE', \n",
        "                          eval_metric='MAE', \n",
        "                          task_type='GPU', \n",
        "                          cat_features = cat_features,\n",
        "                          # early_stopping_rounds=100,\n",
        "                          verbose=True, )\n",
        "\n",
        "\n",
        "model.select_features(X_train, y_train, \n",
        "          eval_set=(X_valid, y_valid), \n",
        "          features_for_select = X_train.columns,\n",
        "          steps = 20,\n",
        "          num_features_to_select = 15,\n",
        "          )\n"
      ],
      "metadata": {
        "id": "IMj_xL5azPcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmfgNwNfzPZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SBgQOmGqzPXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xLnUN0RxvQ6",
        "outputId": "c642f5cf-42de-4528-8941-068238053a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIbu42TEVQXz"
      },
      "source": [
        "# run optimum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qPr4YUOMCpTy",
        "outputId": "e931b5e9-002e-4435-aec8-1a71b3fc9911"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-29 16:30:45,624]\u001b[0m A new study created in memory with name: no-name-9fe44a36-04d9-4200-bc1f-db06f1fd500f\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:31:51,479]\u001b[0m Trial 0 finished with value: 25621.595184986483 and parameters: {'depth': 9, 'learning_rate': 0.019116805238712724, 'random_strength': 43, 'bagging_temperature': 0.08390765620958907, 'od_type': 'IncToDec', 'od_wait': 40}. Best is trial 0 with value: 25621.595184986483.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:32:48,358]\u001b[0m Trial 1 finished with value: 19907.5872470091 and parameters: {'depth': 8, 'learning_rate': 0.0926821928082856, 'random_strength': 88, 'bagging_temperature': 0.023416988762345037, 'od_type': 'IncToDec', 'od_wait': 14}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:33:36,282]\u001b[0m Trial 2 finished with value: 50273.80918949669 and parameters: {'depth': 7, 'learning_rate': 0.0032827229135804587, 'random_strength': 57, 'bagging_temperature': 0.1840629097507512, 'od_type': 'Iter', 'od_wait': 28}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:34:20,742]\u001b[0m Trial 3 finished with value: 28016.179513301435 and parameters: {'depth': 7, 'learning_rate': 0.024825863155625648, 'random_strength': 38, 'bagging_temperature': 0.031010529414023737, 'od_type': 'Iter', 'od_wait': 32}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:36:11,650]\u001b[0m Trial 4 finished with value: 22714.66791498396 and parameters: {'depth': 11, 'learning_rate': 0.023966819946968487, 'random_strength': 28, 'bagging_temperature': 1.2263950296051835, 'od_type': 'Iter', 'od_wait': 31}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:40:03,031]\u001b[0m Trial 5 finished with value: 36011.56173002649 and parameters: {'depth': 13, 'learning_rate': 0.003267810054600239, 'random_strength': 27, 'bagging_temperature': 0.1457873525883153, 'od_type': 'Iter', 'od_wait': 23}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:43:58,412]\u001b[0m Trial 6 finished with value: 29312.626174359106 and parameters: {'depth': 13, 'learning_rate': 0.006637958178385959, 'random_strength': 36, 'bagging_temperature': 1.2325957459235521, 'od_type': 'Iter', 'od_wait': 10}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:50:50,323]\u001b[0m Trial 7 finished with value: 20383.544916015035 and parameters: {'depth': 14, 'learning_rate': 0.019954711101685025, 'random_strength': 59, 'bagging_temperature': 0.518234032824686, 'od_type': 'Iter', 'od_wait': 49}. Best is trial 1 with value: 19907.5872470091.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:55:11,937]\u001b[0m Trial 8 finished with value: 18661.136631268237 and parameters: {'depth': 13, 'learning_rate': 0.08487441568205932, 'random_strength': 95, 'bagging_temperature': 0.03742607457938394, 'od_type': 'IncToDec', 'od_wait': 40}. Best is trial 8 with value: 18661.136631268237.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:56:35,457]\u001b[0m Trial 9 finished with value: 44230.929714519676 and parameters: {'depth': 10, 'learning_rate': 0.0031697302390538396, 'random_strength': 64, 'bagging_temperature': 0.29711748991471537, 'od_type': 'Iter', 'od_wait': 26}. Best is trial 8 with value: 18661.136631268237.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 16:59:11,582]\u001b[0m Trial 10 finished with value: 67755.83049517545 and parameters: {'depth': 12, 'learning_rate': 0.001078714268652987, 'random_strength': 4, 'bagging_temperature': 16.975688509351734, 'od_type': 'IncToDec', 'od_wait': 50}. Best is trial 8 with value: 18661.136631268237.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:00:19,723]\u001b[0m Trial 11 finished with value: 19053.628038200728 and parameters: {'depth': 9, 'learning_rate': 0.09650809897951972, 'random_strength': 99, 'bagging_temperature': 0.011652411967860822, 'od_type': 'IncToDec', 'od_wait': 10}. Best is trial 8 with value: 18661.136631268237.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:01:46,410]\u001b[0m Trial 12 finished with value: 18492.132998317742 and parameters: {'depth': 10, 'learning_rate': 0.08667968229201857, 'random_strength': 99, 'bagging_temperature': 0.01139311838207585, 'od_type': 'IncToDec', 'od_wait': 38}. Best is trial 12 with value: 18492.132998317742.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:03:41,247]\u001b[0m Trial 13 finished with value: 18822.650685193315 and parameters: {'depth': 11, 'learning_rate': 0.051907457812395565, 'random_strength': 79, 'bagging_temperature': 0.01024985820543783, 'od_type': 'IncToDec', 'od_wait': 39}. Best is trial 12 with value: 18492.132998317742.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:05:38,186]\u001b[0m Trial 14 finished with value: 18364.55828094373 and parameters: {'depth': 11, 'learning_rate': 0.08053421185446756, 'random_strength': 100, 'bagging_temperature': 0.04599577941109508, 'od_type': 'IncToDec', 'od_wait': 41}. Best is trial 14 with value: 18364.55828094373.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:07:02,443]\u001b[0m Trial 15 finished with value: 19819.562851127506 and parameters: {'depth': 10, 'learning_rate': 0.04604295505295395, 'random_strength': 74, 'bagging_temperature': 0.057192355398018195, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 14 with value: 18364.55828094373.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:08:54,062]\u001b[0m Trial 16 finished with value: 19129.108307610528 and parameters: {'depth': 11, 'learning_rate': 0.045713363368153424, 'random_strength': 82, 'bagging_temperature': 0.010810340048720243, 'od_type': 'IncToDec', 'od_wait': 35}. Best is trial 14 with value: 18364.55828094373.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:09:31,787]\u001b[0m Trial 17 finished with value: 25836.550728630173 and parameters: {'depth': 6, 'learning_rate': 0.05587912010327423, 'random_strength': 99, 'bagging_temperature': 0.040148118197163386, 'od_type': 'IncToDec', 'od_wait': 43}. Best is trial 14 with value: 18364.55828094373.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:10:38,734]\u001b[0m Trial 18 finished with value: 22402.286089222293 and parameters: {'depth': 9, 'learning_rate': 0.033244301400212534, 'random_strength': 71, 'bagging_temperature': 0.11060310912459931, 'od_type': 'IncToDec', 'od_wait': 36}. Best is trial 14 with value: 18364.55828094373.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:13:13,228]\u001b[0m Trial 19 finished with value: 23310.71293725078 and parameters: {'depth': 12, 'learning_rate': 0.014403291189888092, 'random_strength': 89, 'bagging_temperature': 0.022895926913892462, 'od_type': 'IncToDec', 'od_wait': 22}. Best is trial 14 with value: 18364.55828094373.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:16:01,010]\u001b[0m Trial 20 finished with value: 17879.862509414797 and parameters: {'depth': 12, 'learning_rate': 0.06826314061471614, 'random_strength': 2, 'bagging_temperature': 0.0785269818685941, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 20 with value: 17879.862509414797.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:18:49,318]\u001b[0m Trial 21 finished with value: 17815.76842507288 and parameters: {'depth': 12, 'learning_rate': 0.06668074088560585, 'random_strength': 0, 'bagging_temperature': 0.08233292775467742, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:21:40,670]\u001b[0m Trial 22 finished with value: 17914.916529083286 and parameters: {'depth': 12, 'learning_rate': 0.06285727127519936, 'random_strength': 6, 'bagging_temperature': 0.07656426728915998, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:29:09,724]\u001b[0m Trial 23 finished with value: 18535.45697976495 and parameters: {'depth': 14, 'learning_rate': 0.03274024142971415, 'random_strength': 2, 'bagging_temperature': 0.33237726366549286, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:31:56,443]\u001b[0m Trial 24 finished with value: 18146.12904973445 and parameters: {'depth': 12, 'learning_rate': 0.06180543829967158, 'random_strength': 12, 'bagging_temperature': 0.09720601676488506, 'od_type': 'IncToDec', 'od_wait': 48}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:34:40,508]\u001b[0m Trial 25 finished with value: 19358.034446113426 and parameters: {'depth': 12, 'learning_rate': 0.034941789067328255, 'random_strength': 14, 'bagging_temperature': 0.5641295539206497, 'od_type': 'IncToDec', 'od_wait': 46}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:39:02,469]\u001b[0m Trial 26 finished with value: 18333.433500590214 and parameters: {'depth': 13, 'learning_rate': 0.06307877056098145, 'random_strength': 15, 'bagging_temperature': 0.16674957786179273, 'od_type': 'IncToDec', 'od_wait': 43}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:46:25,646]\u001b[0m Trial 27 finished with value: 18208.963977755266 and parameters: {'depth': 14, 'learning_rate': 0.039057480046176055, 'random_strength': 0, 'bagging_temperature': 0.086664816552, 'od_type': 'IncToDec', 'od_wait': 35}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:49:13,525]\u001b[0m Trial 28 finished with value: 18123.38761776054 and parameters: {'depth': 12, 'learning_rate': 0.06460351176490207, 'random_strength': 9, 'bagging_temperature': 0.22023475001502443, 'od_type': 'IncToDec', 'od_wait': 47}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:53:16,112]\u001b[0m Trial 29 finished with value: 21652.471535476627 and parameters: {'depth': 13, 'learning_rate': 0.014519866423912469, 'random_strength': 22, 'bagging_temperature': 0.0680052668968489, 'od_type': 'IncToDec', 'od_wait': 42}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:55:05,766]\u001b[0m Trial 30 finished with value: 20011.20135889517 and parameters: {'depth': 11, 'learning_rate': 0.029882931807418803, 'random_strength': 18, 'bagging_temperature': 0.08152476391139298, 'od_type': 'IncToDec', 'od_wait': 37}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 17:57:52,898]\u001b[0m Trial 31 finished with value: 17994.199149439755 and parameters: {'depth': 12, 'learning_rate': 0.06579538787397186, 'random_strength': 7, 'bagging_temperature': 0.2163659508279135, 'od_type': 'IncToDec', 'od_wait': 47}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:00:40,637]\u001b[0m Trial 32 finished with value: 18041.49594205651 and parameters: {'depth': 12, 'learning_rate': 0.06817536222405614, 'random_strength': 8, 'bagging_temperature': 0.14539226051820497, 'od_type': 'IncToDec', 'od_wait': 50}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:02:06,170]\u001b[0m Trial 33 finished with value: 18018.04308988609 and parameters: {'depth': 10, 'learning_rate': 0.09815579357467916, 'random_strength': 6, 'bagging_temperature': 0.06432770861347635, 'od_type': 'IncToDec', 'od_wait': 44}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:06:19,399]\u001b[0m Trial 34 finished with value: 18304.72759302547 and parameters: {'depth': 13, 'learning_rate': 0.04563777243578699, 'random_strength': 23, 'bagging_temperature': 0.025614709034497158, 'od_type': 'IncToDec', 'od_wait': 47}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:08:13,497]\u001b[0m Trial 35 finished with value: 18441.25673074315 and parameters: {'depth': 11, 'learning_rate': 0.07025912753795407, 'random_strength': 43, 'bagging_temperature': 0.2642805699057351, 'od_type': 'IncToDec', 'od_wait': 41}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:10:57,061]\u001b[0m Trial 36 finished with value: 18230.44666506132 and parameters: {'depth': 12, 'learning_rate': 0.042890679174864076, 'random_strength': 0, 'bagging_temperature': 0.11596979276951253, 'od_type': 'IncToDec', 'od_wait': 33}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:11:50,098]\u001b[0m Trial 37 finished with value: 25147.81632757598 and parameters: {'depth': 8, 'learning_rate': 0.02739654318289603, 'random_strength': 33, 'bagging_temperature': 0.021046141920361954, 'od_type': 'Iter', 'od_wait': 29}. Best is trial 21 with value: 17815.76842507288.\u001b[0m\n",
            "<ipython-input-322-e7ca9a36cb3d>:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
            "<ipython-input-322-e7ca9a36cb3d>:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "<ipython-input-322-e7ca9a36cb3d>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('learning_rate', 0.1, 100),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/trial/_trial.py:681: RuntimeWarning: Inconsistent parameter values for distribution with name \"learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.001, 'high': 0.1, 'log': True}\n",
            "  warnings.warn(\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[33m[W 2023-05-29 18:13:41,409]\u001b[0m Trial 38 failed with parameters: {'depth': 13, 'learning_rate': 0.05742362578062215, 'random_strength': 17, 'bagging_temperature': 0.1647347646142478, 'od_type': 'Iter', 'od_wait': 19} because of the following error: KeyboardInterrupt('').\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-322-e7ca9a36cb3d>\", line 33, in objective\n",
            "    model.fit(X_train, y_train,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 5734, in fit\n",
            "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 2357, in _fit\n",
            "    self._train(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 1761, in _train\n",
            "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
            "  File \"_catboost.pyx\", line 4624, in _catboost._CatBoost._train\n",
            "  File \"_catboost.pyx\", line 4673, in _catboost._CatBoost._train\n",
            "KeyboardInterrupt\n",
            "\u001b[33m[W 2023-05-29 18:13:41,411]\u001b[0m Trial 38 failed with value None.\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-322-e7ca9a36cb3d>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# you should minimize the MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best trial: score {}, params {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-322-e7ca9a36cb3d>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m                               verbose=False)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     model.fit(X_train, y_train, \n\u001b[0m\u001b[1;32m     34\u001b[0m               \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m               \u001b[0mcat_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5732\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5734\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5735\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5736\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             self._train(\n\u001b[0m\u001b[1;32m   2358\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from catboost import CatBoostRegressor\n",
        "# from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load some data\n",
        "# Load some data\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = get_train_val_data( test_size=0.3, random_state=42)\n",
        "cat_features = [\"city\",\t\"dong\", \"apartment_id\"]\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'iterations': 2500,\n",
        "        'depth': trial.suggest_int('depth', 10, 14),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
        "        'random_strength': trial.suggest_int('random_strength', 0, 100),\n",
        "        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 0.1),\n",
        "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
        "    }\n",
        "    \n",
        "    model = CatBoostRegressor(**param, \n",
        "                              loss_function='RMSE', \n",
        "                              eval_metric='MAE', \n",
        "                              task_type='GPU', \n",
        "                              verbose=False)\n",
        "\n",
        "    model.fit(X_train, y_train, \n",
        "              eval_set=(X_valid, y_valid), \n",
        "              cat_features = cat_features,\n",
        "              early_stopping_rounds=100)\n",
        "\n",
        "    preds = model.predict(X_valid)\n",
        "    mae = mean_absolute_error(y_valid, preds)\n",
        "    \n",
        "    return mae\n",
        "\n",
        "study = optuna.create_study(direction='minimize') # you should minimize the MAE\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwMWHknje-HY"
      },
      "outputs": [],
      "source": [
        "# Trial 14 finished with value: 18364.55828094373 and parameters: {'depth': 11, 'learning_rate': 0.08053421185446756, 'random_strength': 100, 'bagging_temperature': 0.04599577941109508, 'od_type': 'IncToDec', 'od_wait': 41}. Best is trial 14 with value: 18364.55828094373.\n",
        "# Trial 20 finished with value: 17879.86250941479 and parameters: {'depth': 12, 'learning_rate': 0.06826314061471614, 'random_strength': 2, 'bagging_temperature': 0.0785269818685941, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 20 with value: 17879.862509414797.\n",
        "# Trial 21 finished with value: 17815.76842507288 and parameters: {'depth': 12, 'learning_rate': 0.06668074088560585, 'random_strength': 0, 'bagging_temperature': 0.08233292775467742, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 21 with value: 17815.76842507288.\n",
        "# Trial 22 finished with value: 17914.91652908328 and parameters: {'depth': 12, 'learning_rate': 0.06285727127519936, 'random_strength': 6, 'bagging_temperature': 0.07656426728915998, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 21 with value: 17815.76842507288.\n",
        "# Trial 23 finished with value: 18535.45697976495 and parameters: {'depth': 14, 'learning_rate': 0.03274024142971415, 'random_strength': 2, 'bagging_temperature': 0.33237726366549286, 'od_type': 'IncToDec', 'od_wait': 45}. Best is trial 21 with value: 17815.76842507288.\n",
        "# Trial 24 finished with value: 18146.12904973445 and parameters: {'depth': 12, 'learning_rate': 0.06180543829967158, 'random_strength': 12, 'bagging_temperature': 0.09720601676488506, 'od_type': 'IncToDec', 'od_wait': 48}. Best is trial 21 with value: 17815.76842507288.\n",
        "# Trial 25 finished with value: 19358.03444611342 and parameters: {'depth': 12, 'learning_rate': 0.034941789067328255, 'random_strength': 14, 'bagging_temperature': 0.5641295539206497, 'od_type': 'IncToDec', 'od_wait': 46}. Best is trial 21 with value: 17815.76842507288.\n",
        "# Trial 26 finished with value: 18333.43350059021 and parameters: {'depth': 13, 'learning_rate': 0.06307877056098145, 'random_strength': 15, 'bagging_temperature': 0.16674957786179273, 'od_type': 'IncToDec', 'od_wait': 43}. Best is trial 21 with value: 17815.76842507288.\n",
        "# Trial 27 finished with value: 18208.96397775526 and parameters: {'depth': 14, 'learning_rate': 0.039057480046176055, 'random_strength': 0, 'bagging_temperature': 0.086664816552, 'od_type': 'IncToDec', 'od_wait': 35}. Best is trial 21 with value: 17815.76842507288.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ-nih2ahbSx",
        "outputId": "33cb51f1-e3f7-4811-f86a-8f0efe3e4e3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-29 18:15:39,544]\u001b[0m A new study created in memory with name: no-name-4045670d-ada3-40f6-80d9-725742748310\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:17:08,180]\u001b[0m Trial 0 finished with value: 18883.922752923307 and parameters: {'depth': 10, 'learning_rate': 0.08375129644025683, 'random_strength': 92, 'bagging_temperature': 0.263000547283569, 'l2_leaf_reg': 0.9591791031398794, 'min_data_in_leaf': 1}. Best is trial 0 with value: 18883.922752923307.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:18:30,783]\u001b[0m Trial 1 finished with value: 22028.781157721554 and parameters: {'depth': 10, 'learning_rate': 0.03049474085179433, 'random_strength': 38, 'bagging_temperature': 0.03457269726693917, 'l2_leaf_reg': 2.2860692910537543, 'min_data_in_leaf': 82}. Best is trial 0 with value: 18883.922752923307.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:22:43,940]\u001b[0m Trial 2 finished with value: 18765.76313046196 and parameters: {'depth': 13, 'learning_rate': 0.04331673367646232, 'random_strength': 91, 'bagging_temperature': 0.13313021716142676, 'l2_leaf_reg': 0.38898930165240697, 'min_data_in_leaf': 90}. Best is trial 2 with value: 18765.76313046196.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:24:09,347]\u001b[0m Trial 3 finished with value: 19855.137592789237 and parameters: {'depth': 10, 'learning_rate': 0.05491356128437887, 'random_strength': 82, 'bagging_temperature': 0.08348682640104015, 'l2_leaf_reg': 2.1181911943405773, 'min_data_in_leaf': 19}. Best is trial 2 with value: 18765.76313046196.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:28:31,642]\u001b[0m Trial 4 finished with value: 18728.69726637476 and parameters: {'depth': 13, 'learning_rate': 0.09728067342545592, 'random_strength': 64, 'bagging_temperature': 0.05620788284677436, 'l2_leaf_reg': 1.8279290163135116, 'min_data_in_leaf': 44}. Best is trial 4 with value: 18728.69726637476.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:35:36,546]\u001b[0m Trial 5 finished with value: 19036.870835594753 and parameters: {'depth': 14, 'learning_rate': 0.04432708665105404, 'random_strength': 61, 'bagging_temperature': 0.03162962932999269, 'l2_leaf_reg': 3.761932289752509, 'min_data_in_leaf': 95}. Best is trial 4 with value: 18728.69726637476.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:42:45,162]\u001b[0m Trial 6 finished with value: 18807.132656307436 and parameters: {'depth': 14, 'learning_rate': 0.03908067916376411, 'random_strength': 39, 'bagging_temperature': 0.18622434485069247, 'l2_leaf_reg': 0.3115444698507499, 'min_data_in_leaf': 35}. Best is trial 4 with value: 18728.69726637476.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:44:12,139]\u001b[0m Trial 7 finished with value: 17971.010119369847 and parameters: {'depth': 10, 'learning_rate': 0.09832153800860591, 'random_strength': 4, 'bagging_temperature': 0.06764535579317005, 'l2_leaf_reg': 0.3593676275123288, 'min_data_in_leaf': 88}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:48:17,246]\u001b[0m Trial 8 finished with value: 22384.179865986385 and parameters: {'depth': 13, 'learning_rate': 0.012869976907380403, 'random_strength': 14, 'bagging_temperature': 0.2890622468287521, 'l2_leaf_reg': 0.3190982658843398, 'min_data_in_leaf': 93}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:50:08,498]\u001b[0m Trial 9 finished with value: 19724.400784660924 and parameters: {'depth': 11, 'learning_rate': 0.05559339967377549, 'random_strength': 90, 'bagging_temperature': 0.2015280309729588, 'l2_leaf_reg': 5.024568631213304, 'min_data_in_leaf': 1}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:51:55,878]\u001b[0m Trial 10 finished with value: 23498.58141244519 and parameters: {'depth': 11, 'learning_rate': 0.024292495856896943, 'random_strength': 4, 'bagging_temperature': 0.08124695900816106, 'l2_leaf_reg': 15.800211409211062, 'min_data_in_leaf': 68}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:54:42,013]\u001b[0m Trial 11 finished with value: 18501.58299166307 and parameters: {'depth': 12, 'learning_rate': 0.09935040266978726, 'random_strength': 63, 'bagging_temperature': 0.05890406453495976, 'l2_leaf_reg': 0.8609882036774773, 'min_data_in_leaf': 56}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:57:27,983]\u001b[0m Trial 12 finished with value: 18208.14470675834 and parameters: {'depth': 12, 'learning_rate': 0.09803581907571231, 'random_strength': 22, 'bagging_temperature': 0.05694322712357541, 'l2_leaf_reg': 0.7339163329444337, 'min_data_in_leaf': 63}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 18:59:20,551]\u001b[0m Trial 13 finished with value: 18374.141105076305 and parameters: {'depth': 11, 'learning_rate': 0.06908280371474983, 'random_strength': 21, 'bagging_temperature': 0.05021065205628588, 'l2_leaf_reg': 0.6725131155893107, 'min_data_in_leaf': 71}. Best is trial 7 with value: 17971.010119369847.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:02:07,411]\u001b[0m Trial 14 finished with value: 17911.512670539592 and parameters: {'depth': 12, 'learning_rate': 0.07120491279083214, 'random_strength': 2, 'bagging_temperature': 0.09921329095541036, 'l2_leaf_reg': 0.5712336490547983, 'min_data_in_leaf': 72}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:04:00,629]\u001b[0m Trial 15 finished with value: 18047.96794128245 and parameters: {'depth': 11, 'learning_rate': 0.06602262446956517, 'random_strength': 0, 'bagging_temperature': 0.10922022095862918, 'l2_leaf_reg': 0.5317756573600533, 'min_data_in_leaf': 78}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:06:45,546]\u001b[0m Trial 16 finished with value: 18337.204175112292 and parameters: {'depth': 12, 'learning_rate': 0.07082718324435705, 'random_strength': 36, 'bagging_temperature': 0.10805755032745223, 'l2_leaf_reg': 1.0732101794675728, 'min_data_in_leaf': 99}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:08:12,000]\u001b[0m Trial 17 finished with value: 18353.73169272731 and parameters: {'depth': 10, 'learning_rate': 0.07581861680530046, 'random_strength': 10, 'bagging_temperature': 0.07711996678306073, 'l2_leaf_reg': 0.4933663932742684, 'min_data_in_leaf': 82}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:12:30,820]\u001b[0m Trial 18 finished with value: 18392.013898764137 and parameters: {'depth': 13, 'learning_rate': 0.06340102058120123, 'random_strength': 25, 'bagging_temperature': 0.13620412458037837, 'l2_leaf_reg': 0.3042633705621194, 'min_data_in_leaf': 47}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:14:24,062]\u001b[0m Trial 19 finished with value: 18384.35126026292 and parameters: {'depth': 11, 'learning_rate': 0.0847564821656347, 'random_strength': 49, 'bagging_temperature': 0.06902857523066654, 'l2_leaf_reg': 1.2642207740000266, 'min_data_in_leaf': 59}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:17:08,006]\u001b[0m Trial 20 finished with value: 18239.88135989131 and parameters: {'depth': 12, 'learning_rate': 0.05375316990316205, 'random_strength': 11, 'bagging_temperature': 0.04520731379877541, 'l2_leaf_reg': 0.5287965027454941, 'min_data_in_leaf': 75}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:19:02,216]\u001b[0m Trial 21 finished with value: 17987.495033403186 and parameters: {'depth': 11, 'learning_rate': 0.07454609459212867, 'random_strength': 1, 'bagging_temperature': 0.09981309345587357, 'l2_leaf_reg': 0.5621810615321408, 'min_data_in_leaf': 86}. Best is trial 14 with value: 17911.512670539592.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:20:57,449]\u001b[0m Trial 22 finished with value: 17808.0704282241 and parameters: {'depth': 11, 'learning_rate': 0.08277165715420134, 'random_strength': 2, 'bagging_temperature': 0.09606065391621076, 'l2_leaf_reg': 0.5101421491258492, 'min_data_in_leaf': 88}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:22:23,045]\u001b[0m Trial 23 finished with value: 18338.838675080606 and parameters: {'depth': 10, 'learning_rate': 0.08415316568898348, 'random_strength': 16, 'bagging_temperature': 0.09142965857322957, 'l2_leaf_reg': 0.4560872247200737, 'min_data_in_leaf': 69}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:24:19,686]\u001b[0m Trial 24 finished with value: 17931.73768739259 and parameters: {'depth': 11, 'learning_rate': 0.09929214479272114, 'random_strength': 6, 'bagging_temperature': 0.06947885490442643, 'l2_leaf_reg': 1.2713552518824647, 'min_data_in_leaf': 100}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:27:02,363]\u001b[0m Trial 25 finished with value: 18532.76003633747 and parameters: {'depth': 12, 'learning_rate': 0.05931463362541354, 'random_strength': 29, 'bagging_temperature': 0.12171827298138167, 'l2_leaf_reg': 1.3306261634409613, 'min_data_in_leaf': 100}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:28:57,216]\u001b[0m Trial 26 finished with value: 18098.793581660935 and parameters: {'depth': 11, 'learning_rate': 0.07804069246948417, 'random_strength': 11, 'bagging_temperature': 0.09636826243253599, 'l2_leaf_reg': 0.750282908956746, 'min_data_in_leaf': 79}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:31:40,508]\u001b[0m Trial 27 finished with value: 18422.053057441815 and parameters: {'depth': 12, 'learning_rate': 0.06346418333238542, 'random_strength': 30, 'bagging_temperature': 0.0707987166443029, 'l2_leaf_reg': 1.4358710887260804, 'min_data_in_leaf': 89}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:33:36,181]\u001b[0m Trial 28 finished with value: 17999.429812201757 and parameters: {'depth': 11, 'learning_rate': 0.08605767030688205, 'random_strength': 7, 'bagging_temperature': 0.14938663704013946, 'l2_leaf_reg': 0.8983404042890858, 'min_data_in_leaf': 32}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:35:30,620]\u001b[0m Trial 29 finished with value: 18162.715694525017 and parameters: {'depth': 11, 'learning_rate': 0.08331765989868845, 'random_strength': 19, 'bagging_temperature': 0.08894013167162088, 'l2_leaf_reg': 0.9442936613272883, 'min_data_in_leaf': 97}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:38:11,572]\u001b[0m Trial 30 finished with value: 18754.337125838243 and parameters: {'depth': 12, 'learning_rate': 0.04939911195144501, 'random_strength': 76, 'bagging_temperature': 0.10883500382613791, 'l2_leaf_reg': 0.6275673736349515, 'min_data_in_leaf': 73}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:39:38,044]\u001b[0m Trial 31 finished with value: 17911.833314244723 and parameters: {'depth': 10, 'learning_rate': 0.0994999589293261, 'random_strength': 0, 'bagging_temperature': 0.0722885705989139, 'l2_leaf_reg': 0.4054696814552194, 'min_data_in_leaf': 86}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:41:03,917]\u001b[0m Trial 32 finished with value: 18120.477526367755 and parameters: {'depth': 10, 'learning_rate': 0.08652041298167809, 'random_strength': 0, 'bagging_temperature': 0.07328591060074852, 'l2_leaf_reg': 0.5039406514486354, 'min_data_in_leaf': 84}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:42:29,762]\u001b[0m Trial 33 finished with value: 18453.48422041157 and parameters: {'depth': 10, 'learning_rate': 0.07192466689164417, 'random_strength': 8, 'bagging_temperature': 0.087578347428582, 'l2_leaf_reg': 0.41257565072689867, 'min_data_in_leaf': 91}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:43:57,185]\u001b[0m Trial 34 finished with value: 18186.13090204508 and parameters: {'depth': 10, 'learning_rate': 0.08901578474488349, 'random_strength': 14, 'bagging_temperature': 0.042937572618200574, 'l2_leaf_reg': 0.4194418846589981, 'min_data_in_leaf': 84}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:45:49,691]\u001b[0m Trial 35 finished with value: 18294.722918281746 and parameters: {'depth': 11, 'learning_rate': 0.07560700910554968, 'random_strength': 47, 'bagging_temperature': 0.06409293682378843, 'l2_leaf_reg': 0.6953555526369284, 'min_data_in_leaf': 63}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:50:15,976]\u001b[0m Trial 36 finished with value: 18259.142733499728 and parameters: {'depth': 13, 'learning_rate': 0.09847668906163481, 'random_strength': 6, 'bagging_temperature': 0.08539125075038813, 'l2_leaf_reg': 1.0083653846009506, 'min_data_in_leaf': 78}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:51:39,409]\u001b[0m Trial 37 finished with value: 19075.3660918349 and parameters: {'depth': 10, 'learning_rate': 0.059171282452444685, 'random_strength': 17, 'bagging_temperature': 0.07720751049955288, 'l2_leaf_reg': 0.4110128541572457, 'min_data_in_leaf': 94}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:53:03,272]\u001b[0m Trial 38 finished with value: 19167.989543025185 and parameters: {'depth': 10, 'learning_rate': 0.06690953779166804, 'random_strength': 27, 'bagging_temperature': 0.06335896805143088, 'l2_leaf_reg': 1.909016111943312, 'min_data_in_leaf': 89}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 19:55:49,492]\u001b[0m Trial 39 finished with value: 18467.77961720407 and parameters: {'depth': 12, 'learning_rate': 0.08970249850145662, 'random_strength': 56, 'bagging_temperature': 0.07742496915479428, 'l2_leaf_reg': 0.3584284040143257, 'min_data_in_leaf': 100}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:00:09,285]\u001b[0m Trial 40 finished with value: 18516.362318094165 and parameters: {'depth': 13, 'learning_rate': 0.07869556133550297, 'random_strength': 34, 'bagging_temperature': 0.10050320190072988, 'l2_leaf_reg': 0.6169967354290286, 'min_data_in_leaf': 38}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:01:36,468]\u001b[0m Trial 41 finished with value: 18017.48574915181 and parameters: {'depth': 10, 'learning_rate': 0.0990515551247761, 'random_strength': 4, 'bagging_temperature': 0.0684688354177033, 'l2_leaf_reg': 0.3682194583927106, 'min_data_in_leaf': 88}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:03:02,195]\u001b[0m Trial 42 finished with value: 18110.417865566225 and parameters: {'depth': 10, 'learning_rate': 0.09234854779987055, 'random_strength': 4, 'bagging_temperature': 0.0627840175758817, 'l2_leaf_reg': 0.31421843995509785, 'min_data_in_leaf': 94}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:04:58,747]\u001b[0m Trial 43 finished with value: 18034.122170259718 and parameters: {'depth': 11, 'learning_rate': 0.0999476085585338, 'random_strength': 13, 'bagging_temperature': 0.05344209498579429, 'l2_leaf_reg': 0.7873710057348211, 'min_data_in_leaf': 81}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:06:22,833]\u001b[0m Trial 44 finished with value: 18773.62699671222 and parameters: {'depth': 10, 'learning_rate': 0.07941324557036736, 'random_strength': 98, 'bagging_temperature': 0.08662293181032689, 'l2_leaf_reg': 0.4135818830868249, 'min_data_in_leaf': 93}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:12:20,183]\u001b[0m Trial 45 finished with value: 18423.532588595946 and parameters: {'depth': 14, 'learning_rate': 0.0893197797420363, 'random_strength': 5, 'bagging_temperature': 0.07912903546248855, 'l2_leaf_reg': 0.6065047126867328, 'min_data_in_leaf': 5}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:14:15,019]\u001b[0m Trial 46 finished with value: 17955.4248291758 and parameters: {'depth': 11, 'learning_rate': 0.06997034238563077, 'random_strength': 1, 'bagging_temperature': 0.06011605771114854, 'l2_leaf_reg': 0.3586978874785122, 'min_data_in_leaf': 75}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:16:08,894]\u001b[0m Trial 47 finished with value: 17906.569942075213 and parameters: {'depth': 11, 'learning_rate': 0.07007550289513564, 'random_strength': 0, 'bagging_temperature': 0.057852945340061156, 'l2_leaf_reg': 0.45294729980535314, 'min_data_in_leaf': 66}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:17:58,654]\u001b[0m Trial 48 finished with value: 19507.732999058455 and parameters: {'depth': 11, 'learning_rate': 0.039541401382420564, 'random_strength': 22, 'bagging_temperature': 0.05020944469643289, 'l2_leaf_reg': 0.8173841014631693, 'min_data_in_leaf': 52}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:20:39,919]\u001b[0m Trial 49 finished with value: 18765.901158055116 and parameters: {'depth': 12, 'learning_rate': 0.048398752103079305, 'random_strength': 70, 'bagging_temperature': 0.07056749366788158, 'l2_leaf_reg': 0.5422686432453474, 'min_data_in_leaf': 60}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:24:57,566]\u001b[0m Trial 50 finished with value: 18493.454882900733 and parameters: {'depth': 13, 'learning_rate': 0.06167885554975191, 'random_strength': 44, 'bagging_temperature': 0.08175101844994095, 'l2_leaf_reg': 0.474585397621172, 'min_data_in_leaf': 67}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:26:50,794]\u001b[0m Trial 51 finished with value: 17954.59796217534 and parameters: {'depth': 11, 'learning_rate': 0.0694416079840702, 'random_strength': 0, 'bagging_temperature': 0.0586786402556756, 'l2_leaf_reg': 0.3055288966853314, 'min_data_in_leaf': 75}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:28:45,662]\u001b[0m Trial 52 finished with value: 17993.39365872685 and parameters: {'depth': 11, 'learning_rate': 0.069256615765282, 'random_strength': 8, 'bagging_temperature': 0.057701919278319795, 'l2_leaf_reg': 0.3187731340342773, 'min_data_in_leaf': 75}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:30:40,028]\u001b[0m Trial 53 finished with value: 17890.25322017723 and parameters: {'depth': 11, 'learning_rate': 0.07990371158786662, 'random_strength': 1, 'bagging_temperature': 0.06321194977024845, 'l2_leaf_reg': 0.6767543509522637, 'min_data_in_leaf': 66}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:33:26,370]\u001b[0m Trial 54 finished with value: 18074.825559884546 and parameters: {'depth': 12, 'learning_rate': 0.08020997691461257, 'random_strength': 10, 'bagging_temperature': 0.06685182621003169, 'l2_leaf_reg': 0.7216072456533582, 'min_data_in_leaf': 66}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:35:21,847]\u001b[0m Trial 55 finished with value: 17866.860252029714 and parameters: {'depth': 11, 'learning_rate': 0.091510235619994, 'random_strength': 3, 'bagging_temperature': 0.07448416741540012, 'l2_leaf_reg': 0.6183672457833982, 'min_data_in_leaf': 64}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:37:16,099]\u001b[0m Trial 56 finished with value: 18009.36633067192 and parameters: {'depth': 11, 'learning_rate': 0.0903063497656856, 'random_strength': 14, 'bagging_temperature': 0.09502214011586896, 'l2_leaf_reg': 0.47603419330609614, 'min_data_in_leaf': 55}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:39:10,795]\u001b[0m Trial 57 finished with value: 17979.811040964865 and parameters: {'depth': 11, 'learning_rate': 0.07455818597497907, 'random_strength': 3, 'bagging_temperature': 0.07670788192857704, 'l2_leaf_reg': 0.6088566460394218, 'min_data_in_leaf': 62}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:41:02,449]\u001b[0m Trial 58 finished with value: 18439.211424548754 and parameters: {'depth': 11, 'learning_rate': 0.058160543478804186, 'random_strength': 17, 'bagging_temperature': 0.05419797942107205, 'l2_leaf_reg': 0.5743333822355066, 'min_data_in_leaf': 71}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:43:46,479]\u001b[0m Trial 59 finished with value: 18183.298940585155 and parameters: {'depth': 12, 'learning_rate': 0.06456043402425952, 'random_strength': 11, 'bagging_temperature': 0.08193300315563445, 'l2_leaf_reg': 0.8484232226423315, 'min_data_in_leaf': 46}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:45:41,633]\u001b[0m Trial 60 finished with value: 17923.46372526354 and parameters: {'depth': 11, 'learning_rate': 0.08269401251036501, 'random_strength': 3, 'bagging_temperature': 0.11677919850503249, 'l2_leaf_reg': 0.6941762258976538, 'min_data_in_leaf': 56}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:47:37,256]\u001b[0m Trial 61 finished with value: 17925.44531350506 and parameters: {'depth': 11, 'learning_rate': 0.0827579381225635, 'random_strength': 0, 'bagging_temperature': 0.12249142810522087, 'l2_leaf_reg': 1.088852924496593, 'min_data_in_leaf': 57}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:49:31,736]\u001b[0m Trial 62 finished with value: 18062.8957093563 and parameters: {'depth': 11, 'learning_rate': 0.0754276322752668, 'random_strength': 7, 'bagging_temperature': 0.09999181940422558, 'l2_leaf_reg': 0.4688500951677349, 'min_data_in_leaf': 64}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:51:27,520]\u001b[0m Trial 63 finished with value: 17846.42851074192 and parameters: {'depth': 11, 'learning_rate': 0.09072902744799137, 'random_strength': 3, 'bagging_temperature': 0.09003983751953566, 'l2_leaf_reg': 0.6828651576742352, 'min_data_in_leaf': 51}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:54:15,029]\u001b[0m Trial 64 finished with value: 18060.016172413212 and parameters: {'depth': 12, 'learning_rate': 0.09079623334672567, 'random_strength': 9, 'bagging_temperature': 0.09189983182902353, 'l2_leaf_reg': 0.5474816446800893, 'min_data_in_leaf': 51}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:56:09,968]\u001b[0m Trial 65 finished with value: 17808.267311728192 and parameters: {'depth': 11, 'learning_rate': 0.0936195410468612, 'random_strength': 3, 'bagging_temperature': 0.07349565234440127, 'l2_leaf_reg': 0.6762581714892636, 'min_data_in_leaf': 39}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:58:04,926]\u001b[0m Trial 66 finished with value: 18166.865027765853 and parameters: {'depth': 11, 'learning_rate': 0.07294773598928656, 'random_strength': 13, 'bagging_temperature': 0.08443793366791713, 'l2_leaf_reg': 0.9121549752106977, 'min_data_in_leaf': 28}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 20:59:56,674]\u001b[0m Trial 67 finished with value: 18422.53689642021 and parameters: {'depth': 11, 'learning_rate': 0.06476602737254827, 'random_strength': 23, 'bagging_temperature': 0.10194495292072539, 'l2_leaf_reg': 0.6853816806727396, 'min_data_in_leaf': 47}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:01:53,148]\u001b[0m Trial 68 finished with value: 17848.50363362891 and parameters: {'depth': 11, 'learning_rate': 0.09268507942079059, 'random_strength': 3, 'bagging_temperature': 0.09083259779987603, 'l2_leaf_reg': 0.6459291984111488, 'min_data_in_leaf': 39}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:03:49,150]\u001b[0m Trial 69 finished with value: 17964.830589613186 and parameters: {'depth': 11, 'learning_rate': 0.09395027247151945, 'random_strength': 20, 'bagging_temperature': 0.07298341363095917, 'l2_leaf_reg': 0.7872644782581759, 'min_data_in_leaf': 40}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:05:44,261]\u001b[0m Trial 70 finished with value: 17960.433107466706 and parameters: {'depth': 11, 'learning_rate': 0.08248220264068978, 'random_strength': 6, 'bagging_temperature': 0.06510329769795012, 'l2_leaf_reg': 1.0852590870503303, 'min_data_in_leaf': 26}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:07:38,185]\u001b[0m Trial 71 finished with value: 17893.00864095199 and parameters: {'depth': 11, 'learning_rate': 0.0783113997847748, 'random_strength': 3, 'bagging_temperature': 0.09177430656178401, 'l2_leaf_reg': 0.5209187557576072, 'min_data_in_leaf': 43}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:09:34,813]\u001b[0m Trial 72 finished with value: 17872.74155761286 and parameters: {'depth': 11, 'learning_rate': 0.09313877551732115, 'random_strength': 3, 'bagging_temperature': 0.09141900841928616, 'l2_leaf_reg': 0.6524650258441544, 'min_data_in_leaf': 42}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:11:27,832]\u001b[0m Trial 73 finished with value: 18047.502373788648 and parameters: {'depth': 11, 'learning_rate': 0.08542454298664394, 'random_strength': 9, 'bagging_temperature': 0.09084310532133859, 'l2_leaf_reg': 0.6455781551594243, 'min_data_in_leaf': 42}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:13:24,067]\u001b[0m Trial 74 finished with value: 17850.970306932326 and parameters: {'depth': 11, 'learning_rate': 0.0923315918173952, 'random_strength': 4, 'bagging_temperature': 0.09539019772704842, 'l2_leaf_reg': 0.7593413774873884, 'min_data_in_leaf': 38}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:15:19,557]\u001b[0m Trial 75 finished with value: 18084.846397814516 and parameters: {'depth': 11, 'learning_rate': 0.09397991833693652, 'random_strength': 12, 'bagging_temperature': 0.10550064823764234, 'l2_leaf_reg': 0.7672166236926339, 'min_data_in_leaf': 36}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:17:14,062]\u001b[0m Trial 76 finished with value: 18119.881793583736 and parameters: {'depth': 11, 'learning_rate': 0.09336226443929523, 'random_strength': 16, 'bagging_temperature': 0.09638074232078807, 'l2_leaf_reg': 0.9466903442269958, 'min_data_in_leaf': 32}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:19:09,494]\u001b[0m Trial 77 finished with value: 18006.421931756708 and parameters: {'depth': 11, 'learning_rate': 0.08700332023231915, 'random_strength': 5, 'bagging_temperature': 0.08437976088980653, 'l2_leaf_reg': 1.5029468248438198, 'min_data_in_leaf': 33}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:21:04,001]\u001b[0m Trial 78 finished with value: 17854.798222057427 and parameters: {'depth': 11, 'learning_rate': 0.0934395648267817, 'random_strength': 7, 'bagging_temperature': 0.07538022931994924, 'l2_leaf_reg': 0.6945836134157208, 'min_data_in_leaf': 27}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:22:59,951]\u001b[0m Trial 79 finished with value: 17934.088361273378 and parameters: {'depth': 11, 'learning_rate': 0.09395295308013289, 'random_strength': 8, 'bagging_temperature': 0.07559302445404503, 'l2_leaf_reg': 0.8537663544332114, 'min_data_in_leaf': 19}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:24:54,523]\u001b[0m Trial 80 finished with value: 18046.414840923062 and parameters: {'depth': 11, 'learning_rate': 0.08564599278374564, 'random_strength': 16, 'bagging_temperature': 0.07972709660767534, 'l2_leaf_reg': 0.6037590697754998, 'min_data_in_leaf': 25}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:26:49,561]\u001b[0m Trial 81 finished with value: 17940.900415170618 and parameters: {'depth': 11, 'learning_rate': 0.09429110711057462, 'random_strength': 3, 'bagging_temperature': 0.0877279558254019, 'l2_leaf_reg': 0.7044162486748838, 'min_data_in_leaf': 40}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:28:45,061]\u001b[0m Trial 82 finished with value: 17904.14948800992 and parameters: {'depth': 11, 'learning_rate': 0.09988868300536394, 'random_strength': 6, 'bagging_temperature': 0.07301334290230357, 'l2_leaf_reg': 0.6738984304317478, 'min_data_in_leaf': 21}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:30:39,365]\u001b[0m Trial 83 finished with value: 18441.983628686987 and parameters: {'depth': 11, 'learning_rate': 0.078072762013123, 'random_strength': 86, 'bagging_temperature': 0.09595482544403686, 'l2_leaf_reg': 0.5245284992295861, 'min_data_in_leaf': 29}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:32:34,484]\u001b[0m Trial 84 finished with value: 17900.42165437512 and parameters: {'depth': 11, 'learning_rate': 0.08782851362087889, 'random_strength': 2, 'bagging_temperature': 0.081231229956116, 'l2_leaf_reg': 0.9882356414135257, 'min_data_in_leaf': 36}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:34:29,859]\u001b[0m Trial 85 finished with value: 17983.439661266995 and parameters: {'depth': 11, 'learning_rate': 0.08295154550726457, 'random_strength': 10, 'bagging_temperature': 0.10663426533054607, 'l2_leaf_reg': 0.7766883267097598, 'min_data_in_leaf': 14}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:36:25,950]\u001b[0m Trial 86 finished with value: 17906.715299294803 and parameters: {'depth': 11, 'learning_rate': 0.09389999931871967, 'random_strength': 5, 'bagging_temperature': 0.06760297319688467, 'l2_leaf_reg': 0.6093011238800443, 'min_data_in_leaf': 49}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:38:20,719]\u001b[0m Trial 87 finished with value: 17945.559041195414 and parameters: {'depth': 11, 'learning_rate': 0.07443986057245318, 'random_strength': 3, 'bagging_temperature': 0.08713236870172797, 'l2_leaf_reg': 0.44421716185239524, 'min_data_in_leaf': 39}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:41:10,015]\u001b[0m Trial 88 finished with value: 17984.211572549382 and parameters: {'depth': 12, 'learning_rate': 0.08954156990127775, 'random_strength': 8, 'bagging_temperature': 0.0760939191064209, 'l2_leaf_reg': 0.3854732414281484, 'min_data_in_leaf': 45}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:42:36,142]\u001b[0m Trial 89 finished with value: 18174.553357663535 and parameters: {'depth': 10, 'learning_rate': 0.07980361539059326, 'random_strength': 2, 'bagging_temperature': 0.09281149779544295, 'l2_leaf_reg': 0.5034501960840007, 'min_data_in_leaf': 54}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:44:31,174]\u001b[0m Trial 90 finished with value: 17984.43737557897 and parameters: {'depth': 11, 'learning_rate': 0.0876582997787967, 'random_strength': 13, 'bagging_temperature': 0.07108419532859846, 'l2_leaf_reg': 1.1770674411871531, 'min_data_in_leaf': 48}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:46:25,969]\u001b[0m Trial 91 finished with value: 17956.027439901452 and parameters: {'depth': 11, 'learning_rate': 0.0779719570413265, 'random_strength': 5, 'bagging_temperature': 0.09117094382858731, 'l2_leaf_reg': 0.5053946330523649, 'min_data_in_leaf': 43}. Best is trial 22 with value: 17808.0704282241.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:48:22,232]\u001b[0m Trial 92 finished with value: 17762.68180391033 and parameters: {'depth': 11, 'learning_rate': 0.09554473754863968, 'random_strength': 0, 'bagging_temperature': 0.0820840587322251, 'l2_leaf_reg': 0.7328057730781121, 'min_data_in_leaf': 43}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:50:16,409]\u001b[0m Trial 93 finished with value: 17860.40388180278 and parameters: {'depth': 11, 'learning_rate': 0.09611988336508426, 'random_strength': 0, 'bagging_temperature': 0.0826865653645125, 'l2_leaf_reg': 0.8713556277074892, 'min_data_in_leaf': 33}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:52:12,664]\u001b[0m Trial 94 finished with value: 17815.155303339605 and parameters: {'depth': 11, 'learning_rate': 0.09573675867724625, 'random_strength': 0, 'bagging_temperature': 0.08234916850201547, 'l2_leaf_reg': 0.8810562530335514, 'min_data_in_leaf': 33}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:54:08,567]\u001b[0m Trial 95 finished with value: 17864.18094282176 and parameters: {'depth': 11, 'learning_rate': 0.09955476580494932, 'random_strength': 0, 'bagging_temperature': 0.08386816267627828, 'l2_leaf_reg': 0.8777622551868055, 'min_data_in_leaf': 30}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:56:04,246]\u001b[0m Trial 96 finished with value: 17827.46264650865 and parameters: {'depth': 11, 'learning_rate': 0.09980577811023116, 'random_strength': 0, 'bagging_temperature': 0.08221483967870798, 'l2_leaf_reg': 0.8833543510884726, 'min_data_in_leaf': 30}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:57:58,596]\u001b[0m Trial 97 finished with value: 18259.833823689496 and parameters: {'depth': 11, 'learning_rate': 0.09610063237775293, 'random_strength': 59, 'bagging_temperature': 0.07748872021581614, 'l2_leaf_reg': 0.9435859709551957, 'min_data_in_leaf': 34}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 21:59:53,089]\u001b[0m Trial 98 finished with value: 18001.919095628462 and parameters: {'depth': 11, 'learning_rate': 0.08673265688139446, 'random_strength': 6, 'bagging_temperature': 0.08307742795106311, 'l2_leaf_reg': 1.006246189777295, 'min_data_in_leaf': 23}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n",
            "<ipython-input-323-3774a329acc3>:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-323-3774a329acc3>:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
            "<ipython-input-323-3774a329acc3>:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
            "Default metric period is 5 because MAE is/are not implemented for GPU\n",
            "\u001b[32m[I 2023-05-29 22:01:19,447]\u001b[0m Trial 99 finished with value: 18083.538084731823 and parameters: {'depth': 10, 'learning_rate': 0.09616509912536322, 'random_strength': 11, 'bagging_temperature': 0.08070437073818544, 'l2_leaf_reg': 0.7568359447142143, 'min_data_in_leaf': 37}. Best is trial 92 with value: 17762.68180391033.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 17762.68180391033, params {'depth': 11, 'learning_rate': 0.09554473754863968, 'random_strength': 0, 'bagging_temperature': 0.0820840587322251, 'l2_leaf_reg': 0.7328057730781121, 'min_data_in_leaf': 43}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from catboost import CatBoostRegressor\n",
        "# from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load some data\n",
        "# Load some data\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = get_train_val_data( test_size=0.3, random_state=42)\n",
        "cat_features = [\"city\",\t\"dong\", \"apartment_id\"]\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'iterations': 2500,\n",
        "        'depth': trial.suggest_int('depth', 10, 14),\n",
        "        # 'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'random_strength': trial.suggest_int('random_strength', 0, 100),\n",
        "        # 'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
        "        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.03, 0.3),\n",
        "\n",
        "        # 'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
        "        # 'od_wait': trial.suggest_int('od_wait', 10, 50),\n",
        "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.3, 30),\n",
        "        # 'l2_leaf_reg': 3,\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
        "        # 'min_data_in_leaf': 1,\n",
        "    }\n",
        "    \n",
        "    model = CatBoostRegressor(**param, \n",
        "                              loss_function='RMSE', \n",
        "                              eval_metric='MAE', \n",
        "                              task_type='GPU', \n",
        "                              verbose=False)\n",
        "\n",
        "    model.fit(X_train, y_train, \n",
        "              eval_set=(X_valid, y_valid), \n",
        "              cat_features = cat_features,\n",
        "              early_stopping_rounds=100)\n",
        "\n",
        "    preds = model.predict(X_valid)\n",
        "    mae = mean_absolute_error(y_valid, preds)\n",
        "    \n",
        "    return mae\n",
        "\n",
        "study = optuna.create_study(direction='minimize') # you should minimize the MAE\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivxiyKhYe-AH"
      },
      "outputs": [],
      "source": [
        "# Trial 0 finished with value: 18883.92275292330 and parameters: {'depth': 10, 'learning_rate': 0.08375129644025683, 'random_strength': 92, 'bagging_temperature': 0.263000547283569, 'l2_leaf_reg': 0.9591791031398794, 'min_data_in_leaf': 1}. Best is trial 0 with value: 18883.922752923307\n",
        "# Trial 2 finished with value: 18765.76313046196 and parameters: {'depth': 13, 'learning_rate': 0.04331673367646232, 'random_strength': 91, 'bagging_temperature': 0.13313021716142676, 'l2_leaf_reg': 0.38898930165240697, 'min_data_in_leaf': 90}. Best is trial 2 with value: 18765.76313046196.\n",
        "# Trial 4 finished with value: 18728.69726637476 and parameters: {'depth': 13, 'learning_rate': 0.09728067342545592, 'random_strength': 64, 'bagging_temperature': 0.05620788284677436, 'l2_leaf_reg': 1.8279290163135116, 'min_data_in_leaf': 44}. Best is trial 4 with value: 18728.69726637476.\n",
        "# Trial 7 finished with value: 17971.01011936984 and parameters: {'depth': 10, 'learning_rate': 0.09832153800860591, 'random_strength': 4, 'bagging_temperature': 0.06764535579317005, 'l2_leaf_reg': 0.3593676275123288, 'min_data_in_leaf': 88}. Best is trial 7 with value: 17971.010119369847.\n",
        "# Trial 14 finished with value: 17911.5126705395 and parameters: {'depth': 12, 'learning_rate': 0.07120491279083214, 'random_strength': 2, 'bagging_temperature': 0.09921329095541036, 'l2_leaf_reg': 0.5712336490547983, 'min_data_in_leaf': 72}. Best is trial 14 with value: 17911.512670539592.\n",
        "# Trial 22 finished with value: 17808.0704282241 and parameters: {'depth': 11, 'learning_rate': 0.08277165715420134, 'random_strength': 2, 'bagging_temperature': 0.09606065391621076, 'l2_leaf_reg': 0.5101421491258492, 'min_data_in_leaf': 88}. Best is trial 22 with value: 17808.0704282241.\n",
        "# Trial 68 finished with value: 17848.5036336289 and parameters: {'depth': 11, 'learning_rate': 0.09268507942079059, 'random_strength': 3, 'bagging_temperature': 0.09083259779987603, 'l2_leaf_reg': 0.6459291984111488, 'min_data_in_leaf': 39}. Best is trial 22 with value: 17808.0704282241.\n",
        "# Trial 92 finished with value: 17762.6818039103 and parameters: {'depth': 11, 'learning_rate': 0.09554473754863968, 'random_strength': 0, 'bagging_temperature': 0.0820840587322251, 'l2_leaf_reg': 0.7328057730781121, 'min_data_in_leaf': 43}. Best is trial 92 with value: 17762.68180391033.\n",
        "# Trial 96 finished with value: 17827.4626465086 and parameters: {'depth': 11, 'learning_rate': 0.09980577811023116, 'random_strength': 0, 'bagging_temperature': 0.08221483967870798, 'l2_leaf_reg': 0.8833543510884726, 'min_data_in_leaf': 30}. Best is trial 92 with value: 17762.68180391033.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUdIfbK-VP0Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgQvDm4etXtK"
      },
      "source": [
        "# create sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpgu5TZoKmF1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg4mnjnwxTtr"
      },
      "outputs": [],
      "source": [
        "Y_train_vall_comp = pd.concat([y_train, y_valid])\n",
        "\n",
        "X_train_vall_comp = pd.concat([X_train, X_valid])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Wr9YS07jGzJH",
        "outputId": "ce56cf25-a98c-4026-d8dc-8fabd215f6ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        apartment_id   city  dong  house_area  built_year  floor  \\\n",
              "266915          3610  seoul   179  100.965476        2011      6   \n",
              "315915          4254  busan   170  101.623270        1991      8   \n",
              "124041          1640  seoul   146   98.346258        1977     10   \n",
              "77457            988  seoul   198   53.245475        1992     13   \n",
              "255858          3463  seoul    61  106.993265        1991      8   \n",
              "...              ...    ...   ...         ...         ...    ...   \n",
              "49806            602  seoul   138  101.635230        2006      7   \n",
              "111409          1475  seoul   122   71.723520        2000     13   \n",
              "43279            523  seoul    26   97.078508        2008     14   \n",
              "187578          2500  seoul    94   94.566929        1991     13   \n",
              "254926          3448  busan   188   71.669701        1997     21   \n",
              "\n",
              "        transaction_year  transaction_month  transaction_day  dong_mean_price  \\\n",
              "266915              2022                 11                2     4.022878e+05   \n",
              "315915              2020                  2                2     3.495621e+05   \n",
              "124041              2022                  8                2     1.641677e+06   \n",
              "77457               2020                  4                2     3.442521e+05   \n",
              "255858              2021                  1                1     3.041110e+05   \n",
              "...                  ...                ...              ...              ...   \n",
              "49806               2020                  6                1     5.079522e+05   \n",
              "111409              2021                 11                3     3.759755e+05   \n",
              "43279               2018                  8                1     4.509984e+05   \n",
              "187578              2021                  6                2     2.898371e+05   \n",
              "254926              2020                  6                1     3.622816e+05   \n",
              "\n",
              "          bus_lat    bus_long  seoul_lat  seoul_long  \n",
              "266915   0.000000    0.000000  37.592657  126.921611  \n",
              "315915  35.119025  129.112653   0.000000    0.000000  \n",
              "124041   0.000000    0.000000  37.532109  127.030016  \n",
              "77457    0.000000    0.000000  37.648020  127.076780  \n",
              "255858   0.000000    0.000000  37.468399  126.897957  \n",
              "...           ...         ...        ...         ...  \n",
              "49806    0.000000    0.000000  37.543050  126.931100  \n",
              "111409   0.000000    0.000000  37.585187  126.894858  \n",
              "43279    0.000000    0.000000  37.553235  127.071936  \n",
              "187578   0.000000    0.000000  37.629290  127.036840  \n",
              "254926  35.233506  129.083166   0.000000    0.000000  \n",
              "\n",
              "[329690 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-042bed48-df8a-42ae-b224-b5efddc61717\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>apartment_id</th>\n",
              "      <th>city</th>\n",
              "      <th>dong</th>\n",
              "      <th>house_area</th>\n",
              "      <th>built_year</th>\n",
              "      <th>floor</th>\n",
              "      <th>transaction_year</th>\n",
              "      <th>transaction_month</th>\n",
              "      <th>transaction_day</th>\n",
              "      <th>dong_mean_price</th>\n",
              "      <th>bus_lat</th>\n",
              "      <th>bus_long</th>\n",
              "      <th>seoul_lat</th>\n",
              "      <th>seoul_long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>266915</th>\n",
              "      <td>3610</td>\n",
              "      <td>seoul</td>\n",
              "      <td>179</td>\n",
              "      <td>100.965476</td>\n",
              "      <td>2011</td>\n",
              "      <td>6</td>\n",
              "      <td>2022</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>4.022878e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.592657</td>\n",
              "      <td>126.921611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315915</th>\n",
              "      <td>4254</td>\n",
              "      <td>busan</td>\n",
              "      <td>170</td>\n",
              "      <td>101.623270</td>\n",
              "      <td>1991</td>\n",
              "      <td>8</td>\n",
              "      <td>2020</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3.495621e+05</td>\n",
              "      <td>35.119025</td>\n",
              "      <td>129.112653</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124041</th>\n",
              "      <td>1640</td>\n",
              "      <td>seoul</td>\n",
              "      <td>146</td>\n",
              "      <td>98.346258</td>\n",
              "      <td>1977</td>\n",
              "      <td>10</td>\n",
              "      <td>2022</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1.641677e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.532109</td>\n",
              "      <td>127.030016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77457</th>\n",
              "      <td>988</td>\n",
              "      <td>seoul</td>\n",
              "      <td>198</td>\n",
              "      <td>53.245475</td>\n",
              "      <td>1992</td>\n",
              "      <td>13</td>\n",
              "      <td>2020</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3.442521e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.648020</td>\n",
              "      <td>127.076780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255858</th>\n",
              "      <td>3463</td>\n",
              "      <td>seoul</td>\n",
              "      <td>61</td>\n",
              "      <td>106.993265</td>\n",
              "      <td>1991</td>\n",
              "      <td>8</td>\n",
              "      <td>2021</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.041110e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.468399</td>\n",
              "      <td>126.897957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49806</th>\n",
              "      <td>602</td>\n",
              "      <td>seoul</td>\n",
              "      <td>138</td>\n",
              "      <td>101.635230</td>\n",
              "      <td>2006</td>\n",
              "      <td>7</td>\n",
              "      <td>2020</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5.079522e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.543050</td>\n",
              "      <td>126.931100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111409</th>\n",
              "      <td>1475</td>\n",
              "      <td>seoul</td>\n",
              "      <td>122</td>\n",
              "      <td>71.723520</td>\n",
              "      <td>2000</td>\n",
              "      <td>13</td>\n",
              "      <td>2021</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>3.759755e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.585187</td>\n",
              "      <td>126.894858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43279</th>\n",
              "      <td>523</td>\n",
              "      <td>seoul</td>\n",
              "      <td>26</td>\n",
              "      <td>97.078508</td>\n",
              "      <td>2008</td>\n",
              "      <td>14</td>\n",
              "      <td>2018</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>4.509984e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.553235</td>\n",
              "      <td>127.071936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187578</th>\n",
              "      <td>2500</td>\n",
              "      <td>seoul</td>\n",
              "      <td>94</td>\n",
              "      <td>94.566929</td>\n",
              "      <td>1991</td>\n",
              "      <td>13</td>\n",
              "      <td>2021</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2.898371e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.629290</td>\n",
              "      <td>127.036840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254926</th>\n",
              "      <td>3448</td>\n",
              "      <td>busan</td>\n",
              "      <td>188</td>\n",
              "      <td>71.669701</td>\n",
              "      <td>1997</td>\n",
              "      <td>21</td>\n",
              "      <td>2020</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3.622816e+05</td>\n",
              "      <td>35.233506</td>\n",
              "      <td>129.083166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>329690 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-042bed48-df8a-42ae-b224-b5efddc61717')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-042bed48-df8a-42ae-b224-b5efddc61717 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-042bed48-df8a-42ae-b224-b5efddc61717');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "X_train_vall_comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZoM95cfxMzJ",
        "outputId": "cd87c645-0636-4326-bd3f-592aa17d6d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because MAE is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 199822.7307349\ttest: 199677.9811338\tbest: 199677.9811338 (0)\ttotal: 123ms\tremaining: 5m 7s\n",
            "1:\ttotal: 270ms\tremaining: 5m 37s\n",
            "2:\ttotal: 349ms\tremaining: 4m 50s\n",
            "3:\ttotal: 419ms\tremaining: 4m 21s\n",
            "4:\ttotal: 500ms\tremaining: 4m 9s\n",
            "5:\tlearn: 129001.7614122\ttest: 128025.6447774\tbest: 128025.6447774 (5)\ttotal: 575ms\tremaining: 3m 58s\n",
            "6:\ttotal: 638ms\tremaining: 3m 47s\n",
            "7:\ttotal: 702ms\tremaining: 3m 38s\n",
            "8:\ttotal: 770ms\tremaining: 3m 33s\n",
            "9:\ttotal: 830ms\tremaining: 3m 26s\n",
            "10:\tlearn: 88096.0285359\ttest: 86981.2975826\tbest: 86981.2975826 (10)\ttotal: 902ms\tremaining: 3m 24s\n",
            "11:\ttotal: 967ms\tremaining: 3m 20s\n",
            "12:\ttotal: 1.03s\tremaining: 3m 17s\n",
            "13:\ttotal: 1.1s\tremaining: 3m 15s\n",
            "14:\ttotal: 1.17s\tremaining: 3m 13s\n",
            "15:\tlearn: 66355.2559071\ttest: 65170.6835714\tbest: 65170.6835714 (15)\ttotal: 1.23s\tremaining: 3m 11s\n",
            "16:\ttotal: 1.26s\tremaining: 3m 3s\n",
            "17:\ttotal: 1.33s\tremaining: 3m 3s\n",
            "18:\ttotal: 1.4s\tremaining: 3m 2s\n",
            "19:\ttotal: 1.47s\tremaining: 3m 2s\n",
            "20:\tlearn: 55395.6574479\ttest: 54293.5449260\tbest: 54293.5449260 (20)\ttotal: 1.54s\tremaining: 3m 1s\n",
            "21:\ttotal: 1.61s\tremaining: 3m\n",
            "22:\ttotal: 1.68s\tremaining: 3m 1s\n",
            "23:\ttotal: 1.75s\tremaining: 3m\n",
            "24:\ttotal: 1.82s\tremaining: 3m\n",
            "25:\tlearn: 49001.3056144\ttest: 47983.0868189\tbest: 47983.0868189 (25)\ttotal: 1.89s\tremaining: 2m 59s\n",
            "26:\ttotal: 1.95s\tremaining: 2m 59s\n",
            "27:\ttotal: 2.03s\tremaining: 2m 59s\n",
            "28:\ttotal: 2.1s\tremaining: 2m 58s\n",
            "29:\ttotal: 2.17s\tremaining: 2m 58s\n",
            "30:\tlearn: 45490.8296157\ttest: 44574.5625284\tbest: 44574.5625284 (30)\ttotal: 2.25s\tremaining: 2m 59s\n",
            "31:\ttotal: 2.31s\tremaining: 2m 58s\n",
            "32:\ttotal: 2.38s\tremaining: 2m 57s\n",
            "33:\ttotal: 2.45s\tremaining: 2m 57s\n",
            "34:\ttotal: 2.56s\tremaining: 3m\n",
            "35:\tlearn: 42752.5047166\ttest: 41903.5300636\tbest: 41903.5300636 (35)\ttotal: 2.63s\tremaining: 2m 59s\n",
            "36:\ttotal: 2.7s\tremaining: 2m 59s\n",
            "37:\ttotal: 2.85s\tremaining: 3m 4s\n",
            "38:\ttotal: 3.02s\tremaining: 3m 10s\n",
            "39:\ttotal: 3.26s\tremaining: 3m 20s\n",
            "40:\tlearn: 40921.1228487\ttest: 40129.4177359\tbest: 40129.4177359 (40)\ttotal: 3.49s\tremaining: 3m 29s\n",
            "41:\ttotal: 3.71s\tremaining: 3m 37s\n",
            "42:\ttotal: 3.96s\tremaining: 3m 46s\n",
            "43:\ttotal: 4.2s\tremaining: 3m 54s\n",
            "44:\ttotal: 4.42s\tremaining: 4m 1s\n",
            "45:\tlearn: 39538.7958385\ttest: 38773.6843702\tbest: 38773.6843702 (45)\ttotal: 4.67s\tremaining: 4m 9s\n",
            "46:\ttotal: 4.89s\tremaining: 4m 15s\n",
            "47:\ttotal: 5.13s\tremaining: 4m 22s\n",
            "48:\ttotal: 5.41s\tremaining: 4m 30s\n",
            "49:\ttotal: 5.66s\tremaining: 4m 37s\n",
            "50:\tlearn: 38324.9199187\ttest: 37586.4305661\tbest: 37586.4305661 (50)\ttotal: 5.93s\tremaining: 4m 44s\n",
            "51:\ttotal: 6.2s\tremaining: 4m 52s\n",
            "52:\ttotal: 6.46s\tremaining: 4m 58s\n",
            "53:\ttotal: 6.61s\tremaining: 4m 59s\n",
            "54:\ttotal: 6.63s\tremaining: 4m 54s\n",
            "55:\tlearn: 37732.4634414\ttest: 36989.6252439\tbest: 36989.6252439 (55)\ttotal: 6.72s\tremaining: 4m 53s\n",
            "56:\ttotal: 6.8s\tremaining: 4m 51s\n",
            "57:\ttotal: 6.87s\tremaining: 4m 49s\n",
            "58:\ttotal: 6.95s\tremaining: 4m 47s\n",
            "59:\ttotal: 7.01s\tremaining: 4m 45s\n",
            "60:\tlearn: 36735.9293397\ttest: 35996.1877724\tbest: 35996.1877724 (60)\ttotal: 7.09s\tremaining: 4m 43s\n",
            "61:\ttotal: 7.16s\tremaining: 4m 41s\n",
            "62:\ttotal: 7.22s\tremaining: 4m 39s\n",
            "63:\ttotal: 7.29s\tremaining: 4m 37s\n",
            "64:\ttotal: 7.37s\tremaining: 4m 36s\n",
            "65:\tlearn: 35878.6659468\ttest: 35154.0151455\tbest: 35154.0151455 (65)\ttotal: 7.45s\tremaining: 4m 34s\n",
            "66:\ttotal: 7.53s\tremaining: 4m 33s\n",
            "67:\ttotal: 7.6s\tremaining: 4m 31s\n",
            "68:\ttotal: 7.68s\tremaining: 4m 30s\n",
            "69:\ttotal: 7.74s\tremaining: 4m 28s\n",
            "70:\tlearn: 35262.6912312\ttest: 34537.1246929\tbest: 34537.1246929 (70)\ttotal: 7.81s\tremaining: 4m 27s\n",
            "71:\ttotal: 7.92s\tremaining: 4m 27s\n",
            "72:\ttotal: 7.98s\tremaining: 4m 25s\n",
            "73:\ttotal: 8.05s\tremaining: 4m 23s\n",
            "74:\ttotal: 8.12s\tremaining: 4m 22s\n",
            "75:\tlearn: 34652.0804877\ttest: 33930.7737572\tbest: 33930.7737572 (75)\ttotal: 8.2s\tremaining: 4m 21s\n",
            "76:\ttotal: 8.28s\tremaining: 4m 20s\n",
            "77:\ttotal: 8.36s\tremaining: 4m 19s\n",
            "78:\ttotal: 8.43s\tremaining: 4m 18s\n",
            "79:\ttotal: 8.5s\tremaining: 4m 17s\n",
            "80:\tlearn: 34058.2667112\ttest: 33339.1734862\tbest: 33339.1734862 (80)\ttotal: 8.57s\tremaining: 4m 15s\n",
            "81:\ttotal: 8.65s\tremaining: 4m 14s\n",
            "82:\ttotal: 8.72s\tremaining: 4m 13s\n",
            "83:\ttotal: 8.79s\tremaining: 4m 12s\n",
            "84:\ttotal: 8.86s\tremaining: 4m 11s\n",
            "85:\tlearn: 33597.9776396\ttest: 32892.9652300\tbest: 32892.9652300 (85)\ttotal: 8.93s\tremaining: 4m 10s\n",
            "86:\ttotal: 9s\tremaining: 4m 9s\n",
            "87:\ttotal: 9.07s\tremaining: 4m 8s\n",
            "88:\ttotal: 9.14s\tremaining: 4m 7s\n",
            "89:\ttotal: 9.21s\tremaining: 4m 6s\n",
            "90:\tlearn: 33120.6803967\ttest: 32409.1620613\tbest: 32409.1620613 (90)\ttotal: 9.29s\tremaining: 4m 5s\n",
            "91:\ttotal: 9.35s\tremaining: 4m 4s\n",
            "92:\ttotal: 9.42s\tremaining: 4m 3s\n",
            "93:\ttotal: 9.49s\tremaining: 4m 3s\n",
            "94:\ttotal: 9.56s\tremaining: 4m 2s\n",
            "95:\tlearn: 32606.1242015\ttest: 31900.3793058\tbest: 31900.3793058 (95)\ttotal: 9.66s\tremaining: 4m 1s\n",
            "96:\ttotal: 9.73s\tremaining: 4m 1s\n",
            "97:\ttotal: 9.82s\tremaining: 4m\n",
            "98:\ttotal: 9.91s\tremaining: 4m\n",
            "99:\ttotal: 9.98s\tremaining: 3m 59s\n",
            "100:\tlearn: 32249.9402954\ttest: 31553.9108051\tbest: 31553.9108051 (100)\ttotal: 10.1s\tremaining: 3m 58s\n",
            "101:\ttotal: 10.1s\tremaining: 3m 57s\n",
            "102:\ttotal: 10.2s\tremaining: 3m 57s\n",
            "103:\ttotal: 10.3s\tremaining: 3m 56s\n",
            "104:\ttotal: 10.3s\tremaining: 3m 55s\n",
            "105:\tlearn: 31851.9689648\ttest: 31170.3935212\tbest: 31170.3935212 (105)\ttotal: 10.4s\tremaining: 3m 54s\n",
            "106:\ttotal: 10.5s\tremaining: 3m 54s\n",
            "107:\ttotal: 10.5s\tremaining: 3m 53s\n",
            "108:\ttotal: 10.6s\tremaining: 3m 53s\n",
            "109:\ttotal: 10.7s\tremaining: 3m 53s\n",
            "110:\tlearn: 31417.5959234\ttest: 30746.7059763\tbest: 30746.7059763 (110)\ttotal: 10.8s\tremaining: 3m 51s\n",
            "111:\ttotal: 10.8s\tremaining: 3m 50s\n",
            "112:\ttotal: 10.9s\tremaining: 3m 50s\n",
            "113:\ttotal: 11s\tremaining: 3m 49s\n",
            "114:\ttotal: 11s\tremaining: 3m 49s\n",
            "115:\tlearn: 30823.3286785\ttest: 30168.7615235\tbest: 30168.7615235 (115)\ttotal: 11.1s\tremaining: 3m 48s\n",
            "116:\ttotal: 11.2s\tremaining: 3m 48s\n",
            "117:\ttotal: 11.3s\tremaining: 3m 47s\n",
            "118:\ttotal: 11.3s\tremaining: 3m 46s\n",
            "119:\ttotal: 11.4s\tremaining: 3m 46s\n",
            "120:\tlearn: 30400.5749886\ttest: 29742.8221663\tbest: 29742.8221663 (120)\ttotal: 11.5s\tremaining: 3m 45s\n",
            "121:\ttotal: 11.5s\tremaining: 3m 45s\n",
            "122:\ttotal: 11.6s\tremaining: 3m 44s\n",
            "123:\ttotal: 11.7s\tremaining: 3m 44s\n",
            "124:\ttotal: 11.8s\tremaining: 3m 43s\n",
            "125:\tlearn: 30099.9036913\ttest: 29450.9536433\tbest: 29450.9536433 (125)\ttotal: 11.8s\tremaining: 3m 43s\n",
            "126:\ttotal: 11.9s\tremaining: 3m 42s\n",
            "127:\ttotal: 12s\tremaining: 3m 42s\n",
            "128:\ttotal: 12.1s\tremaining: 3m 41s\n",
            "129:\ttotal: 12.2s\tremaining: 3m 41s\n",
            "130:\tlearn: 29814.2341230\ttest: 29178.7820073\tbest: 29178.7820073 (130)\ttotal: 12.2s\tremaining: 3m 41s\n",
            "131:\ttotal: 12.3s\tremaining: 3m 40s\n",
            "132:\ttotal: 12.4s\tremaining: 3m 40s\n",
            "133:\ttotal: 12.6s\tremaining: 3m 42s\n",
            "134:\ttotal: 12.7s\tremaining: 3m 41s\n",
            "135:\tlearn: 29451.0494101\ttest: 28821.9137574\tbest: 28821.9137574 (135)\ttotal: 12.7s\tremaining: 3m 41s\n",
            "136:\ttotal: 12.8s\tremaining: 3m 41s\n",
            "137:\ttotal: 13s\tremaining: 3m 41s\n",
            "138:\ttotal: 13.1s\tremaining: 3m 42s\n",
            "139:\ttotal: 13.2s\tremaining: 3m 42s\n",
            "140:\tlearn: 28981.0946283\ttest: 28359.5209237\tbest: 28359.5209237 (140)\ttotal: 13.3s\tremaining: 3m 43s\n",
            "141:\ttotal: 13.6s\tremaining: 3m 45s\n",
            "142:\ttotal: 13.7s\tremaining: 3m 45s\n",
            "143:\ttotal: 13.8s\tremaining: 3m 45s\n",
            "144:\ttotal: 13.8s\tremaining: 3m 44s\n",
            "145:\tlearn: 28640.1950924\ttest: 28027.5313173\tbest: 28027.5313173 (145)\ttotal: 13.9s\tremaining: 3m 44s\n",
            "146:\ttotal: 14s\tremaining: 3m 44s\n",
            "147:\ttotal: 14.1s\tremaining: 3m 43s\n",
            "148:\ttotal: 14.1s\tremaining: 3m 43s\n",
            "149:\ttotal: 14.2s\tremaining: 3m 42s\n",
            "150:\tlearn: 28371.8877734\ttest: 27765.7542540\tbest: 27765.7542540 (150)\ttotal: 14.3s\tremaining: 3m 42s\n",
            "151:\ttotal: 14.3s\tremaining: 3m 41s\n",
            "152:\ttotal: 14.4s\tremaining: 3m 41s\n",
            "153:\ttotal: 14.5s\tremaining: 3m 40s\n",
            "154:\ttotal: 14.6s\tremaining: 3m 40s\n",
            "155:\tlearn: 28099.0457703\ttest: 27500.9333616\tbest: 27500.9333616 (155)\ttotal: 14.6s\tremaining: 3m 39s\n",
            "156:\ttotal: 14.7s\tremaining: 3m 39s\n",
            "157:\ttotal: 14.8s\tremaining: 3m 39s\n",
            "158:\ttotal: 14.9s\tremaining: 3m 38s\n",
            "159:\ttotal: 14.9s\tremaining: 3m 38s\n",
            "160:\tlearn: 27870.9791137\ttest: 27277.0566290\tbest: 27277.0566290 (160)\ttotal: 15s\tremaining: 3m 38s\n",
            "161:\ttotal: 15.1s\tremaining: 3m 38s\n",
            "162:\ttotal: 15.2s\tremaining: 3m 37s\n",
            "163:\ttotal: 15.3s\tremaining: 3m 37s\n",
            "164:\ttotal: 15.3s\tremaining: 3m 36s\n",
            "165:\tlearn: 27522.3954624\ttest: 26945.3232633\tbest: 26945.3232633 (165)\ttotal: 15.4s\tremaining: 3m 36s\n",
            "166:\ttotal: 15.5s\tremaining: 3m 36s\n",
            "167:\ttotal: 15.5s\tremaining: 3m 35s\n",
            "168:\ttotal: 15.6s\tremaining: 3m 35s\n",
            "169:\ttotal: 15.7s\tremaining: 3m 35s\n",
            "170:\tlearn: 27283.9828930\ttest: 26708.1246019\tbest: 26708.1246019 (170)\ttotal: 15.8s\tremaining: 3m 34s\n",
            "171:\ttotal: 15.8s\tremaining: 3m 34s\n",
            "172:\ttotal: 15.9s\tremaining: 3m 33s\n",
            "173:\ttotal: 16s\tremaining: 3m 33s\n",
            "174:\ttotal: 16s\tremaining: 3m 33s\n",
            "175:\tlearn: 27033.0222937\ttest: 26461.4838586\tbest: 26461.4838586 (175)\ttotal: 16.1s\tremaining: 3m 32s\n",
            "176:\ttotal: 16.2s\tremaining: 3m 32s\n",
            "177:\ttotal: 16.3s\tremaining: 3m 32s\n",
            "178:\ttotal: 16.3s\tremaining: 3m 31s\n",
            "179:\ttotal: 16.4s\tremaining: 3m 31s\n",
            "180:\tlearn: 26750.1853499\ttest: 26181.9511258\tbest: 26181.9511258 (180)\ttotal: 16.5s\tremaining: 3m 31s\n",
            "181:\ttotal: 16.6s\tremaining: 3m 31s\n",
            "182:\ttotal: 16.8s\tremaining: 3m 33s\n",
            "183:\ttotal: 17.1s\tremaining: 3m 34s\n",
            "184:\ttotal: 17.3s\tremaining: 3m 36s\n",
            "185:\tlearn: 26501.8585944\ttest: 25941.7189885\tbest: 25941.7189885 (185)\ttotal: 17.5s\tremaining: 3m 37s\n",
            "186:\ttotal: 17.7s\tremaining: 3m 39s\n",
            "187:\ttotal: 17.9s\tremaining: 3m 40s\n",
            "188:\ttotal: 18.2s\tremaining: 3m 42s\n",
            "189:\ttotal: 18.4s\tremaining: 3m 44s\n",
            "190:\tlearn: 26267.5489824\ttest: 25711.4931805\tbest: 25711.4931805 (190)\ttotal: 18.7s\tremaining: 3m 45s\n",
            "191:\ttotal: 18.9s\tremaining: 3m 47s\n",
            "192:\ttotal: 19.2s\tremaining: 3m 49s\n",
            "193:\ttotal: 19.4s\tremaining: 3m 51s\n",
            "194:\ttotal: 19.7s\tremaining: 3m 52s\n",
            "195:\tlearn: 26050.0984561\ttest: 25494.5737713\tbest: 25494.5737713 (195)\ttotal: 19.9s\tremaining: 3m 54s\n",
            "196:\ttotal: 20.2s\tremaining: 3m 56s\n",
            "197:\ttotal: 20.3s\tremaining: 3m 56s\n",
            "198:\ttotal: 20.4s\tremaining: 3m 55s\n",
            "199:\ttotal: 20.5s\tremaining: 3m 55s\n",
            "200:\tlearn: 25850.6158391\ttest: 25296.9397110\tbest: 25296.9397110 (200)\ttotal: 20.6s\tremaining: 3m 55s\n",
            "201:\ttotal: 20.6s\tremaining: 3m 54s\n",
            "202:\ttotal: 20.7s\tremaining: 3m 54s\n",
            "203:\ttotal: 20.8s\tremaining: 3m 53s\n",
            "204:\ttotal: 20.8s\tremaining: 3m 53s\n",
            "205:\tlearn: 25625.6163062\ttest: 25080.5923140\tbest: 25080.5923140 (205)\ttotal: 20.9s\tremaining: 3m 52s\n",
            "206:\ttotal: 21s\tremaining: 3m 52s\n",
            "207:\ttotal: 21s\tremaining: 3m 51s\n",
            "208:\ttotal: 21.1s\tremaining: 3m 51s\n",
            "209:\ttotal: 21.2s\tremaining: 3m 50s\n",
            "210:\tlearn: 25398.8293003\ttest: 24857.5438341\tbest: 24857.5438341 (210)\ttotal: 21.2s\tremaining: 3m 50s\n",
            "211:\ttotal: 21.3s\tremaining: 3m 50s\n",
            "212:\ttotal: 21.4s\tremaining: 3m 49s\n",
            "213:\ttotal: 21.5s\tremaining: 3m 49s\n",
            "214:\ttotal: 21.5s\tremaining: 3m 48s\n",
            "215:\tlearn: 25232.2697322\ttest: 24703.4448118\tbest: 24703.4448118 (215)\ttotal: 21.6s\tremaining: 3m 48s\n",
            "216:\ttotal: 21.7s\tremaining: 3m 48s\n",
            "217:\ttotal: 21.8s\tremaining: 3m 47s\n",
            "218:\ttotal: 21.8s\tremaining: 3m 47s\n",
            "219:\ttotal: 21.9s\tremaining: 3m 47s\n",
            "220:\tlearn: 25089.2579089\ttest: 24561.0681549\tbest: 24561.0681549 (220)\ttotal: 22s\tremaining: 3m 46s\n",
            "221:\ttotal: 22.1s\tremaining: 3m 46s\n",
            "222:\ttotal: 22.1s\tremaining: 3m 45s\n",
            "223:\ttotal: 22.2s\tremaining: 3m 45s\n",
            "224:\ttotal: 22.3s\tremaining: 3m 45s\n",
            "225:\tlearn: 24962.9514271\ttest: 24449.2824977\tbest: 24449.2824977 (225)\ttotal: 22.4s\tremaining: 3m 44s\n",
            "226:\ttotal: 22.4s\tremaining: 3m 44s\n",
            "227:\ttotal: 22.5s\tremaining: 3m 44s\n",
            "228:\ttotal: 22.6s\tremaining: 3m 43s\n",
            "229:\ttotal: 22.6s\tremaining: 3m 43s\n",
            "230:\tlearn: 24769.8561922\ttest: 24262.8712629\tbest: 24262.8712629 (230)\ttotal: 22.7s\tremaining: 3m 43s\n",
            "231:\ttotal: 22.8s\tremaining: 3m 42s\n",
            "232:\ttotal: 22.8s\tremaining: 3m 42s\n",
            "233:\ttotal: 22.9s\tremaining: 3m 42s\n",
            "234:\ttotal: 23s\tremaining: 3m 41s\n",
            "235:\tlearn: 24563.7035518\ttest: 24062.5609108\tbest: 24062.5609108 (235)\ttotal: 23.1s\tremaining: 3m 41s\n",
            "236:\ttotal: 23.2s\tremaining: 3m 41s\n",
            "237:\ttotal: 23.2s\tremaining: 3m 40s\n",
            "238:\ttotal: 23.3s\tremaining: 3m 40s\n",
            "239:\ttotal: 23.4s\tremaining: 3m 40s\n",
            "240:\tlearn: 24372.5138160\ttest: 23880.6481240\tbest: 23880.6481240 (240)\ttotal: 23.4s\tremaining: 3m 39s\n",
            "241:\ttotal: 23.5s\tremaining: 3m 39s\n",
            "242:\ttotal: 23.6s\tremaining: 3m 39s\n",
            "243:\ttotal: 23.7s\tremaining: 3m 38s\n",
            "244:\ttotal: 23.7s\tremaining: 3m 38s\n",
            "245:\tlearn: 24214.7968819\ttest: 23727.8768945\tbest: 23727.8768945 (245)\ttotal: 23.8s\tremaining: 3m 38s\n",
            "246:\ttotal: 23.9s\tremaining: 3m 37s\n",
            "247:\ttotal: 24s\tremaining: 3m 37s\n",
            "248:\ttotal: 24s\tremaining: 3m 37s\n",
            "249:\ttotal: 24.1s\tremaining: 3m 36s\n",
            "250:\tlearn: 24040.0904122\ttest: 23554.3009898\tbest: 23554.3009898 (250)\ttotal: 24.2s\tremaining: 3m 36s\n",
            "251:\ttotal: 24.2s\tremaining: 3m 36s\n",
            "252:\ttotal: 24.3s\tremaining: 3m 35s\n",
            "253:\ttotal: 24.4s\tremaining: 3m 35s\n",
            "254:\ttotal: 24.5s\tremaining: 3m 35s\n",
            "255:\tlearn: 23897.2230156\ttest: 23420.5640450\tbest: 23420.5640450 (255)\ttotal: 24.5s\tremaining: 3m 34s\n",
            "256:\ttotal: 24.6s\tremaining: 3m 34s\n",
            "257:\ttotal: 24.7s\tremaining: 3m 34s\n",
            "258:\ttotal: 24.7s\tremaining: 3m 33s\n",
            "259:\ttotal: 24.8s\tremaining: 3m 33s\n",
            "260:\tlearn: 23768.4131396\ttest: 23300.0506739\tbest: 23300.0506739 (260)\ttotal: 24.9s\tremaining: 3m 33s\n",
            "261:\ttotal: 25s\tremaining: 3m 33s\n",
            "262:\ttotal: 25s\tremaining: 3m 32s\n",
            "263:\ttotal: 25.1s\tremaining: 3m 32s\n",
            "264:\ttotal: 25.2s\tremaining: 3m 32s\n",
            "265:\tlearn: 23601.1019321\ttest: 23144.3391469\tbest: 23144.3391469 (265)\ttotal: 25.2s\tremaining: 3m 32s\n",
            "266:\ttotal: 25.3s\tremaining: 3m 31s\n",
            "267:\ttotal: 25.4s\tremaining: 3m 31s\n",
            "268:\ttotal: 25.5s\tremaining: 3m 31s\n",
            "269:\ttotal: 25.5s\tremaining: 3m 30s\n",
            "270:\tlearn: 23505.2648003\ttest: 23055.2993822\tbest: 23055.2993822 (270)\ttotal: 25.6s\tremaining: 3m 30s\n",
            "271:\ttotal: 25.7s\tremaining: 3m 30s\n",
            "272:\ttotal: 25.7s\tremaining: 3m 29s\n",
            "273:\ttotal: 25.8s\tremaining: 3m 29s\n",
            "274:\ttotal: 25.9s\tremaining: 3m 29s\n",
            "275:\tlearn: 23400.0647881\ttest: 22948.4314760\tbest: 22948.4314760 (275)\ttotal: 26s\tremaining: 3m 29s\n",
            "276:\ttotal: 26s\tremaining: 3m 29s\n",
            "277:\ttotal: 26.1s\tremaining: 3m 28s\n",
            "278:\ttotal: 26.2s\tremaining: 3m 28s\n",
            "279:\ttotal: 26.3s\tremaining: 3m 28s\n",
            "280:\tlearn: 23288.5628075\ttest: 22846.3648478\tbest: 22846.3648478 (280)\ttotal: 26.3s\tremaining: 3m 27s\n",
            "281:\ttotal: 26.4s\tremaining: 3m 27s\n",
            "282:\ttotal: 26.5s\tremaining: 3m 27s\n",
            "283:\ttotal: 26.5s\tremaining: 3m 27s\n",
            "284:\ttotal: 26.6s\tremaining: 3m 26s\n",
            "285:\tlearn: 23111.0920440\ttest: 22673.0477721\tbest: 22673.0477721 (285)\ttotal: 26.7s\tremaining: 3m 26s\n",
            "286:\ttotal: 26.8s\tremaining: 3m 26s\n",
            "287:\ttotal: 26.8s\tremaining: 3m 26s\n",
            "288:\ttotal: 26.9s\tremaining: 3m 25s\n",
            "289:\ttotal: 27s\tremaining: 3m 25s\n",
            "290:\tlearn: 22994.6562650\ttest: 22564.2930025\tbest: 22564.2930025 (290)\ttotal: 27.1s\tremaining: 3m 25s\n",
            "291:\ttotal: 27.1s\tremaining: 3m 25s\n",
            "292:\ttotal: 27.2s\tremaining: 3m 24s\n",
            "293:\ttotal: 27.3s\tremaining: 3m 24s\n",
            "294:\ttotal: 27.3s\tremaining: 3m 24s\n",
            "295:\tlearn: 22810.0806455\ttest: 22383.8244816\tbest: 22383.8244816 (295)\ttotal: 27.4s\tremaining: 3m 24s\n",
            "296:\ttotal: 27.5s\tremaining: 3m 23s\n",
            "297:\ttotal: 27.6s\tremaining: 3m 23s\n",
            "298:\ttotal: 27.6s\tremaining: 3m 23s\n",
            "299:\ttotal: 27.7s\tremaining: 3m 23s\n",
            "300:\tlearn: 22704.0715339\ttest: 22285.6299352\tbest: 22285.6299352 (300)\ttotal: 27.8s\tremaining: 3m 22s\n",
            "301:\ttotal: 27.8s\tremaining: 3m 22s\n",
            "302:\ttotal: 27.9s\tremaining: 3m 22s\n",
            "303:\ttotal: 28s\tremaining: 3m 22s\n",
            "304:\ttotal: 28.1s\tremaining: 3m 22s\n",
            "305:\tlearn: 22594.3399436\ttest: 22178.0450726\tbest: 22178.0450726 (305)\ttotal: 28.1s\tremaining: 3m 21s\n",
            "306:\ttotal: 28.2s\tremaining: 3m 21s\n",
            "307:\ttotal: 28.3s\tremaining: 3m 21s\n",
            "308:\ttotal: 28.4s\tremaining: 3m 21s\n",
            "309:\ttotal: 28.4s\tremaining: 3m 20s\n",
            "310:\tlearn: 22433.8695866\ttest: 22030.1475932\tbest: 22030.1475932 (310)\ttotal: 28.5s\tremaining: 3m 20s\n",
            "311:\ttotal: 28.6s\tremaining: 3m 20s\n",
            "312:\ttotal: 28.6s\tremaining: 3m 20s\n",
            "313:\ttotal: 28.7s\tremaining: 3m 19s\n",
            "314:\ttotal: 28.8s\tremaining: 3m 19s\n",
            "315:\tlearn: 22322.4794201\ttest: 21926.0439807\tbest: 21926.0439807 (315)\ttotal: 28.9s\tremaining: 3m 19s\n",
            "316:\ttotal: 28.9s\tremaining: 3m 19s\n",
            "317:\ttotal: 29s\tremaining: 3m 19s\n",
            "318:\ttotal: 29.1s\tremaining: 3m 18s\n",
            "319:\ttotal: 29.2s\tremaining: 3m 18s\n",
            "320:\tlearn: 22239.5971974\ttest: 21850.6212098\tbest: 21850.6212098 (320)\ttotal: 29.2s\tremaining: 3m 18s\n",
            "321:\ttotal: 29.3s\tremaining: 3m 18s\n",
            "322:\ttotal: 29.4s\tremaining: 3m 18s\n",
            "323:\ttotal: 29.4s\tremaining: 3m 17s\n",
            "324:\ttotal: 29.5s\tremaining: 3m 17s\n",
            "325:\tlearn: 22149.6691316\ttest: 21762.8781785\tbest: 21762.8781785 (325)\ttotal: 29.6s\tremaining: 3m 17s\n",
            "326:\ttotal: 29.7s\tremaining: 3m 17s\n",
            "327:\ttotal: 29.7s\tremaining: 3m 16s\n",
            "328:\ttotal: 29.8s\tremaining: 3m 16s\n",
            "329:\ttotal: 29.9s\tremaining: 3m 16s\n",
            "330:\tlearn: 22001.5682126\ttest: 21625.4775092\tbest: 21625.4775092 (330)\ttotal: 30s\tremaining: 3m 16s\n",
            "331:\ttotal: 30s\tremaining: 3m 16s\n",
            "332:\ttotal: 30.1s\tremaining: 3m 15s\n",
            "333:\ttotal: 30.2s\tremaining: 3m 15s\n",
            "334:\ttotal: 30.3s\tremaining: 3m 16s\n",
            "335:\tlearn: 21953.9167582\ttest: 21576.9949549\tbest: 21576.9949549 (335)\ttotal: 30.5s\tremaining: 3m 16s\n",
            "336:\ttotal: 30.7s\tremaining: 3m 17s\n",
            "337:\ttotal: 31s\tremaining: 3m 18s\n",
            "338:\ttotal: 31.2s\tremaining: 3m 18s\n",
            "339:\ttotal: 31.4s\tremaining: 3m 19s\n",
            "340:\tlearn: 21874.6762595\ttest: 21502.4884386\tbest: 21502.4884386 (340)\ttotal: 31.6s\tremaining: 3m 20s\n",
            "341:\ttotal: 31.8s\tremaining: 3m 20s\n",
            "342:\ttotal: 32.1s\tremaining: 3m 21s\n",
            "343:\ttotal: 32.3s\tremaining: 3m 22s\n",
            "344:\ttotal: 32.6s\tremaining: 3m 23s\n",
            "345:\tlearn: 21812.4594619\ttest: 21444.5612141\tbest: 21444.5612141 (345)\ttotal: 32.9s\tremaining: 3m 24s\n",
            "346:\ttotal: 33.1s\tremaining: 3m 25s\n",
            "347:\ttotal: 33.4s\tremaining: 3m 26s\n",
            "348:\ttotal: 33.6s\tremaining: 3m 27s\n",
            "349:\ttotal: 33.9s\tremaining: 3m 28s\n",
            "350:\tlearn: 21778.0781704\ttest: 21413.4214970\tbest: 21413.4214970 (350)\ttotal: 34.1s\tremaining: 3m 28s\n",
            "351:\ttotal: 34.2s\tremaining: 3m 28s\n",
            "352:\ttotal: 34.3s\tremaining: 3m 28s\n",
            "353:\ttotal: 34.4s\tremaining: 3m 28s\n",
            "354:\ttotal: 34.4s\tremaining: 3m 28s\n",
            "355:\tlearn: 21649.7714580\ttest: 21292.5871779\tbest: 21292.5871779 (355)\ttotal: 34.5s\tremaining: 3m 27s\n",
            "356:\ttotal: 34.6s\tremaining: 3m 27s\n",
            "357:\ttotal: 34.7s\tremaining: 3m 27s\n",
            "358:\ttotal: 34.7s\tremaining: 3m 27s\n",
            "359:\ttotal: 34.8s\tremaining: 3m 26s\n",
            "360:\tlearn: 21591.3578938\ttest: 21240.6207852\tbest: 21240.6207852 (360)\ttotal: 34.9s\tremaining: 3m 26s\n",
            "361:\ttotal: 34.9s\tremaining: 3m 26s\n",
            "362:\ttotal: 35s\tremaining: 3m 26s\n",
            "363:\ttotal: 35.1s\tremaining: 3m 25s\n",
            "364:\ttotal: 35.2s\tremaining: 3m 25s\n",
            "365:\tlearn: 21538.6173436\ttest: 21191.4484516\tbest: 21191.4484516 (365)\ttotal: 35.2s\tremaining: 3m 25s\n",
            "366:\ttotal: 35.3s\tremaining: 3m 25s\n",
            "367:\ttotal: 35.4s\tremaining: 3m 25s\n",
            "368:\ttotal: 35.5s\tremaining: 3m 24s\n",
            "369:\ttotal: 35.5s\tremaining: 3m 24s\n",
            "370:\tlearn: 21469.2692893\ttest: 21123.8953765\tbest: 21123.8953765 (370)\ttotal: 35.6s\tremaining: 3m 24s\n",
            "371:\ttotal: 35.7s\tremaining: 3m 24s\n",
            "372:\ttotal: 35.8s\tremaining: 3m 24s\n",
            "373:\ttotal: 35.9s\tremaining: 3m 23s\n",
            "374:\ttotal: 35.9s\tremaining: 3m 23s\n",
            "375:\tlearn: 21377.5242440\ttest: 21038.6396918\tbest: 21038.6396918 (375)\ttotal: 36s\tremaining: 3m 23s\n",
            "376:\ttotal: 36.1s\tremaining: 3m 23s\n",
            "377:\ttotal: 36.2s\tremaining: 3m 22s\n",
            "378:\ttotal: 36.2s\tremaining: 3m 22s\n",
            "379:\ttotal: 36.3s\tremaining: 3m 22s\n",
            "380:\tlearn: 21327.8834299\ttest: 20994.8613546\tbest: 20994.8613546 (380)\ttotal: 36.4s\tremaining: 3m 22s\n",
            "381:\ttotal: 36.4s\tremaining: 3m 22s\n",
            "382:\ttotal: 36.5s\tremaining: 3m 21s\n",
            "383:\ttotal: 36.6s\tremaining: 3m 21s\n",
            "384:\ttotal: 36.7s\tremaining: 3m 21s\n",
            "385:\tlearn: 21302.5326822\ttest: 20970.8200228\tbest: 20970.8200228 (385)\ttotal: 36.7s\tremaining: 3m 21s\n",
            "386:\ttotal: 36.8s\tremaining: 3m 20s\n",
            "387:\ttotal: 36.9s\tremaining: 3m 20s\n",
            "388:\ttotal: 36.9s\tremaining: 3m 20s\n",
            "389:\ttotal: 37s\tremaining: 3m 20s\n",
            "390:\tlearn: 21271.6207346\ttest: 20941.1866905\tbest: 20941.1866905 (390)\ttotal: 37.1s\tremaining: 3m 20s\n",
            "391:\ttotal: 37.2s\tremaining: 3m 19s\n",
            "392:\ttotal: 37.2s\tremaining: 3m 19s\n",
            "393:\ttotal: 37.3s\tremaining: 3m 19s\n",
            "394:\ttotal: 37.4s\tremaining: 3m 19s\n",
            "395:\tlearn: 21219.4967151\ttest: 20889.5244219\tbest: 20889.5244219 (395)\ttotal: 37.5s\tremaining: 3m 19s\n",
            "396:\ttotal: 37.5s\tremaining: 3m 18s\n",
            "397:\ttotal: 37.6s\tremaining: 3m 18s\n",
            "398:\ttotal: 37.7s\tremaining: 3m 18s\n",
            "399:\ttotal: 37.8s\tremaining: 3m 18s\n",
            "400:\tlearn: 21173.1016167\ttest: 20849.9468794\tbest: 20849.9468794 (400)\ttotal: 37.9s\tremaining: 3m 18s\n",
            "401:\ttotal: 38s\tremaining: 3m 18s\n",
            "402:\ttotal: 38s\tremaining: 3m 17s\n",
            "403:\ttotal: 38.1s\tremaining: 3m 17s\n",
            "404:\ttotal: 38.2s\tremaining: 3m 17s\n",
            "405:\tlearn: 21122.8349540\ttest: 20806.5140789\tbest: 20806.5140789 (405)\ttotal: 38.3s\tremaining: 3m 17s\n",
            "406:\ttotal: 38.3s\tremaining: 3m 17s\n",
            "407:\ttotal: 38.4s\tremaining: 3m 17s\n",
            "408:\ttotal: 38.5s\tremaining: 3m 16s\n",
            "409:\ttotal: 38.6s\tremaining: 3m 16s\n",
            "410:\tlearn: 21065.8951621\ttest: 20757.8930510\tbest: 20757.8930510 (410)\ttotal: 38.7s\tremaining: 3m 16s\n",
            "411:\ttotal: 38.7s\tremaining: 3m 16s\n",
            "412:\ttotal: 38.8s\tremaining: 3m 16s\n",
            "413:\ttotal: 38.9s\tremaining: 3m 15s\n",
            "414:\ttotal: 39s\tremaining: 3m 15s\n",
            "415:\tlearn: 20988.6424702\ttest: 20682.6242834\tbest: 20682.6242834 (415)\ttotal: 39s\tremaining: 3m 15s\n",
            "416:\ttotal: 39.1s\tremaining: 3m 15s\n",
            "417:\ttotal: 39.2s\tremaining: 3m 15s\n",
            "418:\ttotal: 39.3s\tremaining: 3m 14s\n",
            "419:\ttotal: 39.3s\tremaining: 3m 14s\n",
            "420:\tlearn: 20928.4988929\ttest: 20627.2051119\tbest: 20627.2051119 (420)\ttotal: 39.4s\tremaining: 3m 14s\n",
            "421:\ttotal: 39.5s\tremaining: 3m 14s\n",
            "422:\ttotal: 39.6s\tremaining: 3m 14s\n",
            "423:\ttotal: 39.6s\tremaining: 3m 14s\n",
            "424:\ttotal: 39.7s\tremaining: 3m 13s\n",
            "425:\tlearn: 20844.1832752\ttest: 20547.1091834\tbest: 20547.1091834 (425)\ttotal: 39.8s\tremaining: 3m 13s\n",
            "426:\ttotal: 39.8s\tremaining: 3m 13s\n",
            "427:\ttotal: 39.9s\tremaining: 3m 13s\n",
            "428:\ttotal: 40s\tremaining: 3m 12s\n",
            "429:\ttotal: 40s\tremaining: 3m 12s\n",
            "430:\tlearn: 20761.4004428\ttest: 20469.0502391\tbest: 20469.0502391 (430)\ttotal: 40.1s\tremaining: 3m 12s\n",
            "431:\ttotal: 40.2s\tremaining: 3m 12s\n",
            "432:\ttotal: 40.3s\tremaining: 3m 12s\n",
            "433:\ttotal: 40.3s\tremaining: 3m 11s\n",
            "434:\ttotal: 40.4s\tremaining: 3m 11s\n",
            "435:\tlearn: 20664.5010768\ttest: 20373.6251630\tbest: 20373.6251630 (435)\ttotal: 40.5s\tremaining: 3m 11s\n",
            "436:\ttotal: 40.5s\tremaining: 3m 11s\n",
            "437:\ttotal: 40.6s\tremaining: 3m 11s\n",
            "438:\ttotal: 40.7s\tremaining: 3m 10s\n",
            "439:\ttotal: 40.7s\tremaining: 3m 10s\n",
            "440:\tlearn: 20603.5639298\ttest: 20318.9850668\tbest: 20318.9850668 (440)\ttotal: 40.8s\tremaining: 3m 10s\n",
            "441:\ttotal: 40.9s\tremaining: 3m 10s\n",
            "442:\ttotal: 41s\tremaining: 3m 10s\n",
            "443:\ttotal: 41s\tremaining: 3m 10s\n",
            "444:\ttotal: 41.1s\tremaining: 3m 9s\n",
            "445:\tlearn: 20527.5644879\ttest: 20249.5904233\tbest: 20249.5904233 (445)\ttotal: 41.2s\tremaining: 3m 9s\n",
            "446:\ttotal: 41.3s\tremaining: 3m 9s\n",
            "447:\ttotal: 41.3s\tremaining: 3m 9s\n",
            "448:\ttotal: 41.4s\tremaining: 3m 9s\n",
            "449:\ttotal: 41.5s\tremaining: 3m 8s\n",
            "450:\tlearn: 20444.1697837\ttest: 20167.5869251\tbest: 20167.5869251 (450)\ttotal: 41.5s\tremaining: 3m 8s\n",
            "451:\ttotal: 41.6s\tremaining: 3m 8s\n",
            "452:\ttotal: 41.7s\tremaining: 3m 8s\n",
            "453:\ttotal: 41.8s\tremaining: 3m 8s\n",
            "454:\ttotal: 41.8s\tremaining: 3m 7s\n",
            "455:\tlearn: 20348.5873396\ttest: 20074.0797719\tbest: 20074.0797719 (455)\ttotal: 41.9s\tremaining: 3m 7s\n",
            "456:\ttotal: 42s\tremaining: 3m 7s\n",
            "457:\ttotal: 42s\tremaining: 3m 7s\n",
            "458:\ttotal: 42.1s\tremaining: 3m 7s\n",
            "459:\ttotal: 42.2s\tremaining: 3m 6s\n",
            "460:\tlearn: 20286.1143013\ttest: 20018.7913090\tbest: 20018.7913090 (460)\ttotal: 42.2s\tremaining: 3m 6s\n",
            "461:\ttotal: 42.3s\tremaining: 3m 6s\n",
            "462:\ttotal: 42.4s\tremaining: 3m 6s\n",
            "463:\ttotal: 42.5s\tremaining: 3m 6s\n",
            "464:\ttotal: 42.5s\tremaining: 3m 6s\n",
            "465:\tlearn: 20216.3158846\ttest: 19956.1081015\tbest: 19956.1081015 (465)\ttotal: 42.6s\tremaining: 3m 5s\n",
            "466:\ttotal: 42.7s\tremaining: 3m 5s\n",
            "467:\ttotal: 42.8s\tremaining: 3m 5s\n",
            "468:\ttotal: 42.8s\tremaining: 3m 5s\n",
            "469:\ttotal: 42.9s\tremaining: 3m 5s\n",
            "470:\tlearn: 20163.9138827\ttest: 19914.2282750\tbest: 19914.2282750 (470)\ttotal: 43s\tremaining: 3m 5s\n",
            "471:\ttotal: 43s\tremaining: 3m 4s\n",
            "472:\ttotal: 43.1s\tremaining: 3m 4s\n",
            "473:\ttotal: 43.2s\tremaining: 3m 4s\n",
            "474:\ttotal: 43.3s\tremaining: 3m 4s\n",
            "475:\tlearn: 20129.5496739\ttest: 19882.8789064\tbest: 19882.8789064 (475)\ttotal: 43.4s\tremaining: 3m 4s\n",
            "476:\ttotal: 43.4s\tremaining: 3m 4s\n",
            "477:\ttotal: 43.5s\tremaining: 3m 3s\n",
            "478:\ttotal: 43.6s\tremaining: 3m 3s\n",
            "479:\ttotal: 43.6s\tremaining: 3m 3s\n",
            "480:\tlearn: 20046.2295126\ttest: 19809.9564237\tbest: 19809.9564237 (480)\ttotal: 43.7s\tremaining: 3m 3s\n",
            "481:\ttotal: 43.8s\tremaining: 3m 3s\n",
            "482:\ttotal: 43.9s\tremaining: 3m 3s\n",
            "483:\ttotal: 43.9s\tremaining: 3m 2s\n",
            "484:\ttotal: 44s\tremaining: 3m 2s\n",
            "485:\tlearn: 19952.7699839\ttest: 19718.9482645\tbest: 19718.9482645 (485)\ttotal: 44.1s\tremaining: 3m 2s\n",
            "486:\ttotal: 44.3s\tremaining: 3m 3s\n",
            "487:\ttotal: 44.4s\tremaining: 3m 3s\n",
            "488:\ttotal: 44.6s\tremaining: 3m 3s\n",
            "489:\ttotal: 44.9s\tremaining: 3m 4s\n",
            "490:\tlearn: 19847.9721921\ttest: 19619.0247404\tbest: 19619.0247404 (490)\ttotal: 45.1s\tremaining: 3m 4s\n",
            "491:\ttotal: 45.3s\tremaining: 3m 4s\n",
            "492:\ttotal: 45.5s\tremaining: 3m 5s\n",
            "493:\ttotal: 45.7s\tremaining: 3m 5s\n",
            "494:\ttotal: 45.9s\tremaining: 3m 6s\n",
            "495:\tlearn: 19774.1344900\ttest: 19548.7254896\tbest: 19548.7254896 (495)\ttotal: 46.2s\tremaining: 3m 6s\n",
            "496:\ttotal: 46.5s\tremaining: 3m 7s\n",
            "497:\ttotal: 46.7s\tremaining: 3m 7s\n",
            "498:\ttotal: 47s\tremaining: 3m 8s\n",
            "499:\ttotal: 47.3s\tremaining: 3m 9s\n",
            "500:\tlearn: 19711.4704359\ttest: 19489.2517618\tbest: 19489.2517618 (500)\ttotal: 47.6s\tremaining: 3m 10s\n",
            "501:\ttotal: 47.8s\tremaining: 3m 10s\n",
            "502:\ttotal: 47.9s\tremaining: 3m 10s\n",
            "503:\ttotal: 48s\tremaining: 3m 10s\n",
            "504:\ttotal: 48.1s\tremaining: 3m 10s\n",
            "505:\tlearn: 19623.3484789\ttest: 19410.7243370\tbest: 19410.7243370 (505)\ttotal: 48.2s\tremaining: 3m 9s\n",
            "506:\ttotal: 48.3s\tremaining: 3m 9s\n",
            "507:\ttotal: 48.3s\tremaining: 3m 9s\n",
            "508:\ttotal: 48.4s\tremaining: 3m 9s\n",
            "509:\ttotal: 48.5s\tremaining: 3m 9s\n",
            "510:\tlearn: 19567.0873366\ttest: 19365.1147846\tbest: 19365.1147846 (510)\ttotal: 48.5s\tremaining: 3m 8s\n",
            "511:\ttotal: 48.6s\tremaining: 3m 8s\n",
            "512:\ttotal: 48.7s\tremaining: 3m 8s\n",
            "513:\ttotal: 48.8s\tremaining: 3m 8s\n",
            "514:\ttotal: 48.8s\tremaining: 3m 8s\n",
            "515:\tlearn: 19528.5642634\ttest: 19331.7465498\tbest: 19331.7465498 (515)\ttotal: 48.9s\tremaining: 3m 8s\n",
            "516:\ttotal: 49s\tremaining: 3m 7s\n",
            "517:\ttotal: 49.1s\tremaining: 3m 7s\n",
            "518:\ttotal: 49.1s\tremaining: 3m 7s\n",
            "519:\ttotal: 49.2s\tremaining: 3m 7s\n",
            "520:\tlearn: 19473.8328005\ttest: 19283.1501107\tbest: 19283.1501107 (520)\ttotal: 49.3s\tremaining: 3m 7s\n",
            "521:\ttotal: 49.4s\tremaining: 3m 7s\n",
            "522:\ttotal: 49.5s\tremaining: 3m 6s\n",
            "523:\ttotal: 49.5s\tremaining: 3m 6s\n",
            "524:\ttotal: 49.6s\tremaining: 3m 6s\n",
            "525:\tlearn: 19428.6816343\ttest: 19246.9347569\tbest: 19246.9347569 (525)\ttotal: 49.7s\tremaining: 3m 6s\n",
            "526:\ttotal: 49.7s\tremaining: 3m 6s\n",
            "527:\ttotal: 49.8s\tremaining: 3m 5s\n",
            "528:\ttotal: 49.9s\tremaining: 3m 5s\n",
            "529:\ttotal: 49.9s\tremaining: 3m 5s\n",
            "530:\tlearn: 19388.8400861\ttest: 19207.2368589\tbest: 19207.2368589 (530)\ttotal: 50s\tremaining: 3m 5s\n",
            "531:\ttotal: 50.1s\tremaining: 3m 5s\n",
            "532:\ttotal: 50.1s\tremaining: 3m 5s\n",
            "533:\ttotal: 50.2s\tremaining: 3m 4s\n",
            "534:\ttotal: 50.3s\tremaining: 3m 4s\n",
            "535:\tlearn: 19332.0866511\ttest: 19157.9894649\tbest: 19157.9894649 (535)\ttotal: 50.4s\tremaining: 3m 4s\n",
            "536:\ttotal: 50.4s\tremaining: 3m 4s\n",
            "537:\ttotal: 50.5s\tremaining: 3m 4s\n",
            "538:\ttotal: 50.6s\tremaining: 3m 4s\n",
            "539:\ttotal: 50.7s\tremaining: 3m 3s\n",
            "540:\tlearn: 19276.2401529\ttest: 19108.9116038\tbest: 19108.9116038 (540)\ttotal: 50.7s\tremaining: 3m 3s\n",
            "541:\ttotal: 50.8s\tremaining: 3m 3s\n",
            "542:\ttotal: 50.9s\tremaining: 3m 3s\n",
            "543:\ttotal: 50.9s\tremaining: 3m 3s\n",
            "544:\ttotal: 51s\tremaining: 3m 2s\n",
            "545:\tlearn: 19235.7215081\ttest: 19073.7794494\tbest: 19073.7794494 (545)\ttotal: 51.1s\tremaining: 3m 2s\n",
            "546:\ttotal: 51.1s\tremaining: 3m 2s\n",
            "547:\ttotal: 51.2s\tremaining: 3m 2s\n",
            "548:\ttotal: 51.3s\tremaining: 3m 2s\n",
            "549:\ttotal: 51.4s\tremaining: 3m 2s\n",
            "550:\tlearn: 19173.4410386\ttest: 19012.8200431\tbest: 19012.8200431 (550)\ttotal: 51.4s\tremaining: 3m 1s\n",
            "551:\ttotal: 51.5s\tremaining: 3m 1s\n",
            "552:\ttotal: 51.6s\tremaining: 3m 1s\n",
            "553:\ttotal: 51.7s\tremaining: 3m 1s\n",
            "554:\ttotal: 51.7s\tremaining: 3m 1s\n",
            "555:\tlearn: 19120.2563378\ttest: 18968.7466408\tbest: 18968.7466408 (555)\ttotal: 51.8s\tremaining: 3m 1s\n",
            "556:\ttotal: 51.9s\tremaining: 3m\n",
            "557:\ttotal: 52s\tremaining: 3m\n",
            "558:\ttotal: 52s\tremaining: 3m\n",
            "559:\ttotal: 52.1s\tremaining: 3m\n",
            "560:\tlearn: 19082.5128575\ttest: 18935.4211128\tbest: 18935.4211128 (560)\ttotal: 52.2s\tremaining: 3m\n",
            "561:\ttotal: 52.3s\tremaining: 3m\n",
            "562:\ttotal: 52.3s\tremaining: 3m\n",
            "563:\ttotal: 52.4s\tremaining: 2m 59s\n",
            "564:\ttotal: 52.5s\tremaining: 2m 59s\n",
            "565:\tlearn: 19034.2371076\ttest: 18890.0613303\tbest: 18890.0613303 (565)\ttotal: 52.6s\tremaining: 2m 59s\n",
            "566:\ttotal: 52.6s\tremaining: 2m 59s\n",
            "567:\ttotal: 52.7s\tremaining: 2m 59s\n",
            "568:\ttotal: 52.8s\tremaining: 2m 59s\n",
            "569:\ttotal: 52.9s\tremaining: 2m 58s\n",
            "570:\tlearn: 19006.1997149\ttest: 18864.4087881\tbest: 18864.4087881 (570)\ttotal: 52.9s\tremaining: 2m 58s\n",
            "571:\ttotal: 53s\tremaining: 2m 58s\n",
            "572:\ttotal: 53.1s\tremaining: 2m 58s\n",
            "573:\ttotal: 53.2s\tremaining: 2m 58s\n",
            "574:\ttotal: 53.2s\tremaining: 2m 58s\n",
            "575:\tlearn: 18977.2833389\ttest: 18846.0060461\tbest: 18846.0060461 (575)\ttotal: 53.3s\tremaining: 2m 58s\n",
            "576:\ttotal: 53.4s\tremaining: 2m 57s\n",
            "577:\ttotal: 53.5s\tremaining: 2m 57s\n",
            "578:\ttotal: 53.5s\tremaining: 2m 57s\n",
            "579:\ttotal: 53.6s\tremaining: 2m 57s\n",
            "580:\tlearn: 18915.2684279\ttest: 18792.4142073\tbest: 18792.4142073 (580)\ttotal: 53.7s\tremaining: 2m 57s\n",
            "581:\ttotal: 53.7s\tremaining: 2m 57s\n",
            "582:\ttotal: 53.8s\tremaining: 2m 56s\n",
            "583:\ttotal: 53.9s\tremaining: 2m 56s\n",
            "584:\ttotal: 54s\tremaining: 2m 56s\n",
            "585:\tlearn: 18853.4383208\ttest: 18737.9449382\tbest: 18737.9449382 (585)\ttotal: 54s\tremaining: 2m 56s\n",
            "586:\ttotal: 54.1s\tremaining: 2m 56s\n",
            "587:\ttotal: 54.2s\tremaining: 2m 56s\n",
            "588:\ttotal: 54.3s\tremaining: 2m 56s\n",
            "589:\ttotal: 54.3s\tremaining: 2m 55s\n",
            "590:\tlearn: 18814.0114168\ttest: 18705.0218892\tbest: 18705.0218892 (590)\ttotal: 54.4s\tremaining: 2m 55s\n",
            "591:\ttotal: 54.5s\tremaining: 2m 55s\n",
            "592:\ttotal: 54.6s\tremaining: 2m 55s\n",
            "593:\ttotal: 54.6s\tremaining: 2m 55s\n",
            "594:\ttotal: 54.7s\tremaining: 2m 55s\n",
            "595:\tlearn: 18763.9031575\ttest: 18657.2407615\tbest: 18657.2407615 (595)\ttotal: 54.8s\tremaining: 2m 55s\n",
            "596:\ttotal: 54.9s\tremaining: 2m 54s\n",
            "597:\ttotal: 55s\tremaining: 2m 54s\n",
            "598:\ttotal: 55s\tremaining: 2m 54s\n",
            "599:\ttotal: 55.1s\tremaining: 2m 54s\n",
            "600:\tlearn: 18700.7266220\ttest: 18601.7944129\tbest: 18601.7944129 (600)\ttotal: 55.2s\tremaining: 2m 54s\n",
            "601:\ttotal: 55.2s\tremaining: 2m 54s\n",
            "602:\ttotal: 55.3s\tremaining: 2m 54s\n",
            "603:\ttotal: 55.4s\tremaining: 2m 53s\n",
            "604:\ttotal: 55.5s\tremaining: 2m 53s\n",
            "605:\tlearn: 18661.4037672\ttest: 18567.9680508\tbest: 18567.9680508 (605)\ttotal: 55.5s\tremaining: 2m 53s\n",
            "606:\ttotal: 55.6s\tremaining: 2m 53s\n",
            "607:\ttotal: 55.7s\tremaining: 2m 53s\n",
            "608:\ttotal: 55.7s\tremaining: 2m 53s\n",
            "609:\ttotal: 55.8s\tremaining: 2m 52s\n",
            "610:\tlearn: 18634.4922806\ttest: 18543.6407130\tbest: 18543.6407130 (610)\ttotal: 55.9s\tremaining: 2m 52s\n",
            "611:\ttotal: 56s\tremaining: 2m 52s\n",
            "612:\ttotal: 56s\tremaining: 2m 52s\n",
            "613:\ttotal: 56.1s\tremaining: 2m 52s\n",
            "614:\ttotal: 56.2s\tremaining: 2m 52s\n",
            "615:\tlearn: 18596.6695987\ttest: 18507.9585469\tbest: 18507.9585469 (615)\ttotal: 56.3s\tremaining: 2m 52s\n",
            "616:\ttotal: 56.4s\tremaining: 2m 52s\n",
            "617:\ttotal: 56.5s\tremaining: 2m 51s\n",
            "618:\ttotal: 56.5s\tremaining: 2m 51s\n",
            "619:\ttotal: 56.6s\tremaining: 2m 51s\n",
            "620:\tlearn: 18572.0099245\ttest: 18491.9590322\tbest: 18491.9590322 (620)\ttotal: 56.7s\tremaining: 2m 51s\n",
            "621:\ttotal: 56.8s\tremaining: 2m 51s\n",
            "622:\ttotal: 56.8s\tremaining: 2m 51s\n",
            "623:\ttotal: 56.9s\tremaining: 2m 51s\n",
            "624:\ttotal: 57s\tremaining: 2m 50s\n",
            "625:\tlearn: 18541.0125633\ttest: 18468.4172404\tbest: 18468.4172404 (625)\ttotal: 57.1s\tremaining: 2m 50s\n",
            "626:\ttotal: 57.1s\tremaining: 2m 50s\n",
            "627:\ttotal: 57.2s\tremaining: 2m 50s\n",
            "628:\ttotal: 57.3s\tremaining: 2m 50s\n",
            "629:\ttotal: 57.4s\tremaining: 2m 50s\n",
            "630:\tlearn: 18511.7157087\ttest: 18439.7596934\tbest: 18439.7596934 (630)\ttotal: 57.4s\tremaining: 2m 50s\n",
            "631:\ttotal: 57.5s\tremaining: 2m 49s\n",
            "632:\ttotal: 57.6s\tremaining: 2m 49s\n",
            "633:\ttotal: 57.6s\tremaining: 2m 49s\n",
            "634:\ttotal: 57.7s\tremaining: 2m 49s\n",
            "635:\tlearn: 18453.2679790\ttest: 18384.9060633\tbest: 18384.9060633 (635)\ttotal: 57.9s\tremaining: 2m 49s\n",
            "636:\ttotal: 58.1s\tremaining: 2m 50s\n",
            "637:\ttotal: 58.3s\tremaining: 2m 50s\n",
            "638:\ttotal: 58.4s\tremaining: 2m 50s\n",
            "639:\ttotal: 58.6s\tremaining: 2m 50s\n",
            "640:\tlearn: 18412.1172738\ttest: 18346.5320756\tbest: 18346.5320756 (640)\ttotal: 58.7s\tremaining: 2m 50s\n",
            "641:\ttotal: 58.9s\tremaining: 2m 50s\n",
            "642:\ttotal: 59.1s\tremaining: 2m 50s\n",
            "643:\ttotal: 59.3s\tremaining: 2m 51s\n",
            "644:\ttotal: 59.6s\tremaining: 2m 51s\n",
            "645:\tlearn: 18347.4219297\ttest: 18290.4024184\tbest: 18290.4024184 (645)\ttotal: 59.8s\tremaining: 2m 51s\n",
            "646:\ttotal: 1m\tremaining: 2m 51s\n",
            "647:\ttotal: 1m\tremaining: 2m 51s\n",
            "648:\ttotal: 1m\tremaining: 2m 52s\n",
            "649:\ttotal: 1m\tremaining: 2m 52s\n",
            "650:\tlearn: 18267.6580788\ttest: 18215.4002447\tbest: 18215.4002447 (650)\ttotal: 1m\tremaining: 2m 52s\n",
            "651:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "652:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "653:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "654:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "655:\tlearn: 18217.9753344\ttest: 18168.9210268\tbest: 18168.9210268 (655)\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "656:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "657:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "658:\ttotal: 1m 1s\tremaining: 2m 53s\n",
            "659:\ttotal: 1m 2s\tremaining: 2m 53s\n",
            "660:\tlearn: 18176.7034972\ttest: 18130.7359843\tbest: 18130.7359843 (660)\ttotal: 1m 2s\tremaining: 2m 52s\n",
            "661:\ttotal: 1m 2s\tremaining: 2m 52s\n",
            "662:\ttotal: 1m 2s\tremaining: 2m 52s\n",
            "663:\ttotal: 1m 2s\tremaining: 2m 52s\n",
            "664:\ttotal: 1m 2s\tremaining: 2m 52s\n",
            "665:\tlearn: 18118.5865510\ttest: 18077.0134369\tbest: 18077.0134369 (665)\ttotal: 1m 2s\tremaining: 2m 52s\n",
            "666:\ttotal: 1m 2s\tremaining: 2m 51s\n",
            "667:\ttotal: 1m 2s\tremaining: 2m 51s\n",
            "668:\ttotal: 1m 2s\tremaining: 2m 51s\n",
            "669:\ttotal: 1m 2s\tremaining: 2m 51s\n",
            "670:\tlearn: 18067.2048530\ttest: 18029.2568979\tbest: 18029.2568979 (670)\ttotal: 1m 2s\tremaining: 2m 51s\n",
            "671:\ttotal: 1m 2s\tremaining: 2m 51s\n",
            "672:\ttotal: 1m 3s\tremaining: 2m 51s\n",
            "673:\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "674:\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "675:\tlearn: 18016.1632564\ttest: 17984.7486629\tbest: 17984.7486629 (675)\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "676:\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "677:\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "678:\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "679:\ttotal: 1m 3s\tremaining: 2m 50s\n",
            "680:\tlearn: 17973.1520883\ttest: 17944.5356951\tbest: 17944.5356951 (680)\ttotal: 1m 3s\tremaining: 2m 49s\n",
            "681:\ttotal: 1m 3s\tremaining: 2m 49s\n",
            "682:\ttotal: 1m 3s\tremaining: 2m 49s\n",
            "683:\ttotal: 1m 3s\tremaining: 2m 49s\n",
            "684:\ttotal: 1m 3s\tremaining: 2m 49s\n",
            "685:\tlearn: 17955.6593891\ttest: 17929.4058459\tbest: 17929.4058459 (685)\ttotal: 1m 3s\tremaining: 2m 49s\n",
            "686:\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "687:\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "688:\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "689:\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "690:\tlearn: 17923.3746125\ttest: 17901.5131386\tbest: 17901.5131386 (690)\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "691:\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "692:\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "693:\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "694:\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "695:\tlearn: 17859.2678456\ttest: 17839.6555957\tbest: 17839.6555957 (695)\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "696:\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "697:\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "698:\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "699:\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "700:\tlearn: 17813.7377536\ttest: 17798.8563802\tbest: 17798.8563802 (700)\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "701:\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "702:\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "703:\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "704:\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "705:\tlearn: 17769.5012891\ttest: 17757.5576248\tbest: 17757.5576248 (705)\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "706:\ttotal: 1m 5s\tremaining: 2m 46s\n",
            "707:\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "708:\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "709:\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "710:\tlearn: 17729.0074919\ttest: 17723.3430192\tbest: 17723.3430192 (710)\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "711:\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "712:\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "713:\ttotal: 1m 5s\tremaining: 2m 45s\n",
            "714:\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "715:\tlearn: 17666.2129880\ttest: 17666.5378184\tbest: 17666.5378184 (715)\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "716:\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "717:\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "718:\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "719:\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "720:\tlearn: 17661.8646607\ttest: 17661.8879554\tbest: 17661.8879554 (720)\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "721:\ttotal: 1m 6s\tremaining: 2m 44s\n",
            "722:\ttotal: 1m 6s\tremaining: 2m 43s\n",
            "723:\ttotal: 1m 6s\tremaining: 2m 43s\n",
            "724:\ttotal: 1m 6s\tremaining: 2m 43s\n",
            "725:\tlearn: 17614.2753253\ttest: 17623.3457288\tbest: 17623.3457288 (725)\ttotal: 1m 6s\tremaining: 2m 43s\n",
            "726:\ttotal: 1m 6s\tremaining: 2m 43s\n",
            "727:\ttotal: 1m 7s\tremaining: 2m 43s\n",
            "728:\ttotal: 1m 7s\tremaining: 2m 43s\n",
            "729:\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "730:\tlearn: 17560.7520762\ttest: 17576.4937972\tbest: 17576.4937972 (730)\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "731:\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "732:\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "733:\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "734:\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "735:\tlearn: 17512.2651218\ttest: 17534.5492634\tbest: 17534.5492634 (735)\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "736:\ttotal: 1m 7s\tremaining: 2m 42s\n",
            "737:\ttotal: 1m 7s\tremaining: 2m 41s\n",
            "738:\ttotal: 1m 7s\tremaining: 2m 41s\n",
            "739:\ttotal: 1m 7s\tremaining: 2m 41s\n",
            "740:\tlearn: 17477.6074009\ttest: 17506.5685543\tbest: 17506.5685543 (740)\ttotal: 1m 8s\tremaining: 2m 41s\n",
            "741:\ttotal: 1m 8s\tremaining: 2m 41s\n",
            "742:\ttotal: 1m 8s\tremaining: 2m 41s\n",
            "743:\ttotal: 1m 8s\tremaining: 2m 41s\n",
            "744:\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "745:\tlearn: 17450.7098911\ttest: 17481.3586096\tbest: 17481.3586096 (745)\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "746:\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "747:\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "748:\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "749:\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "750:\tlearn: 17417.2091359\ttest: 17450.4324264\tbest: 17450.4324264 (750)\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "751:\ttotal: 1m 8s\tremaining: 2m 40s\n",
            "752:\ttotal: 1m 8s\tremaining: 2m 39s\n",
            "753:\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "754:\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "755:\tlearn: 17372.3670114\ttest: 17412.5968031\tbest: 17412.5968031 (755)\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "756:\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "757:\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "758:\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "759:\ttotal: 1m 9s\tremaining: 2m 39s\n",
            "760:\tlearn: 17327.6988201\ttest: 17374.0390468\tbest: 17374.0390468 (760)\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "761:\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "762:\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "763:\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "764:\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "765:\tlearn: 17307.5474294\ttest: 17359.7853337\tbest: 17359.7853337 (765)\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "766:\ttotal: 1m 9s\tremaining: 2m 38s\n",
            "767:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "768:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "769:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "770:\tlearn: 17265.5286117\ttest: 17321.8034719\tbest: 17321.8034719 (770)\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "771:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "772:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "773:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "774:\ttotal: 1m 10s\tremaining: 2m 37s\n",
            "775:\tlearn: 17232.8913100\ttest: 17293.6144055\tbest: 17293.6144055 (775)\ttotal: 1m 10s\tremaining: 2m 36s\n",
            "776:\ttotal: 1m 10s\tremaining: 2m 36s\n",
            "777:\ttotal: 1m 10s\tremaining: 2m 36s\n",
            "778:\ttotal: 1m 10s\tremaining: 2m 36s\n",
            "779:\ttotal: 1m 10s\tremaining: 2m 36s\n",
            "780:\tlearn: 17214.5056508\ttest: 17276.9380529\tbest: 17276.9380529 (780)\ttotal: 1m 11s\tremaining: 2m 36s\n",
            "781:\ttotal: 1m 11s\tremaining: 2m 36s\n",
            "782:\ttotal: 1m 11s\tremaining: 2m 36s\n",
            "783:\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "784:\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "785:\tlearn: 17195.4646365\ttest: 17260.2629945\tbest: 17260.2629945 (785)\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "786:\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "787:\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "788:\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "789:\ttotal: 1m 11s\tremaining: 2m 35s\n",
            "790:\tlearn: 17170.8531044\ttest: 17236.2527223\tbest: 17236.2527223 (790)\ttotal: 1m 12s\tremaining: 2m 35s\n",
            "791:\ttotal: 1m 12s\tremaining: 2m 35s\n",
            "792:\ttotal: 1m 12s\tremaining: 2m 36s\n",
            "793:\ttotal: 1m 12s\tremaining: 2m 36s\n",
            "794:\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "795:\tlearn: 17120.6206072\ttest: 17191.6086020\tbest: 17191.6086020 (795)\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "796:\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "797:\ttotal: 1m 13s\tremaining: 2m 37s\n",
            "798:\ttotal: 1m 14s\tremaining: 2m 37s\n",
            "799:\ttotal: 1m 14s\tremaining: 2m 37s\n",
            "800:\tlearn: 17080.5569838\ttest: 17158.8292032\tbest: 17158.8292032 (800)\ttotal: 1m 14s\tremaining: 2m 38s\n",
            "801:\ttotal: 1m 14s\tremaining: 2m 38s\n",
            "802:\ttotal: 1m 14s\tremaining: 2m 38s\n",
            "803:\ttotal: 1m 15s\tremaining: 2m 38s\n",
            "804:\ttotal: 1m 15s\tremaining: 2m 38s\n",
            "805:\tlearn: 17038.5117656\ttest: 17121.0764051\tbest: 17121.0764051 (805)\ttotal: 1m 15s\tremaining: 2m 38s\n",
            "806:\ttotal: 1m 15s\tremaining: 2m 38s\n",
            "807:\ttotal: 1m 15s\tremaining: 2m 38s\n",
            "808:\ttotal: 1m 15s\tremaining: 2m 38s\n",
            "809:\ttotal: 1m 15s\tremaining: 2m 37s\n",
            "810:\tlearn: 16997.6002184\ttest: 17086.5757934\tbest: 17086.5757934 (810)\ttotal: 1m 15s\tremaining: 2m 37s\n",
            "811:\ttotal: 1m 15s\tremaining: 2m 37s\n",
            "812:\ttotal: 1m 15s\tremaining: 2m 37s\n",
            "813:\ttotal: 1m 15s\tremaining: 2m 37s\n",
            "814:\ttotal: 1m 16s\tremaining: 2m 37s\n",
            "815:\tlearn: 16955.8019230\ttest: 17048.1500399\tbest: 17048.1500399 (815)\ttotal: 1m 16s\tremaining: 2m 37s\n",
            "816:\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "817:\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "818:\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "819:\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "820:\tlearn: 16910.2407716\ttest: 17008.5673208\tbest: 17008.5673208 (820)\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "821:\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "822:\ttotal: 1m 16s\tremaining: 2m 36s\n",
            "823:\ttotal: 1m 16s\tremaining: 2m 35s\n",
            "824:\ttotal: 1m 16s\tremaining: 2m 35s\n",
            "825:\tlearn: 16867.1084716\ttest: 16968.8189511\tbest: 16968.8189511 (825)\ttotal: 1m 16s\tremaining: 2m 35s\n",
            "826:\ttotal: 1m 16s\tremaining: 2m 35s\n",
            "827:\ttotal: 1m 16s\tremaining: 2m 35s\n",
            "828:\ttotal: 1m 17s\tremaining: 2m 35s\n",
            "829:\ttotal: 1m 17s\tremaining: 2m 35s\n",
            "830:\tlearn: 16855.3089751\ttest: 16960.3681843\tbest: 16960.3681843 (830)\ttotal: 1m 17s\tremaining: 2m 35s\n",
            "831:\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "832:\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "833:\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "834:\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "835:\tlearn: 16836.4729534\ttest: 16943.1495850\tbest: 16943.1495850 (835)\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "836:\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "837:\ttotal: 1m 17s\tremaining: 2m 34s\n",
            "838:\ttotal: 1m 17s\tremaining: 2m 33s\n",
            "839:\ttotal: 1m 17s\tremaining: 2m 33s\n",
            "840:\tlearn: 16801.1086293\ttest: 16910.9616104\tbest: 16910.9616104 (840)\ttotal: 1m 17s\tremaining: 2m 33s\n",
            "841:\ttotal: 1m 17s\tremaining: 2m 33s\n",
            "842:\ttotal: 1m 18s\tremaining: 2m 33s\n",
            "843:\ttotal: 1m 18s\tremaining: 2m 33s\n",
            "844:\ttotal: 1m 18s\tremaining: 2m 33s\n",
            "845:\tlearn: 16773.3538779\ttest: 16885.2961267\tbest: 16885.2961267 (845)\ttotal: 1m 18s\tremaining: 2m 33s\n",
            "846:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "847:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "848:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "849:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "850:\tlearn: 16735.2796142\ttest: 16854.0606428\tbest: 16854.0606428 (850)\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "851:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "852:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "853:\ttotal: 1m 18s\tremaining: 2m 32s\n",
            "854:\ttotal: 1m 18s\tremaining: 2m 31s\n",
            "855:\tlearn: 16696.4987473\ttest: 16818.5311859\tbest: 16818.5311859 (855)\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "856:\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "857:\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "858:\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "859:\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "860:\tlearn: 16682.7254208\ttest: 16809.4488762\tbest: 16809.4488762 (860)\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "861:\ttotal: 1m 19s\tremaining: 2m 31s\n",
            "862:\ttotal: 1m 19s\tremaining: 2m 30s\n",
            "863:\ttotal: 1m 19s\tremaining: 2m 30s\n",
            "864:\ttotal: 1m 19s\tremaining: 2m 30s\n",
            "865:\tlearn: 16657.4756165\ttest: 16789.7545775\tbest: 16789.7545775 (865)\ttotal: 1m 19s\tremaining: 2m 30s\n",
            "866:\ttotal: 1m 19s\tremaining: 2m 30s\n",
            "867:\ttotal: 1m 19s\tremaining: 2m 30s\n",
            "868:\ttotal: 1m 20s\tremaining: 2m 30s\n",
            "869:\ttotal: 1m 20s\tremaining: 2m 30s\n",
            "870:\tlearn: 16633.4495556\ttest: 16769.0275511\tbest: 16769.0275511 (870)\ttotal: 1m 20s\tremaining: 2m 30s\n",
            "871:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "872:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "873:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "874:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "875:\tlearn: 16607.2415663\ttest: 16746.4343676\tbest: 16746.4343676 (875)\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "876:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "877:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "878:\ttotal: 1m 20s\tremaining: 2m 29s\n",
            "879:\ttotal: 1m 20s\tremaining: 2m 28s\n",
            "880:\tlearn: 16579.8672935\ttest: 16721.4043091\tbest: 16721.4043091 (880)\ttotal: 1m 20s\tremaining: 2m 28s\n",
            "881:\ttotal: 1m 21s\tremaining: 2m 28s\n",
            "882:\ttotal: 1m 21s\tremaining: 2m 28s\n",
            "883:\ttotal: 1m 21s\tremaining: 2m 28s\n",
            "884:\ttotal: 1m 21s\tremaining: 2m 28s\n",
            "885:\tlearn: 16545.3462343\ttest: 16694.3696199\tbest: 16694.3696199 (885)\ttotal: 1m 21s\tremaining: 2m 28s\n",
            "886:\ttotal: 1m 21s\tremaining: 2m 28s\n",
            "887:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "888:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "889:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "890:\tlearn: 16516.6876520\ttest: 16670.3218175\tbest: 16670.3218175 (890)\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "891:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "892:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "893:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "894:\ttotal: 1m 21s\tremaining: 2m 27s\n",
            "895:\tlearn: 16480.8900482\ttest: 16638.9530367\tbest: 16638.9530367 (895)\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "896:\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "897:\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "898:\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "899:\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "900:\tlearn: 16449.8693925\ttest: 16617.4598764\tbest: 16617.4598764 (900)\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "901:\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "902:\ttotal: 1m 22s\tremaining: 2m 26s\n",
            "903:\ttotal: 1m 22s\tremaining: 2m 25s\n",
            "904:\ttotal: 1m 22s\tremaining: 2m 25s\n",
            "905:\tlearn: 16412.0327338\ttest: 16588.1345506\tbest: 16588.1345506 (905)\ttotal: 1m 22s\tremaining: 2m 25s\n",
            "906:\ttotal: 1m 22s\tremaining: 2m 25s\n",
            "907:\ttotal: 1m 22s\tremaining: 2m 25s\n",
            "908:\ttotal: 1m 23s\tremaining: 2m 25s\n",
            "909:\ttotal: 1m 23s\tremaining: 2m 25s\n",
            "910:\tlearn: 16369.6458613\ttest: 16548.3098264\tbest: 16548.3098264 (910)\ttotal: 1m 23s\tremaining: 2m 25s\n",
            "911:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "912:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "913:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "914:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "915:\tlearn: 16338.6174406\ttest: 16525.2481624\tbest: 16525.2481624 (915)\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "916:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "917:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "918:\ttotal: 1m 23s\tremaining: 2m 24s\n",
            "919:\ttotal: 1m 23s\tremaining: 2m 23s\n",
            "920:\tlearn: 16306.7224605\ttest: 16500.0641006\tbest: 16500.0641006 (920)\ttotal: 1m 23s\tremaining: 2m 23s\n",
            "921:\ttotal: 1m 23s\tremaining: 2m 23s\n",
            "922:\ttotal: 1m 24s\tremaining: 2m 23s\n",
            "923:\ttotal: 1m 24s\tremaining: 2m 23s\n",
            "924:\ttotal: 1m 24s\tremaining: 2m 23s\n",
            "925:\tlearn: 16285.8101125\ttest: 16481.1165236\tbest: 16481.1165236 (925)\ttotal: 1m 24s\tremaining: 2m 23s\n",
            "926:\ttotal: 1m 24s\tremaining: 2m 23s\n",
            "927:\ttotal: 1m 24s\tremaining: 2m 23s\n",
            "928:\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "929:\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "930:\tlearn: 16270.8580788\ttest: 16469.1456823\tbest: 16469.1456823 (930)\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "931:\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "932:\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "933:\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "934:\ttotal: 1m 24s\tremaining: 2m 22s\n",
            "935:\tlearn: 16233.0571385\ttest: 16438.8691599\tbest: 16438.8691599 (935)\ttotal: 1m 25s\tremaining: 2m 22s\n",
            "936:\ttotal: 1m 25s\tremaining: 2m 22s\n",
            "937:\ttotal: 1m 25s\tremaining: 2m 21s\n",
            "938:\ttotal: 1m 25s\tremaining: 2m 21s\n",
            "939:\ttotal: 1m 25s\tremaining: 2m 21s\n",
            "940:\tlearn: 16203.5723740\ttest: 16416.9346962\tbest: 16416.9346962 (940)\ttotal: 1m 25s\tremaining: 2m 21s\n",
            "941:\ttotal: 1m 25s\tremaining: 2m 21s\n",
            "942:\ttotal: 1m 26s\tremaining: 2m 22s\n",
            "943:\ttotal: 1m 26s\tremaining: 2m 22s\n",
            "944:\ttotal: 1m 26s\tremaining: 2m 22s\n",
            "945:\tlearn: 16185.3715187\ttest: 16404.6856138\tbest: 16404.6856138 (945)\ttotal: 1m 26s\tremaining: 2m 22s\n",
            "946:\ttotal: 1m 27s\tremaining: 2m 22s\n",
            "947:\ttotal: 1m 27s\tremaining: 2m 23s\n",
            "948:\ttotal: 1m 28s\tremaining: 2m 24s\n",
            "949:\ttotal: 1m 28s\tremaining: 2m 24s\n",
            "950:\tlearn: 16160.1356911\ttest: 16382.6722072\tbest: 16382.6722072 (950)\ttotal: 1m 28s\tremaining: 2m 24s\n",
            "951:\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "952:\ttotal: 1m 29s\tremaining: 2m 25s\n",
            "953:\ttotal: 1m 29s\tremaining: 2m 25s\n",
            "954:\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "955:\tlearn: 16144.5329613\ttest: 16372.6257596\tbest: 16372.6257596 (955)\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "956:\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "957:\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "958:\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "959:\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "960:\tlearn: 16117.6152628\ttest: 16351.6761402\tbest: 16351.6761402 (960)\ttotal: 1m 29s\tremaining: 2m 24s\n",
            "961:\ttotal: 1m 30s\tremaining: 2m 24s\n",
            "962:\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "963:\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "964:\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "965:\tlearn: 16093.9929752\ttest: 16332.4852639\tbest: 16332.4852639 (965)\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "966:\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "967:\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "968:\ttotal: 1m 30s\tremaining: 2m 23s\n",
            "969:\ttotal: 1m 30s\tremaining: 2m 22s\n",
            "970:\tlearn: 16047.5342049\ttest: 16289.9946010\tbest: 16289.9946010 (970)\ttotal: 1m 30s\tremaining: 2m 22s\n",
            "971:\ttotal: 1m 30s\tremaining: 2m 22s\n",
            "972:\ttotal: 1m 30s\tremaining: 2m 22s\n",
            "973:\ttotal: 1m 30s\tremaining: 2m 22s\n",
            "974:\ttotal: 1m 31s\tremaining: 2m 22s\n",
            "975:\tlearn: 16012.0891261\ttest: 16257.3834410\tbest: 16257.3834410 (975)\ttotal: 1m 31s\tremaining: 2m 22s\n",
            "976:\ttotal: 1m 31s\tremaining: 2m 22s\n",
            "977:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "978:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "979:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "980:\tlearn: 15987.3968395\ttest: 16236.4946465\tbest: 16236.4946465 (980)\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "981:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "982:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "983:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "984:\ttotal: 1m 31s\tremaining: 2m 21s\n",
            "985:\tlearn: 15974.9823653\ttest: 16227.8536403\tbest: 16227.8536403 (985)\ttotal: 1m 31s\tremaining: 2m 20s\n",
            "986:\ttotal: 1m 31s\tremaining: 2m 20s\n",
            "987:\ttotal: 1m 31s\tremaining: 2m 20s\n",
            "988:\ttotal: 1m 32s\tremaining: 2m 20s\n",
            "989:\ttotal: 1m 32s\tremaining: 2m 20s\n",
            "990:\tlearn: 15958.4198975\ttest: 16214.9096019\tbest: 16214.9096019 (990)\ttotal: 1m 32s\tremaining: 2m 20s\n",
            "991:\ttotal: 1m 32s\tremaining: 2m 20s\n",
            "992:\ttotal: 1m 32s\tremaining: 2m 20s\n",
            "993:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "994:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "995:\tlearn: 15930.2085717\ttest: 16189.1379983\tbest: 16189.1379983 (995)\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "996:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "997:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "998:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "999:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "1000:\tlearn: 15905.8035852\ttest: 16170.1386555\tbest: 16170.1386555 (1000)\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "1001:\ttotal: 1m 32s\tremaining: 2m 19s\n",
            "1002:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1003:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1004:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1005:\tlearn: 15875.6945252\ttest: 16151.3657881\tbest: 16151.3657881 (1005)\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1006:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1007:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1008:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1009:\ttotal: 1m 33s\tremaining: 2m 18s\n",
            "1010:\tlearn: 15851.8159968\ttest: 16129.3886176\tbest: 16129.3886176 (1010)\ttotal: 1m 33s\tremaining: 2m 17s\n",
            "1011:\ttotal: 1m 33s\tremaining: 2m 17s\n",
            "1012:\ttotal: 1m 33s\tremaining: 2m 17s\n",
            "1013:\ttotal: 1m 33s\tremaining: 2m 17s\n",
            "1014:\ttotal: 1m 33s\tremaining: 2m 17s\n",
            "1015:\tlearn: 15830.0510662\ttest: 16112.8041494\tbest: 16112.8041494 (1015)\ttotal: 1m 34s\tremaining: 2m 17s\n",
            "1016:\ttotal: 1m 34s\tremaining: 2m 17s\n",
            "1017:\ttotal: 1m 34s\tremaining: 2m 17s\n",
            "1018:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1019:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1020:\tlearn: 15810.8904729\ttest: 16098.3343141\tbest: 16098.3343141 (1020)\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1021:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1022:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1023:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1024:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1025:\tlearn: 15787.1066760\ttest: 16078.9596692\tbest: 16078.9596692 (1025)\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1026:\ttotal: 1m 34s\tremaining: 2m 16s\n",
            "1027:\ttotal: 1m 34s\tremaining: 2m 15s\n",
            "1028:\ttotal: 1m 34s\tremaining: 2m 15s\n",
            "1029:\ttotal: 1m 35s\tremaining: 2m 15s\n",
            "1030:\tlearn: 15751.3665322\ttest: 16045.8334395\tbest: 16045.8334395 (1030)\ttotal: 1m 35s\tremaining: 2m 15s\n",
            "1031:\ttotal: 1m 35s\tremaining: 2m 15s\n",
            "1032:\ttotal: 1m 35s\tremaining: 2m 15s\n",
            "1033:\ttotal: 1m 35s\tremaining: 2m 15s\n",
            "1034:\ttotal: 1m 35s\tremaining: 2m 15s\n",
            "1035:\tlearn: 15723.3633049\ttest: 16023.3489642\tbest: 16023.3489642 (1035)\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1036:\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1037:\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1038:\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1039:\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1040:\tlearn: 15706.6284570\ttest: 16012.2206113\tbest: 16012.2206113 (1040)\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1041:\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1042:\ttotal: 1m 35s\tremaining: 2m 14s\n",
            "1043:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1044:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1045:\tlearn: 15677.5334890\ttest: 15985.7165216\tbest: 15985.7165216 (1045)\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1046:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1047:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1048:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1049:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1050:\tlearn: 15642.0449271\ttest: 15956.7677920\tbest: 15956.7677920 (1050)\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1051:\ttotal: 1m 36s\tremaining: 2m 13s\n",
            "1052:\ttotal: 1m 37s\tremaining: 2m 13s\n",
            "1053:\ttotal: 1m 37s\tremaining: 2m 13s\n",
            "1054:\ttotal: 1m 37s\tremaining: 2m 13s\n",
            "1055:\tlearn: 15621.4804453\ttest: 15939.1531843\tbest: 15939.1531843 (1055)\ttotal: 1m 37s\tremaining: 2m 13s\n",
            "1056:\ttotal: 1m 37s\tremaining: 2m 12s\n",
            "1057:\ttotal: 1m 37s\tremaining: 2m 12s\n",
            "1058:\ttotal: 1m 37s\tremaining: 2m 13s\n",
            "1059:\ttotal: 1m 37s\tremaining: 2m 13s\n",
            "1060:\tlearn: 15608.7491644\ttest: 15930.8085373\tbest: 15930.8085373 (1060)\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1061:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1062:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1063:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1064:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1065:\tlearn: 15585.0647578\ttest: 15911.8531954\tbest: 15911.8531954 (1065)\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1066:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1067:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1068:\ttotal: 1m 38s\tremaining: 2m 12s\n",
            "1069:\ttotal: 1m 38s\tremaining: 2m 11s\n",
            "1070:\tlearn: 15567.9090540\ttest: 15897.6098355\tbest: 15897.6098355 (1070)\ttotal: 1m 38s\tremaining: 2m 11s\n",
            "1071:\ttotal: 1m 38s\tremaining: 2m 11s\n",
            "1072:\ttotal: 1m 38s\tremaining: 2m 11s\n",
            "1073:\ttotal: 1m 38s\tremaining: 2m 11s\n",
            "1074:\ttotal: 1m 39s\tremaining: 2m 11s\n",
            "1075:\tlearn: 15554.6140435\ttest: 15888.0072796\tbest: 15888.0072796 (1075)\ttotal: 1m 39s\tremaining: 2m 11s\n",
            "1076:\ttotal: 1m 39s\tremaining: 2m 11s\n",
            "1077:\ttotal: 1m 39s\tremaining: 2m 11s\n",
            "1078:\ttotal: 1m 39s\tremaining: 2m 11s\n",
            "1079:\ttotal: 1m 39s\tremaining: 2m 11s\n",
            "1080:\tlearn: 15526.7102066\ttest: 15862.0428483\tbest: 15862.0428483 (1080)\ttotal: 1m 40s\tremaining: 2m 11s\n",
            "1081:\ttotal: 1m 40s\tremaining: 2m 11s\n",
            "1082:\ttotal: 1m 40s\tremaining: 2m 11s\n",
            "1083:\ttotal: 1m 40s\tremaining: 2m 11s\n",
            "1084:\ttotal: 1m 40s\tremaining: 2m 11s\n",
            "1085:\tlearn: 15509.0140678\ttest: 15847.1446510\tbest: 15847.1446510 (1085)\ttotal: 1m 41s\tremaining: 2m 11s\n",
            "1086:\ttotal: 1m 41s\tremaining: 2m 11s\n",
            "1087:\ttotal: 1m 41s\tremaining: 2m 12s\n",
            "1088:\ttotal: 1m 41s\tremaining: 2m 12s\n",
            "1089:\ttotal: 1m 42s\tremaining: 2m 12s\n",
            "1090:\tlearn: 15490.5461009\ttest: 15833.4331847\tbest: 15833.4331847 (1090)\ttotal: 1m 42s\tremaining: 2m 12s\n",
            "1091:\ttotal: 1m 42s\tremaining: 2m 12s\n",
            "1092:\ttotal: 1m 43s\tremaining: 2m 12s\n",
            "1093:\ttotal: 1m 43s\tremaining: 2m 12s\n",
            "1094:\ttotal: 1m 43s\tremaining: 2m 12s\n",
            "1095:\tlearn: 15472.6232279\ttest: 15818.3615720\tbest: 15818.3615720 (1095)\ttotal: 1m 43s\tremaining: 2m 12s\n",
            "1096:\ttotal: 1m 43s\tremaining: 2m 12s\n",
            "1097:\ttotal: 1m 43s\tremaining: 2m 12s\n",
            "1098:\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1099:\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1100:\tlearn: 15449.5553520\ttest: 15797.5840739\tbest: 15797.5840739 (1100)\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1101:\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1102:\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1103:\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1104:\ttotal: 1m 43s\tremaining: 2m 11s\n",
            "1105:\tlearn: 15433.8330432\ttest: 15783.9593153\tbest: 15783.9593153 (1105)\ttotal: 1m 44s\tremaining: 2m 11s\n",
            "1106:\ttotal: 1m 44s\tremaining: 2m 11s\n",
            "1107:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1108:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1109:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1110:\tlearn: 15415.6182110\ttest: 15767.6867360\tbest: 15767.6867360 (1110)\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1111:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1112:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1113:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1114:\ttotal: 1m 44s\tremaining: 2m 10s\n",
            "1115:\tlearn: 15391.4228760\ttest: 15747.7620795\tbest: 15747.7620795 (1115)\ttotal: 1m 44s\tremaining: 2m 9s\n",
            "1116:\ttotal: 1m 44s\tremaining: 2m 9s\n",
            "1117:\ttotal: 1m 44s\tremaining: 2m 9s\n",
            "1118:\ttotal: 1m 45s\tremaining: 2m 9s\n",
            "1119:\ttotal: 1m 45s\tremaining: 2m 9s\n",
            "1120:\tlearn: 15372.4936759\ttest: 15734.6925091\tbest: 15734.6925091 (1120)\ttotal: 1m 45s\tremaining: 2m 9s\n",
            "1121:\ttotal: 1m 45s\tremaining: 2m 9s\n",
            "1122:\ttotal: 1m 45s\tremaining: 2m 9s\n",
            "1123:\ttotal: 1m 45s\tremaining: 2m 9s\n",
            "1124:\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1125:\tlearn: 15338.5332403\ttest: 15703.4156126\tbest: 15703.4156126 (1125)\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1126:\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1127:\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1128:\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1129:\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1130:\tlearn: 15317.6069156\ttest: 15684.5042717\tbest: 15684.5042717 (1130)\ttotal: 1m 45s\tremaining: 2m 8s\n",
            "1131:\ttotal: 1m 46s\tremaining: 2m 8s\n",
            "1132:\ttotal: 1m 46s\tremaining: 2m 8s\n",
            "1133:\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1134:\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1135:\tlearn: 15305.8431375\ttest: 15676.1182121\tbest: 15676.1182121 (1135)\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1136:\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1137:\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1138:\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1139:\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1140:\tlearn: 15292.6506233\ttest: 15667.6920339\tbest: 15667.6920339 (1140)\ttotal: 1m 46s\tremaining: 2m 7s\n",
            "1141:\ttotal: 1m 46s\tremaining: 2m 6s\n",
            "1142:\ttotal: 1m 46s\tremaining: 2m 6s\n",
            "1143:\ttotal: 1m 46s\tremaining: 2m 6s\n",
            "1144:\ttotal: 1m 46s\tremaining: 2m 6s\n",
            "1145:\tlearn: 15269.8762595\ttest: 15648.0559718\tbest: 15648.0559718 (1145)\ttotal: 1m 47s\tremaining: 2m 6s\n",
            "1146:\ttotal: 1m 47s\tremaining: 2m 6s\n",
            "1147:\ttotal: 1m 47s\tremaining: 2m 6s\n",
            "1148:\ttotal: 1m 47s\tremaining: 2m 6s\n",
            "1149:\ttotal: 1m 47s\tremaining: 2m 6s\n",
            "1150:\tlearn: 15246.7353939\ttest: 15627.6408343\tbest: 15627.6408343 (1150)\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1151:\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1152:\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1153:\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1154:\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1155:\tlearn: 15228.4211714\ttest: 15609.9990496\tbest: 15609.9990496 (1155)\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1156:\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1157:\ttotal: 1m 47s\tremaining: 2m 5s\n",
            "1158:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1159:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1160:\tlearn: 15210.7980224\ttest: 15594.7384917\tbest: 15594.7384917 (1160)\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1161:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1162:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1163:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1164:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1165:\tlearn: 15192.8767024\ttest: 15578.8813330\tbest: 15578.8813330 (1165)\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1166:\ttotal: 1m 48s\tremaining: 2m 4s\n",
            "1167:\ttotal: 1m 48s\tremaining: 2m 3s\n",
            "1168:\ttotal: 1m 48s\tremaining: 2m 3s\n",
            "1169:\ttotal: 1m 48s\tremaining: 2m 3s\n",
            "1170:\tlearn: 15179.6453638\ttest: 15567.0554359\tbest: 15567.0554359 (1170)\ttotal: 1m 48s\tremaining: 2m 3s\n",
            "1171:\ttotal: 1m 48s\tremaining: 2m 3s\n",
            "1172:\ttotal: 1m 49s\tremaining: 2m 3s\n",
            "1173:\ttotal: 1m 49s\tremaining: 2m 3s\n",
            "1174:\ttotal: 1m 49s\tremaining: 2m 3s\n",
            "1175:\tlearn: 15157.5356729\ttest: 15546.7852427\tbest: 15546.7852427 (1175)\ttotal: 1m 49s\tremaining: 2m 3s\n",
            "1176:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1177:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1178:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1179:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1180:\tlearn: 15133.8310777\ttest: 15527.1569454\tbest: 15527.1569454 (1180)\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1181:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1182:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1183:\ttotal: 1m 49s\tremaining: 2m 2s\n",
            "1184:\ttotal: 1m 49s\tremaining: 2m 1s\n",
            "1185:\tlearn: 15114.3148533\ttest: 15509.1126816\tbest: 15509.1126816 (1185)\ttotal: 1m 49s\tremaining: 2m 1s\n",
            "1186:\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1187:\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1188:\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1189:\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1190:\tlearn: 15095.2909218\ttest: 15492.5579787\tbest: 15492.5579787 (1190)\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1191:\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1192:\ttotal: 1m 50s\tremaining: 2m 1s\n",
            "1193:\ttotal: 1m 50s\tremaining: 2m\n",
            "1194:\ttotal: 1m 50s\tremaining: 2m\n",
            "1195:\tlearn: 15070.6824957\ttest: 15471.0505829\tbest: 15471.0505829 (1195)\ttotal: 1m 50s\tremaining: 2m\n",
            "1196:\ttotal: 1m 50s\tremaining: 2m\n",
            "1197:\ttotal: 1m 50s\tremaining: 2m\n",
            "1198:\ttotal: 1m 50s\tremaining: 2m\n",
            "1199:\ttotal: 1m 50s\tremaining: 2m\n",
            "1200:\tlearn: 15053.4506961\ttest: 15456.4189794\tbest: 15456.4189794 (1200)\ttotal: 1m 51s\tremaining: 2m\n",
            "1201:\ttotal: 1m 51s\tremaining: 2m\n",
            "1202:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1203:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1204:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1205:\tlearn: 15026.4864084\ttest: 15433.5268485\tbest: 15433.5268485 (1205)\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1206:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1207:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1208:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1209:\ttotal: 1m 51s\tremaining: 1m 59s\n",
            "1210:\tlearn: 15009.8447390\ttest: 15421.6155378\tbest: 15421.6155378 (1210)\ttotal: 1m 51s\tremaining: 1m 58s\n",
            "1211:\ttotal: 1m 51s\tremaining: 1m 58s\n",
            "1212:\ttotal: 1m 51s\tremaining: 1m 58s\n",
            "1213:\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1214:\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1215:\tlearn: 14980.2497134\ttest: 15397.5767944\tbest: 15397.5767944 (1215)\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1216:\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1217:\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1218:\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1219:\ttotal: 1m 52s\tremaining: 1m 58s\n",
            "1220:\tlearn: 14955.9922594\ttest: 15378.3975654\tbest: 15378.3975654 (1220)\ttotal: 1m 52s\tremaining: 1m 57s\n",
            "1221:\ttotal: 1m 52s\tremaining: 1m 57s\n",
            "1222:\ttotal: 1m 52s\tremaining: 1m 57s\n",
            "1223:\ttotal: 1m 52s\tremaining: 1m 57s\n",
            "1224:\ttotal: 1m 52s\tremaining: 1m 57s\n",
            "1225:\tlearn: 14933.5704207\ttest: 15358.8275046\tbest: 15358.8275046 (1225)\ttotal: 1m 52s\tremaining: 1m 57s\n",
            "1226:\ttotal: 1m 53s\tremaining: 1m 57s\n",
            "1227:\ttotal: 1m 53s\tremaining: 1m 57s\n",
            "1228:\ttotal: 1m 53s\tremaining: 1m 57s\n",
            "1229:\ttotal: 1m 53s\tremaining: 1m 57s\n",
            "1230:\tlearn: 14908.7181777\ttest: 15337.4805828\tbest: 15337.4805828 (1230)\ttotal: 1m 53s\tremaining: 1m 57s\n",
            "1231:\ttotal: 1m 54s\tremaining: 1m 57s\n",
            "1232:\ttotal: 1m 54s\tremaining: 1m 57s\n",
            "1233:\ttotal: 1m 54s\tremaining: 1m 57s\n",
            "1234:\ttotal: 1m 54s\tremaining: 1m 57s\n",
            "1235:\tlearn: 14885.3769784\ttest: 15318.4605336\tbest: 15318.4605336 (1235)\ttotal: 1m 55s\tremaining: 1m 57s\n",
            "1236:\ttotal: 1m 55s\tremaining: 1m 57s\n",
            "1237:\ttotal: 1m 55s\tremaining: 1m 57s\n",
            "1238:\ttotal: 1m 55s\tremaining: 1m 57s\n",
            "1239:\ttotal: 1m 56s\tremaining: 1m 58s\n",
            "1240:\tlearn: 14864.6525403\ttest: 15300.0241439\tbest: 15300.0241439 (1240)\ttotal: 1m 56s\tremaining: 1m 58s\n",
            "1241:\ttotal: 1m 56s\tremaining: 1m 58s\n",
            "1242:\ttotal: 1m 56s\tremaining: 1m 58s\n",
            "1243:\ttotal: 1m 56s\tremaining: 1m 57s\n",
            "1244:\ttotal: 1m 56s\tremaining: 1m 57s\n",
            "1245:\tlearn: 14849.7657314\ttest: 15288.7107283\tbest: 15288.7107283 (1245)\ttotal: 1m 56s\tremaining: 1m 57s\n",
            "1246:\ttotal: 1m 57s\tremaining: 1m 57s\n",
            "1247:\ttotal: 1m 57s\tremaining: 1m 57s\n",
            "1248:\ttotal: 1m 57s\tremaining: 1m 57s\n",
            "1249:\ttotal: 1m 57s\tremaining: 1m 57s\n",
            "1250:\tlearn: 14835.8930147\ttest: 15277.8166156\tbest: 15277.8166156 (1250)\ttotal: 1m 57s\tremaining: 1m 57s\n",
            "1251:\ttotal: 1m 57s\tremaining: 1m 57s\n",
            "1252:\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1253:\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1254:\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1255:\tlearn: 14816.4528860\ttest: 15262.0746358\tbest: 15262.0746358 (1255)\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1256:\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1257:\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1258:\ttotal: 1m 57s\tremaining: 1m 56s\n",
            "1259:\ttotal: 1m 58s\tremaining: 1m 56s\n",
            "1260:\tlearn: 14794.1444144\ttest: 15243.0222330\tbest: 15243.0222330 (1260)\ttotal: 1m 58s\tremaining: 1m 56s\n",
            "1261:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1262:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1263:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1264:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1265:\tlearn: 14777.9034123\ttest: 15229.9241914\tbest: 15229.9241914 (1265)\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1266:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1267:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1268:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1269:\ttotal: 1m 58s\tremaining: 1m 55s\n",
            "1270:\tlearn: 14753.4596014\ttest: 15209.7601181\tbest: 15209.7601181 (1270)\ttotal: 1m 58s\tremaining: 1m 54s\n",
            "1271:\ttotal: 1m 58s\tremaining: 1m 54s\n",
            "1272:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1273:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1274:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1275:\tlearn: 14725.6023537\ttest: 15187.3972924\tbest: 15187.3972924 (1275)\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1276:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1277:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1278:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1279:\ttotal: 1m 59s\tremaining: 1m 54s\n",
            "1280:\tlearn: 14710.2341230\ttest: 15173.4787629\tbest: 15173.4787629 (1280)\ttotal: 1m 59s\tremaining: 1m 53s\n",
            "1281:\ttotal: 1m 59s\tremaining: 1m 53s\n",
            "1282:\ttotal: 1m 59s\tremaining: 1m 53s\n",
            "1283:\ttotal: 1m 59s\tremaining: 1m 53s\n",
            "1284:\ttotal: 1m 59s\tremaining: 1m 53s\n",
            "1285:\tlearn: 14694.9746004\ttest: 15160.4920178\tbest: 15160.4920178 (1285)\ttotal: 2m\tremaining: 1m 53s\n",
            "1286:\ttotal: 2m\tremaining: 1m 53s\n",
            "1287:\ttotal: 2m\tremaining: 1m 53s\n",
            "1288:\ttotal: 2m\tremaining: 1m 52s\n",
            "1289:\ttotal: 2m\tremaining: 1m 52s\n",
            "1290:\tlearn: 14677.7723073\ttest: 15148.0449311\tbest: 15148.0449311 (1290)\ttotal: 2m\tremaining: 1m 52s\n",
            "1291:\ttotal: 2m\tremaining: 1m 52s\n",
            "1292:\ttotal: 2m\tremaining: 1m 52s\n",
            "1293:\ttotal: 2m\tremaining: 1m 52s\n",
            "1294:\ttotal: 2m\tremaining: 1m 52s\n",
            "1295:\tlearn: 14658.1830932\ttest: 15131.5251701\tbest: 15131.5251701 (1295)\ttotal: 2m\tremaining: 1m 52s\n",
            "1296:\ttotal: 2m\tremaining: 1m 52s\n",
            "1297:\ttotal: 2m\tremaining: 1m 51s\n",
            "1298:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1299:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1300:\tlearn: 14636.5097880\ttest: 15116.3202604\tbest: 15116.3202604 (1300)\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1301:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1302:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1303:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1304:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1305:\tlearn: 14621.1943583\ttest: 15105.7678021\tbest: 15105.7678021 (1305)\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1306:\ttotal: 2m 1s\tremaining: 1m 51s\n",
            "1307:\ttotal: 2m 1s\tremaining: 1m 50s\n",
            "1308:\ttotal: 2m 1s\tremaining: 1m 50s\n",
            "1309:\ttotal: 2m 1s\tremaining: 1m 50s\n",
            "1310:\tlearn: 14590.5883466\ttest: 15081.7769420\tbest: 15081.7769420 (1310)\ttotal: 2m 1s\tremaining: 1m 50s\n",
            "1311:\ttotal: 2m 2s\tremaining: 1m 50s\n",
            "1312:\ttotal: 2m 2s\tremaining: 1m 50s\n",
            "1313:\ttotal: 2m 2s\tremaining: 1m 50s\n",
            "1314:\ttotal: 2m 2s\tremaining: 1m 50s\n",
            "1315:\tlearn: 14566.7843611\ttest: 15064.2386889\tbest: 15064.2386889 (1315)\ttotal: 2m 2s\tremaining: 1m 50s\n",
            "1316:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1317:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1318:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1319:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1320:\tlearn: 14549.3180624\ttest: 15050.3706310\tbest: 15050.3706310 (1320)\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1321:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1322:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1323:\ttotal: 2m 2s\tremaining: 1m 49s\n",
            "1324:\ttotal: 2m 3s\tremaining: 1m 49s\n",
            "1325:\tlearn: 14530.2599654\ttest: 15032.6964927\tbest: 15032.6964927 (1325)\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1326:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1327:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1328:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1329:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1330:\tlearn: 14503.8361127\ttest: 15012.9271336\tbest: 15012.9271336 (1330)\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1331:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1332:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1333:\ttotal: 2m 3s\tremaining: 1m 48s\n",
            "1334:\ttotal: 2m 3s\tremaining: 1m 47s\n",
            "1335:\tlearn: 14482.9548242\ttest: 14997.6808113\tbest: 14997.6808113 (1335)\ttotal: 2m 3s\tremaining: 1m 47s\n",
            "1336:\ttotal: 2m 3s\tremaining: 1m 47s\n",
            "1337:\ttotal: 2m 3s\tremaining: 1m 47s\n",
            "1338:\ttotal: 2m 4s\tremaining: 1m 47s\n",
            "1339:\ttotal: 2m 4s\tremaining: 1m 47s\n",
            "1340:\tlearn: 14468.2714550\ttest: 14985.4602000\tbest: 14985.4602000 (1340)\ttotal: 2m 4s\tremaining: 1m 47s\n",
            "1341:\ttotal: 2m 4s\tremaining: 1m 47s\n",
            "1342:\ttotal: 2m 4s\tremaining: 1m 47s\n",
            "1343:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1344:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1345:\tlearn: 14452.6190300\ttest: 14971.9092076\tbest: 14971.9092076 (1345)\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1346:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1347:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1348:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1349:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1350:\tlearn: 14436.2600018\ttest: 14957.4264309\tbest: 14957.4264309 (1350)\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1351:\ttotal: 2m 4s\tremaining: 1m 46s\n",
            "1352:\ttotal: 2m 5s\tremaining: 1m 46s\n",
            "1353:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1354:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1355:\tlearn: 14415.2886408\ttest: 14941.0503200\tbest: 14941.0503200 (1355)\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1356:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1357:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1358:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1359:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1360:\tlearn: 14399.9468106\ttest: 14931.7027106\tbest: 14931.7027106 (1360)\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1361:\ttotal: 2m 5s\tremaining: 1m 45s\n",
            "1362:\ttotal: 2m 5s\tremaining: 1m 44s\n",
            "1363:\ttotal: 2m 5s\tremaining: 1m 44s\n",
            "1364:\ttotal: 2m 5s\tremaining: 1m 44s\n",
            "1365:\tlearn: 14392.9708514\ttest: 14926.9984935\tbest: 14926.9984935 (1365)\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1366:\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1367:\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1368:\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1369:\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1370:\tlearn: 14374.2481968\ttest: 14910.6353241\tbest: 14910.6353241 (1370)\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1371:\ttotal: 2m 6s\tremaining: 1m 44s\n",
            "1372:\ttotal: 2m 6s\tremaining: 1m 43s\n",
            "1373:\ttotal: 2m 6s\tremaining: 1m 43s\n",
            "1374:\ttotal: 2m 6s\tremaining: 1m 43s\n",
            "1375:\tlearn: 14351.9785495\ttest: 14893.3779005\tbest: 14893.3779005 (1375)\ttotal: 2m 6s\tremaining: 1m 43s\n",
            "1376:\ttotal: 2m 6s\tremaining: 1m 43s\n",
            "1377:\ttotal: 2m 7s\tremaining: 1m 43s\n",
            "1378:\ttotal: 2m 7s\tremaining: 1m 43s\n",
            "1379:\ttotal: 2m 7s\tremaining: 1m 43s\n",
            "1380:\tlearn: 14327.7024599\ttest: 14878.1005187\tbest: 14878.1005187 (1380)\ttotal: 2m 7s\tremaining: 1m 43s\n",
            "1381:\ttotal: 2m 8s\tremaining: 1m 43s\n",
            "1382:\ttotal: 2m 8s\tremaining: 1m 43s\n",
            "1383:\ttotal: 2m 8s\tremaining: 1m 43s\n",
            "1384:\ttotal: 2m 8s\tremaining: 1m 43s\n",
            "1385:\tlearn: 14310.7082653\ttest: 14865.6275491\tbest: 14865.6275491 (1385)\ttotal: 2m 9s\tremaining: 1m 43s\n",
            "1386:\ttotal: 2m 9s\tremaining: 1m 43s\n",
            "1387:\ttotal: 2m 9s\tremaining: 1m 44s\n",
            "1388:\ttotal: 2m 10s\tremaining: 1m 44s\n",
            "1389:\ttotal: 2m 10s\tremaining: 1m 44s\n",
            "1390:\tlearn: 14296.0388729\ttest: 14852.5955089\tbest: 14852.5955089 (1390)\ttotal: 2m 10s\tremaining: 1m 44s\n",
            "1391:\ttotal: 2m 10s\tremaining: 1m 44s\n",
            "1392:\ttotal: 2m 10s\tremaining: 1m 43s\n",
            "1393:\ttotal: 2m 10s\tremaining: 1m 43s\n",
            "1394:\ttotal: 2m 10s\tremaining: 1m 43s\n",
            "1395:\tlearn: 14284.0421487\ttest: 14843.9933271\tbest: 14843.9933271 (1395)\ttotal: 2m 11s\tremaining: 1m 43s\n",
            "1396:\ttotal: 2m 11s\tremaining: 1m 43s\n",
            "1397:\ttotal: 2m 11s\tremaining: 1m 43s\n",
            "1398:\ttotal: 2m 11s\tremaining: 1m 43s\n",
            "1399:\ttotal: 2m 11s\tremaining: 1m 43s\n",
            "1400:\tlearn: 14272.2892414\ttest: 14834.3532409\tbest: 14834.3532409 (1400)\ttotal: 2m 11s\tremaining: 1m 43s\n",
            "1401:\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1402:\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1403:\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1404:\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1405:\tlearn: 14264.1205981\ttest: 14830.9690517\tbest: 14830.9690517 (1405)\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1406:\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1407:\ttotal: 2m 11s\tremaining: 1m 42s\n",
            "1408:\ttotal: 2m 12s\tremaining: 1m 42s\n",
            "1409:\ttotal: 2m 12s\tremaining: 1m 42s\n",
            "1410:\tlearn: 14245.2209045\ttest: 14813.9846927\tbest: 14813.9846927 (1410)\ttotal: 2m 12s\tremaining: 1m 42s\n",
            "1411:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1412:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1413:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1414:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1415:\tlearn: 14222.8767145\ttest: 14797.7522319\tbest: 14797.7522319 (1415)\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1416:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1417:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1418:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1419:\ttotal: 2m 12s\tremaining: 1m 41s\n",
            "1420:\tlearn: 14205.5299948\ttest: 14784.6839556\tbest: 14784.6839556 (1420)\ttotal: 2m 12s\tremaining: 1m 40s\n",
            "1421:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1422:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1423:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1424:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1425:\tlearn: 14188.9923747\ttest: 14768.5627913\tbest: 14768.5627913 (1425)\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1426:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1427:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1428:\ttotal: 2m 13s\tremaining: 1m 40s\n",
            "1429:\ttotal: 2m 13s\tremaining: 1m 39s\n",
            "1430:\tlearn: 14171.7636810\ttest: 14754.1149565\tbest: 14754.1149565 (1430)\ttotal: 2m 13s\tremaining: 1m 39s\n",
            "1431:\ttotal: 2m 13s\tremaining: 1m 39s\n",
            "1432:\ttotal: 2m 13s\tremaining: 1m 39s\n",
            "1433:\ttotal: 2m 13s\tremaining: 1m 39s\n",
            "1434:\ttotal: 2m 13s\tremaining: 1m 39s\n",
            "1435:\tlearn: 14147.2096090\ttest: 14735.6630370\tbest: 14735.6630370 (1435)\ttotal: 2m 14s\tremaining: 1m 39s\n",
            "1436:\ttotal: 2m 14s\tremaining: 1m 39s\n",
            "1437:\ttotal: 2m 14s\tremaining: 1m 39s\n",
            "1438:\ttotal: 2m 14s\tremaining: 1m 39s\n",
            "1439:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1440:\tlearn: 14131.1378810\ttest: 14721.0366101\tbest: 14721.0366101 (1440)\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1441:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1442:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1443:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1444:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1445:\tlearn: 14113.7632079\ttest: 14706.6146582\tbest: 14706.6146582 (1445)\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1446:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1447:\ttotal: 2m 14s\tremaining: 1m 38s\n",
            "1448:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1449:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1450:\tlearn: 14101.2804028\ttest: 14695.9884336\tbest: 14695.9884336 (1450)\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1451:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1452:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1453:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1454:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1455:\tlearn: 14083.5563105\ttest: 14680.3719858\tbest: 14680.3719858 (1455)\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1456:\ttotal: 2m 15s\tremaining: 1m 37s\n",
            "1457:\ttotal: 2m 15s\tremaining: 1m 36s\n",
            "1458:\ttotal: 2m 15s\tremaining: 1m 36s\n",
            "1459:\ttotal: 2m 15s\tremaining: 1m 36s\n",
            "1460:\tlearn: 14065.8788074\ttest: 14667.5819507\tbest: 14667.5819507 (1460)\ttotal: 2m 15s\tremaining: 1m 36s\n",
            "1461:\ttotal: 2m 16s\tremaining: 1m 36s\n",
            "1462:\ttotal: 2m 16s\tremaining: 1m 36s\n",
            "1463:\ttotal: 2m 16s\tremaining: 1m 36s\n",
            "1464:\ttotal: 2m 16s\tremaining: 1m 36s\n",
            "1465:\tlearn: 14044.1853135\ttest: 14648.9837929\tbest: 14648.9837929 (1465)\ttotal: 2m 16s\tremaining: 1m 36s\n",
            "1466:\ttotal: 2m 16s\tremaining: 1m 36s\n",
            "1467:\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1468:\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1469:\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1470:\tlearn: 14030.0268495\ttest: 14638.0495617\tbest: 14638.0495617 (1470)\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1471:\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1472:\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1473:\ttotal: 2m 16s\tremaining: 1m 35s\n",
            "1474:\ttotal: 2m 17s\tremaining: 1m 35s\n",
            "1475:\tlearn: 14014.7875155\ttest: 14624.3122125\tbest: 14624.3122125 (1475)\ttotal: 2m 17s\tremaining: 1m 35s\n",
            "1476:\ttotal: 2m 17s\tremaining: 1m 35s\n",
            "1477:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1478:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1479:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1480:\tlearn: 13995.5384027\ttest: 14610.4739199\tbest: 14610.4739199 (1480)\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1481:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1482:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1483:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1484:\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1485:\tlearn: 13984.2420698\ttest: 14601.5300838\tbest: 14601.5300838 (1485)\ttotal: 2m 17s\tremaining: 1m 34s\n",
            "1486:\ttotal: 2m 17s\tremaining: 1m 33s\n",
            "1487:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1488:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1489:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1490:\tlearn: 13967.8613000\ttest: 14587.9790915\tbest: 14587.9790915 (1490)\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1491:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1492:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1493:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1494:\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1495:\tlearn: 13955.8552580\ttest: 14580.3151648\tbest: 14580.3151648 (1495)\ttotal: 2m 18s\tremaining: 1m 33s\n",
            "1496:\ttotal: 2m 18s\tremaining: 1m 32s\n",
            "1497:\ttotal: 2m 18s\tremaining: 1m 32s\n",
            "1498:\ttotal: 2m 18s\tremaining: 1m 32s\n",
            "1499:\ttotal: 2m 18s\tremaining: 1m 32s\n",
            "1500:\tlearn: 13937.4851285\ttest: 14566.3060451\tbest: 14566.3060451 (1500)\ttotal: 2m 19s\tremaining: 1m 32s\n",
            "1501:\ttotal: 2m 19s\tremaining: 1m 32s\n",
            "1502:\ttotal: 2m 19s\tremaining: 1m 32s\n",
            "1503:\ttotal: 2m 19s\tremaining: 1m 32s\n",
            "1504:\ttotal: 2m 19s\tremaining: 1m 32s\n",
            "1505:\tlearn: 13916.2357851\ttest: 14551.3858473\tbest: 14551.3858473 (1505)\ttotal: 2m 19s\tremaining: 1m 32s\n",
            "1506:\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1507:\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1508:\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1509:\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1510:\tlearn: 13895.0998089\ttest: 14533.0232238\tbest: 14533.0232238 (1510)\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1511:\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1512:\ttotal: 2m 19s\tremaining: 1m 31s\n",
            "1513:\ttotal: 2m 20s\tremaining: 1m 31s\n",
            "1514:\ttotal: 2m 20s\tremaining: 1m 31s\n",
            "1515:\tlearn: 13877.8866450\ttest: 14520.2228356\tbest: 14520.2228356 (1515)\ttotal: 2m 20s\tremaining: 1m 31s\n",
            "1516:\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1517:\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1518:\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1519:\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1520:\tlearn: 13862.2078195\ttest: 14508.0112833\tbest: 14508.0112833 (1520)\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1521:\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1522:\ttotal: 2m 20s\tremaining: 1m 30s\n",
            "1523:\ttotal: 2m 21s\tremaining: 1m 30s\n",
            "1524:\ttotal: 2m 21s\tremaining: 1m 30s\n",
            "1525:\tlearn: 13848.9112560\ttest: 14496.7353979\tbest: 14496.7353979 (1525)\ttotal: 2m 21s\tremaining: 1m 30s\n",
            "1526:\ttotal: 2m 21s\tremaining: 1m 30s\n",
            "1527:\ttotal: 2m 21s\tremaining: 1m 30s\n",
            "1528:\ttotal: 2m 22s\tremaining: 1m 30s\n",
            "1529:\ttotal: 2m 22s\tremaining: 1m 30s\n",
            "1530:\tlearn: 13833.9794110\ttest: 14483.2491128\tbest: 14483.2491128 (1530)\ttotal: 2m 22s\tremaining: 1m 30s\n",
            "1531:\ttotal: 2m 22s\tremaining: 1m 30s\n",
            "1532:\ttotal: 2m 23s\tremaining: 1m 30s\n",
            "1533:\ttotal: 2m 23s\tremaining: 1m 30s\n",
            "1534:\ttotal: 2m 23s\tremaining: 1m 30s\n",
            "1535:\tlearn: 13812.7052201\ttest: 14467.8681994\tbest: 14467.8681994 (1535)\ttotal: 2m 23s\tremaining: 1m 30s\n",
            "1536:\ttotal: 2m 23s\tremaining: 1m 29s\n",
            "1537:\ttotal: 2m 23s\tremaining: 1m 29s\n",
            "1538:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1539:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1540:\tlearn: 13793.1268768\ttest: 14450.8411336\tbest: 14450.8411336 (1540)\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1541:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1542:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1543:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1544:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1545:\tlearn: 13781.1705299\ttest: 14443.2108546\tbest: 14443.2108546 (1545)\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1546:\ttotal: 2m 24s\tremaining: 1m 29s\n",
            "1547:\ttotal: 2m 25s\tremaining: 1m 29s\n",
            "1548:\ttotal: 2m 25s\tremaining: 1m 29s\n",
            "1549:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1550:\tlearn: 13764.8860445\ttest: 14431.1753061\tbest: 14431.1753061 (1550)\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1551:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1552:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1553:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1554:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1555:\tlearn: 13748.6341715\ttest: 14417.3577199\tbest: 14417.3577199 (1555)\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1556:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1557:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1558:\ttotal: 2m 25s\tremaining: 1m 28s\n",
            "1559:\ttotal: 2m 25s\tremaining: 1m 27s\n",
            "1560:\tlearn: 13731.2020383\ttest: 14403.3990719\tbest: 14403.3990719 (1560)\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1561:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1562:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1563:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1564:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1565:\tlearn: 13710.2431011\ttest: 14387.6208560\tbest: 14387.6208560 (1565)\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1566:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1567:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1568:\ttotal: 2m 26s\tremaining: 1m 27s\n",
            "1569:\ttotal: 2m 26s\tremaining: 1m 26s\n",
            "1570:\tlearn: 13689.1614790\ttest: 14368.0171474\tbest: 14368.0171474 (1570)\ttotal: 2m 26s\tremaining: 1m 26s\n",
            "1571:\ttotal: 2m 26s\tremaining: 1m 26s\n",
            "1572:\ttotal: 2m 26s\tremaining: 1m 26s\n",
            "1573:\ttotal: 2m 26s\tremaining: 1m 26s\n",
            "1574:\ttotal: 2m 27s\tremaining: 1m 26s\n",
            "1575:\tlearn: 13672.7496497\ttest: 14353.0154185\tbest: 14353.0154185 (1575)\ttotal: 2m 27s\tremaining: 1m 26s\n",
            "1576:\ttotal: 2m 27s\tremaining: 1m 26s\n",
            "1577:\ttotal: 2m 27s\tremaining: 1m 26s\n",
            "1578:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1579:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1580:\tlearn: 13653.5176196\ttest: 14336.4658922\tbest: 14336.4658922 (1580)\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1581:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1582:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1583:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1584:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1585:\tlearn: 13640.2909400\ttest: 14327.8585338\tbest: 14327.8585338 (1585)\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1586:\ttotal: 2m 27s\tremaining: 1m 25s\n",
            "1587:\ttotal: 2m 28s\tremaining: 1m 25s\n",
            "1588:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1589:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1590:\tlearn: 13630.3394825\ttest: 14320.7045002\tbest: 14320.7045002 (1590)\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1591:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1592:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1593:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1594:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1595:\tlearn: 13615.2337044\ttest: 14308.2794140\tbest: 14308.2794140 (1595)\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1596:\ttotal: 2m 28s\tremaining: 1m 24s\n",
            "1597:\ttotal: 2m 28s\tremaining: 1m 23s\n",
            "1598:\ttotal: 2m 28s\tremaining: 1m 23s\n",
            "1599:\ttotal: 2m 28s\tremaining: 1m 23s\n",
            "1600:\tlearn: 13602.4868937\ttest: 14299.5672298\tbest: 14299.5672298 (1600)\ttotal: 2m 28s\tremaining: 1m 23s\n",
            "1601:\ttotal: 2m 29s\tremaining: 1m 23s\n",
            "1602:\ttotal: 2m 29s\tremaining: 1m 23s\n",
            "1603:\ttotal: 2m 29s\tremaining: 1m 23s\n",
            "1604:\ttotal: 2m 29s\tremaining: 1m 23s\n",
            "1605:\tlearn: 13588.4045255\ttest: 14288.2680498\tbest: 14288.2680498 (1605)\ttotal: 2m 29s\tremaining: 1m 23s\n",
            "1606:\ttotal: 2m 29s\tremaining: 1m 23s\n",
            "1607:\ttotal: 2m 29s\tremaining: 1m 22s\n",
            "1608:\ttotal: 2m 29s\tremaining: 1m 22s\n",
            "1609:\ttotal: 2m 29s\tremaining: 1m 22s\n",
            "1610:\tlearn: 13570.1042798\ttest: 14272.9570202\tbest: 14272.9570202 (1610)\ttotal: 2m 29s\tremaining: 1m 22s\n",
            "1611:\ttotal: 2m 29s\tremaining: 1m 22s\n",
            "1612:\ttotal: 2m 29s\tremaining: 1m 22s\n",
            "1613:\ttotal: 2m 30s\tremaining: 1m 22s\n",
            "1614:\ttotal: 2m 30s\tremaining: 1m 22s\n",
            "1615:\tlearn: 13554.2981103\ttest: 14261.4106585\tbest: 14261.4106585 (1615)\ttotal: 2m 30s\tremaining: 1m 22s\n",
            "1616:\ttotal: 2m 30s\tremaining: 1m 22s\n",
            "1617:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1618:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1619:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1620:\tlearn: 13543.3077133\ttest: 14251.1700891\tbest: 14251.1700891 (1620)\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1621:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1622:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1623:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1624:\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1625:\tlearn: 13523.1873821\ttest: 14234.8612737\tbest: 14234.8612737 (1625)\ttotal: 2m 30s\tremaining: 1m 21s\n",
            "1626:\ttotal: 2m 31s\tremaining: 1m 21s\n",
            "1627:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1628:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1629:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1630:\tlearn: 13506.0254663\ttest: 14223.3705602\tbest: 14223.3705602 (1630)\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1631:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1632:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1633:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1634:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1635:\tlearn: 13485.9905487\ttest: 14208.6897793\tbest: 14208.6897793 (1635)\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1636:\ttotal: 2m 31s\tremaining: 1m 20s\n",
            "1637:\ttotal: 2m 31s\tremaining: 1m 19s\n",
            "1638:\ttotal: 2m 31s\tremaining: 1m 19s\n",
            "1639:\ttotal: 2m 31s\tremaining: 1m 19s\n",
            "1640:\tlearn: 13463.7581728\ttest: 14191.7235383\tbest: 14191.7235383 (1640)\ttotal: 2m 32s\tremaining: 1m 19s\n",
            "1641:\ttotal: 2m 32s\tremaining: 1m 19s\n",
            "1642:\ttotal: 2m 32s\tremaining: 1m 19s\n",
            "1643:\ttotal: 2m 32s\tremaining: 1m 19s\n",
            "1644:\ttotal: 2m 32s\tremaining: 1m 19s\n",
            "1645:\tlearn: 13444.7218175\ttest: 14174.7029432\tbest: 14174.7029432 (1645)\ttotal: 2m 32s\tremaining: 1m 19s\n",
            "1646:\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1647:\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1648:\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1649:\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1650:\tlearn: 13427.7742121\ttest: 14161.7563165\tbest: 14161.7563165 (1650)\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1651:\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1652:\ttotal: 2m 32s\tremaining: 1m 18s\n",
            "1653:\ttotal: 2m 33s\tremaining: 1m 18s\n",
            "1654:\ttotal: 2m 33s\tremaining: 1m 18s\n",
            "1655:\tlearn: 13410.9865631\ttest: 14148.2790905\tbest: 14148.2790905 (1655)\ttotal: 2m 33s\tremaining: 1m 18s\n",
            "1656:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1657:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1658:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1659:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1660:\tlearn: 13397.4648185\ttest: 14136.2008351\tbest: 14136.2008351 (1660)\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1661:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1662:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1663:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1664:\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1665:\tlearn: 13378.9890867\ttest: 14121.6546453\tbest: 14121.6546453 (1665)\ttotal: 2m 33s\tremaining: 1m 17s\n",
            "1666:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1667:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1668:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1669:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1670:\tlearn: 13354.7782220\ttest: 14103.2609623\tbest: 14103.2609623 (1670)\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1671:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1672:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1673:\ttotal: 2m 34s\tremaining: 1m 16s\n",
            "1674:\ttotal: 2m 35s\tremaining: 1m 16s\n",
            "1675:\tlearn: 13336.9733750\ttest: 14089.1651349\tbest: 14089.1651349 (1675)\ttotal: 2m 35s\tremaining: 1m 16s\n",
            "1676:\ttotal: 2m 35s\tremaining: 1m 16s\n",
            "1677:\ttotal: 2m 35s\tremaining: 1m 16s\n",
            "1678:\ttotal: 2m 35s\tremaining: 1m 16s\n",
            "1679:\ttotal: 2m 35s\tremaining: 1m 16s\n",
            "1680:\tlearn: 13326.7579120\ttest: 14082.5999373\tbest: 14082.5999373 (1680)\ttotal: 2m 36s\tremaining: 1m 16s\n",
            "1681:\ttotal: 2m 36s\tremaining: 1m 15s\n",
            "1682:\ttotal: 2m 36s\tremaining: 1m 15s\n",
            "1683:\ttotal: 2m 36s\tremaining: 1m 15s\n",
            "1684:\ttotal: 2m 36s\tremaining: 1m 15s\n",
            "1685:\tlearn: 13317.5130577\ttest: 14076.2638034\tbest: 14076.2638034 (1685)\ttotal: 2m 36s\tremaining: 1m 15s\n",
            "1686:\ttotal: 2m 37s\tremaining: 1m 15s\n",
            "1687:\ttotal: 2m 37s\tremaining: 1m 15s\n",
            "1688:\ttotal: 2m 37s\tremaining: 1m 15s\n",
            "1689:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1690:\tlearn: 13301.7550305\ttest: 14067.9799812\tbest: 14067.9799812 (1690)\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1691:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1692:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1693:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1694:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1695:\tlearn: 13283.2451333\ttest: 14054.3979294\tbest: 14054.3979294 (1695)\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1696:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1697:\ttotal: 2m 38s\tremaining: 1m 15s\n",
            "1698:\ttotal: 2m 38s\tremaining: 1m 14s\n",
            "1699:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1700:\tlearn: 13268.8645940\ttest: 14043.6371137\tbest: 14043.6371137 (1700)\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1701:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1702:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1703:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1704:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1705:\tlearn: 13256.2948224\ttest: 14033.6294903\tbest: 14033.6294903 (1705)\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1706:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1707:\ttotal: 2m 39s\tremaining: 1m 14s\n",
            "1708:\ttotal: 2m 39s\tremaining: 1m 13s\n",
            "1709:\ttotal: 2m 39s\tremaining: 1m 13s\n",
            "1710:\tlearn: 13238.8005702\ttest: 14019.6837838\tbest: 14019.6837838 (1710)\ttotal: 2m 39s\tremaining: 1m 13s\n",
            "1711:\ttotal: 2m 39s\tremaining: 1m 13s\n",
            "1712:\ttotal: 2m 40s\tremaining: 1m 13s\n",
            "1713:\ttotal: 2m 40s\tremaining: 1m 13s\n",
            "1714:\ttotal: 2m 40s\tremaining: 1m 13s\n",
            "1715:\tlearn: 13229.0758470\ttest: 14012.3226870\tbest: 14012.3226870 (1715)\ttotal: 2m 40s\tremaining: 1m 13s\n",
            "1716:\ttotal: 2m 40s\tremaining: 1m 13s\n",
            "1717:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1718:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1719:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1720:\tlearn: 13219.9552307\ttest: 14006.6478611\tbest: 14006.6478611 (1720)\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1721:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1722:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1723:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1724:\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1725:\tlearn: 13204.9271012\ttest: 13997.1307188\tbest: 13997.1307188 (1725)\ttotal: 2m 40s\tremaining: 1m 12s\n",
            "1726:\ttotal: 2m 41s\tremaining: 1m 12s\n",
            "1727:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1728:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1729:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1730:\tlearn: 13192.6213352\ttest: 13989.4590272\tbest: 13989.4590272 (1730)\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1731:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1732:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1733:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1734:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1735:\tlearn: 13175.5355152\ttest: 13976.3299261\tbest: 13976.3299261 (1735)\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1736:\ttotal: 2m 41s\tremaining: 1m 11s\n",
            "1737:\ttotal: 2m 41s\tremaining: 1m 10s\n",
            "1738:\ttotal: 2m 41s\tremaining: 1m 10s\n",
            "1739:\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1740:\tlearn: 13159.6004489\ttest: 13962.2897469\tbest: 13962.2897469 (1740)\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1741:\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1742:\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1743:\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1744:\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1745:\tlearn: 13147.9888623\ttest: 13956.3858574\tbest: 13956.3858574 (1745)\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1746:\ttotal: 2m 42s\tremaining: 1m 10s\n",
            "1747:\ttotal: 2m 42s\tremaining: 1m 9s\n",
            "1748:\ttotal: 2m 42s\tremaining: 1m 9s\n",
            "1749:\ttotal: 2m 42s\tremaining: 1m 9s\n",
            "1750:\tlearn: 13134.0183081\ttest: 13949.1502927\tbest: 13949.1502927 (1750)\ttotal: 2m 42s\tremaining: 1m 9s\n",
            "1751:\ttotal: 2m 42s\tremaining: 1m 9s\n",
            "1752:\ttotal: 2m 43s\tremaining: 1m 9s\n",
            "1753:\ttotal: 2m 43s\tremaining: 1m 9s\n",
            "1754:\ttotal: 2m 43s\tremaining: 1m 9s\n",
            "1755:\tlearn: 13109.3291274\ttest: 13930.0137705\tbest: 13930.0137705 (1755)\ttotal: 2m 43s\tremaining: 1m 9s\n",
            "1756:\ttotal: 2m 43s\tremaining: 1m 9s\n",
            "1757:\ttotal: 2m 43s\tremaining: 1m 9s\n",
            "1758:\ttotal: 2m 43s\tremaining: 1m 8s\n",
            "1759:\ttotal: 2m 43s\tremaining: 1m 8s\n",
            "1760:\tlearn: 13095.3430435\ttest: 13919.4302527\tbest: 13919.4302527 (1760)\ttotal: 2m 43s\tremaining: 1m 8s\n",
            "1761:\ttotal: 2m 43s\tremaining: 1m 8s\n",
            "1762:\ttotal: 2m 43s\tremaining: 1m 8s\n",
            "1763:\ttotal: 2m 43s\tremaining: 1m 8s\n",
            "1764:\ttotal: 2m 44s\tremaining: 1m 8s\n",
            "1765:\tlearn: 13075.8206072\ttest: 13906.2558565\tbest: 13906.2558565 (1765)\ttotal: 2m 44s\tremaining: 1m 8s\n",
            "1766:\ttotal: 2m 44s\tremaining: 1m 8s\n",
            "1767:\ttotal: 2m 44s\tremaining: 1m 8s\n",
            "1768:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1769:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1770:\tlearn: 13059.5889229\ttest: 13894.8531449\tbest: 13894.8531449 (1770)\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1771:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1772:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1773:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1774:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1775:\tlearn: 13041.2514059\ttest: 13880.8052008\tbest: 13880.8052008 (1775)\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1776:\ttotal: 2m 44s\tremaining: 1m 7s\n",
            "1777:\ttotal: 2m 45s\tremaining: 1m 7s\n",
            "1778:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1779:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1780:\tlearn: 13029.2702114\ttest: 13871.4562973\tbest: 13871.4562973 (1780)\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1781:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1782:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1783:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1784:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1785:\tlearn: 13020.4586369\ttest: 13864.6930955\tbest: 13864.6930955 (1785)\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1786:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1787:\ttotal: 2m 45s\tremaining: 1m 6s\n",
            "1788:\ttotal: 2m 45s\tremaining: 1m 5s\n",
            "1789:\ttotal: 2m 45s\tremaining: 1m 5s\n",
            "1790:\tlearn: 13008.7655191\ttest: 13855.2083068\tbest: 13855.2083068 (1790)\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1791:\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1792:\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1793:\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1794:\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1795:\tlearn: 12990.3255058\ttest: 13841.6806091\tbest: 13841.6806091 (1795)\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1796:\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1797:\ttotal: 2m 46s\tremaining: 1m 5s\n",
            "1798:\ttotal: 2m 46s\tremaining: 1m 4s\n",
            "1799:\ttotal: 2m 46s\tremaining: 1m 4s\n",
            "1800:\tlearn: 12976.2120780\ttest: 13832.4818264\tbest: 13832.4818264 (1800)\ttotal: 2m 46s\tremaining: 1m 4s\n",
            "1801:\ttotal: 2m 46s\tremaining: 1m 4s\n",
            "1802:\ttotal: 2m 47s\tremaining: 1m 4s\n",
            "1803:\ttotal: 2m 47s\tremaining: 1m 4s\n",
            "1804:\ttotal: 2m 47s\tremaining: 1m 4s\n",
            "1805:\tlearn: 12959.8010252\ttest: 13820.1745074\tbest: 13820.1745074 (1805)\ttotal: 2m 47s\tremaining: 1m 4s\n",
            "1806:\ttotal: 2m 47s\tremaining: 1m 4s\n",
            "1807:\ttotal: 2m 47s\tremaining: 1m 4s\n",
            "1808:\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1809:\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1810:\tlearn: 12952.4678819\ttest: 13815.1454599\tbest: 13815.1454599 (1810)\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1811:\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1812:\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1813:\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1814:\ttotal: 2m 47s\tremaining: 1m 3s\n",
            "1815:\tlearn: 12937.9879523\ttest: 13804.7767701\tbest: 13804.7767701 (1815)\ttotal: 2m 48s\tremaining: 1m 3s\n",
            "1816:\ttotal: 2m 48s\tremaining: 1m 3s\n",
            "1817:\ttotal: 2m 48s\tremaining: 1m 3s\n",
            "1818:\ttotal: 2m 48s\tremaining: 1m 3s\n",
            "1819:\ttotal: 2m 48s\tremaining: 1m 3s\n",
            "1820:\tlearn: 12930.9972398\ttest: 13800.1346720\tbest: 13800.1346720 (1820)\ttotal: 2m 49s\tremaining: 1m 3s\n",
            "1821:\ttotal: 2m 49s\tremaining: 1m 2s\n",
            "1822:\ttotal: 2m 49s\tremaining: 1m 2s\n",
            "1823:\ttotal: 2m 49s\tremaining: 1m 2s\n",
            "1824:\ttotal: 2m 49s\tremaining: 1m 2s\n",
            "1825:\tlearn: 12924.8839577\ttest: 13796.8708383\tbest: 13796.8708383 (1825)\ttotal: 2m 50s\tremaining: 1m 2s\n",
            "1826:\ttotal: 2m 50s\tremaining: 1m 2s\n",
            "1827:\ttotal: 2m 50s\tremaining: 1m 2s\n",
            "1828:\ttotal: 2m 50s\tremaining: 1m 2s\n",
            "1829:\ttotal: 2m 51s\tremaining: 1m 2s\n",
            "1830:\tlearn: 12910.6222209\ttest: 13785.2442395\tbest: 13785.2442395 (1830)\ttotal: 2m 51s\tremaining: 1m 2s\n",
            "1831:\ttotal: 2m 51s\tremaining: 1m 2s\n",
            "1832:\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1833:\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1834:\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1835:\tlearn: 12902.2074312\ttest: 13781.0369741\tbest: 13781.0369741 (1835)\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1836:\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1837:\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1838:\ttotal: 2m 52s\tremaining: 1m 2s\n",
            "1839:\ttotal: 2m 52s\tremaining: 1m 1s\n",
            "1840:\tlearn: 12894.1948618\ttest: 13776.9047691\tbest: 13776.9047691 (1840)\ttotal: 2m 52s\tremaining: 1m 1s\n",
            "1841:\ttotal: 2m 52s\tremaining: 1m 1s\n",
            "1842:\ttotal: 2m 52s\tremaining: 1m 1s\n",
            "1843:\ttotal: 2m 52s\tremaining: 1m 1s\n",
            "1844:\ttotal: 2m 53s\tremaining: 1m 1s\n",
            "1845:\tlearn: 12883.4405168\ttest: 13769.9927407\tbest: 13769.9927407 (1845)\ttotal: 2m 53s\tremaining: 1m 1s\n",
            "1846:\ttotal: 2m 53s\tremaining: 1m 1s\n",
            "1847:\ttotal: 2m 53s\tremaining: 1m 1s\n",
            "1848:\ttotal: 2m 53s\tremaining: 1m 1s\n",
            "1849:\ttotal: 2m 53s\tremaining: 1m\n",
            "1850:\tlearn: 12869.3915375\ttest: 13759.3199268\tbest: 13759.3199268 (1850)\ttotal: 2m 53s\tremaining: 1m\n",
            "1851:\ttotal: 2m 53s\tremaining: 1m\n",
            "1852:\ttotal: 2m 53s\tremaining: 1m\n",
            "1853:\ttotal: 2m 53s\tremaining: 1m\n",
            "1854:\ttotal: 2m 53s\tremaining: 1m\n",
            "1855:\tlearn: 12855.9940308\ttest: 13750.0771432\tbest: 13750.0771432 (1855)\ttotal: 2m 53s\tremaining: 1m\n",
            "1856:\ttotal: 2m 53s\tremaining: 1m\n",
            "1857:\ttotal: 2m 54s\tremaining: 1m\n",
            "1858:\ttotal: 2m 54s\tremaining: 1m\n",
            "1859:\ttotal: 2m 54s\tremaining: 59.9s\n",
            "1860:\tlearn: 12841.2469896\ttest: 13739.3629167\tbest: 13739.3629167 (1860)\ttotal: 2m 54s\tremaining: 59.8s\n",
            "1861:\ttotal: 2m 54s\tremaining: 59.7s\n",
            "1862:\ttotal: 2m 54s\tremaining: 59.6s\n",
            "1863:\ttotal: 2m 54s\tremaining: 59.5s\n",
            "1864:\ttotal: 2m 54s\tremaining: 59.4s\n",
            "1865:\tlearn: 12831.4422882\ttest: 13732.1195871\tbest: 13732.0471150 (1864)\ttotal: 2m 54s\tremaining: 59.3s\n",
            "1866:\ttotal: 2m 54s\tremaining: 59.2s\n",
            "1867:\ttotal: 2m 54s\tremaining: 59.1s\n",
            "1868:\ttotal: 2m 54s\tremaining: 59s\n",
            "1869:\ttotal: 2m 54s\tremaining: 58.9s\n",
            "1870:\tlearn: 12820.9713610\ttest: 13724.2964805\tbest: 13724.2964805 (1870)\ttotal: 2m 55s\tremaining: 58.8s\n",
            "1871:\ttotal: 2m 55s\tremaining: 58.7s\n",
            "1872:\ttotal: 2m 55s\tremaining: 58.7s\n",
            "1873:\ttotal: 2m 55s\tremaining: 58.6s\n",
            "1874:\ttotal: 2m 55s\tremaining: 58.5s\n",
            "1875:\tlearn: 12808.2183384\ttest: 13714.2668567\tbest: 13714.2668567 (1875)\ttotal: 2m 55s\tremaining: 58.4s\n",
            "1876:\ttotal: 2m 55s\tremaining: 58.3s\n",
            "1877:\ttotal: 2m 55s\tremaining: 58.2s\n",
            "1878:\ttotal: 2m 55s\tremaining: 58.1s\n",
            "1879:\ttotal: 2m 55s\tremaining: 58s\n",
            "1880:\tlearn: 12797.0291607\ttest: 13705.4343171\tbest: 13705.4343171 (1880)\ttotal: 2m 55s\tremaining: 57.9s\n",
            "1881:\ttotal: 2m 55s\tremaining: 57.8s\n",
            "1882:\ttotal: 2m 55s\tremaining: 57.7s\n",
            "1883:\ttotal: 2m 56s\tremaining: 57.6s\n",
            "1884:\ttotal: 2m 56s\tremaining: 57.5s\n",
            "1885:\tlearn: 12780.5886014\ttest: 13692.4320422\tbest: 13692.4320422 (1885)\ttotal: 2m 56s\tremaining: 57.4s\n",
            "1886:\ttotal: 2m 56s\tremaining: 57.3s\n",
            "1887:\ttotal: 2m 56s\tremaining: 57.2s\n",
            "1888:\ttotal: 2m 56s\tremaining: 57.1s\n",
            "1889:\ttotal: 2m 56s\tremaining: 57s\n",
            "1890:\tlearn: 12766.5799994\ttest: 13681.4201624\tbest: 13681.4201624 (1890)\ttotal: 2m 56s\tremaining: 56.9s\n",
            "1891:\ttotal: 2m 56s\tremaining: 56.8s\n",
            "1892:\ttotal: 2m 56s\tremaining: 56.7s\n",
            "1893:\ttotal: 2m 56s\tremaining: 56.6s\n",
            "1894:\ttotal: 2m 56s\tremaining: 56.5s\n",
            "1895:\tlearn: 12753.3036246\ttest: 13669.3212007\tbest: 13669.3212007 (1895)\ttotal: 2m 56s\tremaining: 56.4s\n",
            "1896:\ttotal: 2m 57s\tremaining: 56.3s\n",
            "1897:\ttotal: 2m 57s\tremaining: 56.2s\n",
            "1898:\ttotal: 2m 57s\tremaining: 56.1s\n",
            "1899:\ttotal: 2m 57s\tremaining: 56s\n",
            "1900:\tlearn: 12736.4018320\ttest: 13656.0756266\tbest: 13656.0756266 (1900)\ttotal: 2m 57s\tremaining: 55.9s\n",
            "1901:\ttotal: 2m 57s\tremaining: 55.8s\n",
            "1902:\ttotal: 2m 57s\tremaining: 55.7s\n",
            "1903:\ttotal: 2m 57s\tremaining: 55.6s\n",
            "1904:\ttotal: 2m 57s\tremaining: 55.5s\n",
            "1905:\tlearn: 12721.0118596\ttest: 13643.2014721\tbest: 13643.2014721 (1905)\ttotal: 2m 57s\tremaining: 55.4s\n",
            "1906:\ttotal: 2m 57s\tremaining: 55.3s\n",
            "1907:\ttotal: 2m 57s\tremaining: 55.2s\n",
            "1908:\ttotal: 2m 57s\tremaining: 55.1s\n",
            "1909:\ttotal: 2m 57s\tremaining: 55s\n",
            "1910:\tlearn: 12708.0849040\ttest: 13634.2912837\tbest: 13634.2912837 (1910)\ttotal: 2m 58s\tremaining: 54.9s\n",
            "1911:\ttotal: 2m 58s\tremaining: 54.8s\n",
            "1912:\ttotal: 2m 58s\tremaining: 54.7s\n",
            "1913:\ttotal: 2m 58s\tremaining: 54.6s\n",
            "1914:\ttotal: 2m 58s\tremaining: 54.5s\n",
            "1915:\tlearn: 12697.6846371\ttest: 13626.1821711\tbest: 13626.1821711 (1915)\ttotal: 2m 58s\tremaining: 54.4s\n",
            "1916:\ttotal: 2m 58s\tremaining: 54.3s\n",
            "1917:\ttotal: 2m 58s\tremaining: 54.2s\n",
            "1918:\ttotal: 2m 58s\tremaining: 54.1s\n",
            "1919:\ttotal: 2m 58s\tremaining: 54s\n",
            "1920:\tlearn: 12683.9050987\ttest: 13615.7655980\tbest: 13615.7655980 (1920)\ttotal: 2m 58s\tremaining: 53.9s\n",
            "1921:\ttotal: 2m 58s\tremaining: 53.8s\n",
            "1922:\ttotal: 2m 59s\tremaining: 53.7s\n",
            "1923:\ttotal: 2m 59s\tremaining: 53.6s\n",
            "1924:\ttotal: 2m 59s\tremaining: 53.5s\n",
            "1925:\tlearn: 12668.5811277\ttest: 13604.8119547\tbest: 13604.8119547 (1925)\ttotal: 2m 59s\tremaining: 53.4s\n",
            "1926:\ttotal: 2m 59s\tremaining: 53.3s\n",
            "1927:\ttotal: 2m 59s\tremaining: 53.2s\n",
            "1928:\ttotal: 2m 59s\tremaining: 53.1s\n",
            "1929:\ttotal: 2m 59s\tremaining: 53s\n",
            "1930:\tlearn: 12653.4714671\ttest: 13593.8181928\tbest: 13593.8181928 (1930)\ttotal: 2m 59s\tremaining: 52.9s\n",
            "1931:\ttotal: 2m 59s\tremaining: 52.8s\n",
            "1932:\ttotal: 2m 59s\tremaining: 52.7s\n",
            "1933:\ttotal: 2m 59s\tremaining: 52.6s\n",
            "1934:\ttotal: 2m 59s\tremaining: 52.5s\n",
            "1935:\tlearn: 12639.5319725\ttest: 13582.3844217\tbest: 13582.3844217 (1935)\ttotal: 3m\tremaining: 52.4s\n",
            "1936:\ttotal: 3m\tremaining: 52.3s\n",
            "1937:\ttotal: 3m\tremaining: 52.3s\n",
            "1938:\ttotal: 3m\tremaining: 52.2s\n",
            "1939:\ttotal: 3m\tremaining: 52.1s\n",
            "1940:\tlearn: 12630.4299918\ttest: 13573.9660085\tbest: 13573.9660085 (1940)\ttotal: 3m\tremaining: 52s\n",
            "1941:\ttotal: 3m\tremaining: 51.9s\n",
            "1942:\ttotal: 3m\tremaining: 51.8s\n",
            "1943:\ttotal: 3m\tremaining: 51.7s\n",
            "1944:\ttotal: 3m\tremaining: 51.6s\n",
            "1945:\tlearn: 12618.2445813\ttest: 13564.6766356\tbest: 13564.6766356 (1945)\ttotal: 3m\tremaining: 51.5s\n",
            "1946:\ttotal: 3m\tremaining: 51.4s\n",
            "1947:\ttotal: 3m\tremaining: 51.3s\n",
            "1948:\ttotal: 3m 1s\tremaining: 51.2s\n",
            "1949:\ttotal: 3m 1s\tremaining: 51.1s\n",
            "1950:\tlearn: 12609.9346174\ttest: 13558.5255644\tbest: 13558.5255644 (1950)\ttotal: 3m 1s\tremaining: 51s\n",
            "1951:\ttotal: 3m 1s\tremaining: 50.9s\n",
            "1952:\ttotal: 3m 1s\tremaining: 50.8s\n",
            "1953:\ttotal: 3m 1s\tremaining: 50.7s\n",
            "1954:\ttotal: 3m 1s\tremaining: 50.6s\n",
            "1955:\tlearn: 12596.5836998\ttest: 13548.9786567\tbest: 13548.9786567 (1955)\ttotal: 3m 1s\tremaining: 50.5s\n",
            "1956:\ttotal: 3m 1s\tremaining: 50.4s\n",
            "1957:\ttotal: 3m 1s\tremaining: 50.3s\n",
            "1958:\ttotal: 3m 1s\tremaining: 50.2s\n",
            "1959:\ttotal: 3m 1s\tremaining: 50.1s\n",
            "1960:\tlearn: 12585.1639055\ttest: 13541.8000344\tbest: 13541.8000344 (1960)\ttotal: 3m 2s\tremaining: 50s\n",
            "1961:\ttotal: 3m 2s\tremaining: 49.9s\n",
            "1962:\ttotal: 3m 2s\tremaining: 49.9s\n",
            "1963:\ttotal: 3m 2s\tremaining: 49.8s\n",
            "1964:\ttotal: 3m 2s\tremaining: 49.7s\n",
            "1965:\tlearn: 12574.5407868\ttest: 13533.7724529\tbest: 13533.7724529 (1965)\ttotal: 3m 2s\tremaining: 49.7s\n",
            "1966:\ttotal: 3m 3s\tremaining: 49.6s\n",
            "1967:\ttotal: 3m 3s\tremaining: 49.6s\n",
            "1968:\ttotal: 3m 3s\tremaining: 49.5s\n",
            "1969:\ttotal: 3m 3s\tremaining: 49.4s\n",
            "1970:\tlearn: 12558.4993418\ttest: 13520.7339420\tbest: 13520.7339420 (1970)\ttotal: 3m 3s\tremaining: 49.4s\n",
            "1971:\ttotal: 3m 4s\tremaining: 49.3s\n",
            "1972:\ttotal: 3m 4s\tremaining: 49.3s\n",
            "1973:\ttotal: 3m 4s\tremaining: 49.2s\n",
            "1974:\ttotal: 3m 4s\tremaining: 49.1s\n",
            "1975:\tlearn: 12548.8848797\ttest: 13514.2489814\tbest: 13514.2489814 (1975)\ttotal: 3m 5s\tremaining: 49.1s\n",
            "1976:\ttotal: 3m 5s\tremaining: 49s\n",
            "1977:\ttotal: 3m 5s\tremaining: 49s\n",
            "1978:\ttotal: 3m 5s\tremaining: 48.9s\n",
            "1979:\ttotal: 3m 5s\tremaining: 48.8s\n",
            "1980:\tlearn: 12538.7757954\ttest: 13506.4672875\tbest: 13506.4672875 (1980)\ttotal: 3m 5s\tremaining: 48.7s\n",
            "1981:\ttotal: 3m 5s\tremaining: 48.6s\n",
            "1982:\ttotal: 3m 6s\tremaining: 48.5s\n",
            "1983:\ttotal: 3m 6s\tremaining: 48.4s\n",
            "1984:\ttotal: 3m 6s\tremaining: 48.3s\n",
            "1985:\tlearn: 12528.1930541\ttest: 13497.4859211\tbest: 13497.4859211 (1985)\ttotal: 3m 6s\tremaining: 48.2s\n",
            "1986:\ttotal: 3m 6s\tremaining: 48.1s\n",
            "1987:\ttotal: 3m 6s\tremaining: 48s\n",
            "1988:\ttotal: 3m 6s\tremaining: 47.9s\n",
            "1989:\ttotal: 3m 6s\tremaining: 47.8s\n",
            "1990:\tlearn: 12517.6739847\ttest: 13489.0920966\tbest: 13489.0920966 (1990)\ttotal: 3m 6s\tremaining: 47.7s\n",
            "1991:\ttotal: 3m 6s\tremaining: 47.6s\n",
            "1992:\ttotal: 3m 6s\tremaining: 47.5s\n",
            "1993:\ttotal: 3m 6s\tremaining: 47.4s\n",
            "1994:\ttotal: 3m 6s\tremaining: 47.3s\n",
            "1995:\tlearn: 12506.2052716\ttest: 13480.2867340\tbest: 13480.2867340 (1995)\ttotal: 3m 6s\tremaining: 47.2s\n",
            "1996:\ttotal: 3m 7s\tremaining: 47.1s\n",
            "1997:\ttotal: 3m 7s\tremaining: 47s\n",
            "1998:\ttotal: 3m 7s\tremaining: 46.9s\n",
            "1999:\ttotal: 3m 7s\tremaining: 46.8s\n",
            "2000:\tlearn: 12494.2893021\ttest: 13469.9193384\tbest: 13469.9193384 (2000)\ttotal: 3m 7s\tremaining: 46.7s\n",
            "2001:\ttotal: 3m 7s\tremaining: 46.6s\n",
            "2002:\ttotal: 3m 7s\tremaining: 46.5s\n",
            "2003:\ttotal: 3m 7s\tremaining: 46.4s\n",
            "2004:\ttotal: 3m 7s\tremaining: 46.3s\n",
            "2005:\tlearn: 12486.2433437\ttest: 13464.6521682\tbest: 13464.6521682 (2005)\ttotal: 3m 7s\tremaining: 46.2s\n",
            "2006:\ttotal: 3m 7s\tremaining: 46.1s\n",
            "2007:\ttotal: 3m 7s\tremaining: 46.1s\n",
            "2008:\ttotal: 3m 8s\tremaining: 46s\n",
            "2009:\ttotal: 3m 8s\tremaining: 45.9s\n",
            "2010:\tlearn: 12475.1287088\ttest: 13456.1069287\tbest: 13456.1069287 (2010)\ttotal: 3m 8s\tremaining: 45.8s\n",
            "2011:\ttotal: 3m 8s\tremaining: 45.7s\n",
            "2012:\ttotal: 3m 8s\tremaining: 45.6s\n",
            "2013:\ttotal: 3m 8s\tremaining: 45.5s\n",
            "2014:\ttotal: 3m 8s\tremaining: 45.4s\n",
            "2015:\tlearn: 12463.0589948\ttest: 13445.7317682\tbest: 13445.7317682 (2015)\ttotal: 3m 8s\tremaining: 45.3s\n",
            "2016:\ttotal: 3m 8s\tremaining: 45.2s\n",
            "2017:\ttotal: 3m 8s\tremaining: 45.1s\n",
            "2018:\ttotal: 3m 8s\tremaining: 45s\n",
            "2019:\ttotal: 3m 8s\tremaining: 44.9s\n",
            "2020:\tlearn: 12455.1947344\ttest: 13440.5021283\tbest: 13440.5021283 (2020)\ttotal: 3m 8s\tremaining: 44.8s\n",
            "2021:\ttotal: 3m 9s\tremaining: 44.7s\n",
            "2022:\ttotal: 3m 9s\tremaining: 44.6s\n",
            "2023:\ttotal: 3m 9s\tremaining: 44.5s\n",
            "2024:\ttotal: 3m 9s\tremaining: 44.4s\n",
            "2025:\tlearn: 12442.8043313\ttest: 13432.2325417\tbest: 13432.2325417 (2025)\ttotal: 3m 9s\tremaining: 44.3s\n",
            "2026:\ttotal: 3m 9s\tremaining: 44.2s\n",
            "2027:\ttotal: 3m 9s\tremaining: 44.1s\n",
            "2028:\ttotal: 3m 9s\tremaining: 44s\n",
            "2029:\ttotal: 3m 9s\tremaining: 43.9s\n",
            "2030:\tlearn: 12428.4439807\ttest: 13421.2724276\tbest: 13421.2724276 (2030)\ttotal: 3m 9s\tremaining: 43.8s\n",
            "2031:\ttotal: 3m 9s\tremaining: 43.7s\n",
            "2032:\ttotal: 3m 9s\tremaining: 43.6s\n",
            "2033:\ttotal: 3m 10s\tremaining: 43.5s\n",
            "2034:\ttotal: 3m 10s\tremaining: 43.4s\n",
            "2035:\tlearn: 12419.0515939\ttest: 13414.8172323\tbest: 13414.8172323 (2035)\ttotal: 3m 10s\tremaining: 43.3s\n",
            "2036:\ttotal: 3m 10s\tremaining: 43.2s\n",
            "2037:\ttotal: 3m 10s\tremaining: 43.1s\n",
            "2038:\ttotal: 3m 10s\tremaining: 43s\n",
            "2039:\ttotal: 3m 10s\tremaining: 42.9s\n",
            "2040:\tlearn: 12410.1336407\ttest: 13408.7865166\tbest: 13408.7865166 (2040)\ttotal: 3m 10s\tremaining: 42.8s\n",
            "2041:\ttotal: 3m 10s\tremaining: 42.7s\n",
            "2042:\ttotal: 3m 10s\tremaining: 42.6s\n",
            "2043:\ttotal: 3m 10s\tremaining: 42.5s\n",
            "2044:\ttotal: 3m 10s\tremaining: 42.5s\n",
            "2045:\tlearn: 12397.7828384\ttest: 13397.7836958\tbest: 13397.7836958 (2045)\ttotal: 3m 10s\tremaining: 42.4s\n",
            "2046:\ttotal: 3m 10s\tremaining: 42.3s\n",
            "2047:\ttotal: 3m 11s\tremaining: 42.2s\n",
            "2048:\ttotal: 3m 11s\tremaining: 42.1s\n",
            "2049:\ttotal: 3m 11s\tremaining: 42s\n",
            "2050:\tlearn: 12384.5460645\ttest: 13385.8387374\tbest: 13385.8387374 (2050)\ttotal: 3m 11s\tremaining: 41.9s\n",
            "2051:\ttotal: 3m 11s\tremaining: 41.8s\n",
            "2052:\ttotal: 3m 11s\tremaining: 41.7s\n",
            "2053:\ttotal: 3m 11s\tremaining: 41.6s\n",
            "2054:\ttotal: 3m 11s\tremaining: 41.5s\n",
            "2055:\tlearn: 12372.7434620\ttest: 13377.6713883\tbest: 13377.6713883 (2055)\ttotal: 3m 11s\tremaining: 41.4s\n",
            "2056:\ttotal: 3m 11s\tremaining: 41.3s\n",
            "2057:\ttotal: 3m 11s\tremaining: 41.2s\n",
            "2058:\ttotal: 3m 11s\tremaining: 41.1s\n",
            "2059:\ttotal: 3m 12s\tremaining: 41s\n",
            "2060:\tlearn: 12363.3961115\ttest: 13371.4517274\tbest: 13371.4517274 (2060)\ttotal: 3m 12s\tremaining: 40.9s\n",
            "2061:\ttotal: 3m 12s\tremaining: 40.8s\n",
            "2062:\ttotal: 3m 12s\tremaining: 40.7s\n",
            "2063:\ttotal: 3m 12s\tremaining: 40.6s\n",
            "2064:\ttotal: 3m 12s\tremaining: 40.5s\n",
            "2065:\tlearn: 12349.4589463\ttest: 13361.5502239\tbest: 13361.5502239 (2065)\ttotal: 3m 12s\tremaining: 40.4s\n",
            "2066:\ttotal: 3m 12s\tremaining: 40.3s\n",
            "2067:\ttotal: 3m 12s\tremaining: 40.2s\n",
            "2068:\ttotal: 3m 12s\tremaining: 40.1s\n",
            "2069:\ttotal: 3m 12s\tremaining: 40s\n",
            "2070:\tlearn: 12340.2901878\ttest: 13355.6696290\tbest: 13355.6696290 (2070)\ttotal: 3m 12s\tremaining: 40s\n",
            "2071:\ttotal: 3m 12s\tremaining: 39.9s\n",
            "2072:\ttotal: 3m 13s\tremaining: 39.8s\n",
            "2073:\ttotal: 3m 13s\tremaining: 39.7s\n",
            "2074:\ttotal: 3m 13s\tremaining: 39.6s\n",
            "2075:\tlearn: 12329.3774394\ttest: 13347.0402702\tbest: 13347.0402702 (2075)\ttotal: 3m 13s\tremaining: 39.5s\n",
            "2076:\ttotal: 3m 13s\tremaining: 39.4s\n",
            "2077:\ttotal: 3m 13s\tremaining: 39.3s\n",
            "2078:\ttotal: 3m 13s\tremaining: 39.2s\n",
            "2079:\ttotal: 3m 13s\tremaining: 39.1s\n",
            "2080:\tlearn: 12313.1527435\ttest: 13333.9668173\tbest: 13333.9668173 (2080)\ttotal: 3m 13s\tremaining: 39s\n",
            "2081:\ttotal: 3m 13s\tremaining: 38.9s\n",
            "2082:\ttotal: 3m 13s\tremaining: 38.8s\n",
            "2083:\ttotal: 3m 13s\tremaining: 38.7s\n",
            "2084:\ttotal: 3m 13s\tremaining: 38.6s\n",
            "2085:\tlearn: 12295.6786800\ttest: 13320.9412478\tbest: 13320.9412478 (2085)\ttotal: 3m 14s\tremaining: 38.5s\n",
            "2086:\ttotal: 3m 14s\tremaining: 38.4s\n",
            "2087:\ttotal: 3m 14s\tremaining: 38.3s\n",
            "2088:\ttotal: 3m 14s\tremaining: 38.2s\n",
            "2089:\ttotal: 3m 14s\tremaining: 38.1s\n",
            "2090:\tlearn: 12280.3997452\ttest: 13310.9789196\tbest: 13310.9789196 (2090)\ttotal: 3m 14s\tremaining: 38s\n",
            "2091:\ttotal: 3m 14s\tremaining: 37.9s\n",
            "2092:\ttotal: 3m 14s\tremaining: 37.8s\n",
            "2093:\ttotal: 3m 14s\tremaining: 37.7s\n",
            "2094:\ttotal: 3m 14s\tremaining: 37.6s\n",
            "2095:\tlearn: 12268.0753435\ttest: 13303.7161778\tbest: 13303.7161778 (2095)\ttotal: 3m 14s\tremaining: 37.6s\n",
            "2096:\ttotal: 3m 14s\tremaining: 37.5s\n",
            "2097:\ttotal: 3m 14s\tremaining: 37.4s\n",
            "2098:\ttotal: 3m 15s\tremaining: 37.3s\n",
            "2099:\ttotal: 3m 15s\tremaining: 37.2s\n",
            "2100:\tlearn: 12255.1250933\ttest: 13294.0515029\tbest: 13294.0515029 (2100)\ttotal: 3m 15s\tremaining: 37.1s\n",
            "2101:\ttotal: 3m 15s\tremaining: 37s\n",
            "2102:\ttotal: 3m 15s\tremaining: 36.9s\n",
            "2103:\ttotal: 3m 15s\tremaining: 36.8s\n",
            "2104:\ttotal: 3m 15s\tremaining: 36.7s\n",
            "2105:\tlearn: 12243.1516637\ttest: 13285.1102551\tbest: 13285.1102551 (2105)\ttotal: 3m 15s\tremaining: 36.6s\n",
            "2106:\ttotal: 3m 15s\tremaining: 36.5s\n",
            "2107:\ttotal: 3m 15s\tremaining: 36.4s\n",
            "2108:\ttotal: 3m 16s\tremaining: 36.4s\n",
            "2109:\ttotal: 3m 16s\tremaining: 36.3s\n",
            "2110:\tlearn: 12234.6506840\ttest: 13277.9044557\tbest: 13277.9044557 (2110)\ttotal: 3m 16s\tremaining: 36.2s\n",
            "2111:\ttotal: 3m 16s\tremaining: 36.1s\n",
            "2112:\ttotal: 3m 17s\tremaining: 36.1s\n",
            "2113:\ttotal: 3m 17s\tremaining: 36s\n",
            "2114:\ttotal: 3m 17s\tremaining: 35.9s\n",
            "2115:\tlearn: 12226.2809306\ttest: 13273.5237749\tbest: 13273.5237749 (2115)\ttotal: 3m 17s\tremaining: 35.9s\n",
            "2116:\ttotal: 3m 18s\tremaining: 35.8s\n",
            "2117:\ttotal: 3m 18s\tremaining: 35.8s\n",
            "2118:\ttotal: 3m 18s\tremaining: 35.7s\n",
            "2119:\ttotal: 3m 18s\tremaining: 35.6s\n",
            "2120:\tlearn: 12216.8318602\ttest: 13268.3161354\tbest: 13268.3161354 (2120)\ttotal: 3m 19s\tremaining: 35.6s\n",
            "2121:\ttotal: 3m 19s\tremaining: 35.5s\n",
            "2122:\ttotal: 3m 19s\tremaining: 35.4s\n",
            "2123:\ttotal: 3m 19s\tremaining: 35.3s\n",
            "2124:\ttotal: 3m 19s\tremaining: 35.2s\n",
            "2125:\tlearn: 12205.1620371\ttest: 13260.8864489\tbest: 13260.8864489 (2125)\ttotal: 3m 19s\tremaining: 35.2s\n",
            "2126:\ttotal: 3m 19s\tremaining: 35.1s\n",
            "2127:\ttotal: 3m 19s\tremaining: 35s\n",
            "2128:\ttotal: 3m 20s\tremaining: 34.9s\n",
            "2129:\ttotal: 3m 20s\tremaining: 34.8s\n",
            "2130:\tlearn: 12196.0988808\ttest: 13255.5998665\tbest: 13255.5998665 (2130)\ttotal: 3m 20s\tremaining: 34.7s\n",
            "2131:\ttotal: 3m 20s\tremaining: 34.6s\n",
            "2132:\ttotal: 3m 20s\tremaining: 34.5s\n",
            "2133:\ttotal: 3m 20s\tremaining: 34.4s\n",
            "2134:\ttotal: 3m 20s\tremaining: 34.3s\n",
            "2135:\tlearn: 12180.9853377\ttest: 13244.4119830\tbest: 13244.4119830 (2135)\ttotal: 3m 20s\tremaining: 34.2s\n",
            "2136:\ttotal: 3m 20s\tremaining: 34.1s\n",
            "2137:\ttotal: 3m 20s\tremaining: 34s\n",
            "2138:\ttotal: 3m 20s\tremaining: 33.9s\n",
            "2139:\ttotal: 3m 20s\tremaining: 33.8s\n",
            "2140:\tlearn: 12170.4709272\ttest: 13237.3200683\tbest: 13237.3200683 (2140)\ttotal: 3m 21s\tremaining: 33.7s\n",
            "2141:\ttotal: 3m 21s\tremaining: 33.6s\n",
            "2142:\ttotal: 3m 21s\tremaining: 33.5s\n",
            "2143:\ttotal: 3m 21s\tremaining: 33.4s\n",
            "2144:\ttotal: 3m 21s\tremaining: 33.3s\n",
            "2145:\tlearn: 12161.6112105\ttest: 13228.8304771\tbest: 13228.8304771 (2145)\ttotal: 3m 21s\tremaining: 33.2s\n",
            "2146:\ttotal: 3m 21s\tremaining: 33.1s\n",
            "2147:\ttotal: 3m 21s\tremaining: 33s\n",
            "2148:\ttotal: 3m 21s\tremaining: 32.9s\n",
            "2149:\ttotal: 3m 21s\tremaining: 32.8s\n",
            "2150:\tlearn: 12151.4244775\ttest: 13222.2212786\tbest: 13222.2212786 (2150)\ttotal: 3m 21s\tremaining: 32.7s\n",
            "2151:\ttotal: 3m 21s\tremaining: 32.6s\n",
            "2152:\ttotal: 3m 21s\tremaining: 32.5s\n",
            "2153:\ttotal: 3m 21s\tremaining: 32.4s\n",
            "2154:\ttotal: 3m 22s\tremaining: 32.3s\n",
            "2155:\tlearn: 12140.9574327\ttest: 13214.5754699\tbest: 13214.5754699 (2155)\ttotal: 3m 22s\tremaining: 32.3s\n",
            "2156:\ttotal: 3m 22s\tremaining: 32.2s\n",
            "2157:\ttotal: 3m 22s\tremaining: 32.1s\n",
            "2158:\ttotal: 3m 22s\tremaining: 32s\n",
            "2159:\ttotal: 3m 22s\tremaining: 31.9s\n",
            "2160:\tlearn: 12132.4277230\ttest: 13209.0662946\tbest: 13209.0662946 (2160)\ttotal: 3m 22s\tremaining: 31.8s\n",
            "2161:\ttotal: 3m 22s\tremaining: 31.7s\n",
            "2162:\ttotal: 3m 22s\tremaining: 31.6s\n",
            "2163:\ttotal: 3m 22s\tremaining: 31.5s\n",
            "2164:\ttotal: 3m 22s\tremaining: 31.4s\n",
            "2165:\tlearn: 12122.3543571\ttest: 13202.3005045\tbest: 13202.3005045 (2165)\ttotal: 3m 22s\tremaining: 31.3s\n",
            "2166:\ttotal: 3m 23s\tremaining: 31.2s\n",
            "2167:\ttotal: 3m 23s\tremaining: 31.1s\n",
            "2168:\ttotal: 3m 23s\tremaining: 31s\n",
            "2169:\ttotal: 3m 23s\tremaining: 30.9s\n",
            "2170:\tlearn: 12108.5732658\ttest: 13189.9685968\tbest: 13189.9685968 (2170)\ttotal: 3m 23s\tremaining: 30.8s\n",
            "2171:\ttotal: 3m 23s\tremaining: 30.7s\n",
            "2172:\ttotal: 3m 23s\tremaining: 30.6s\n",
            "2173:\ttotal: 3m 23s\tremaining: 30.5s\n",
            "2174:\ttotal: 3m 23s\tremaining: 30.4s\n",
            "2175:\tlearn: 12093.6561740\ttest: 13181.4660641\tbest: 13181.4660641 (2175)\ttotal: 3m 23s\tremaining: 30.3s\n",
            "2176:\ttotal: 3m 23s\tremaining: 30.2s\n",
            "2177:\ttotal: 3m 23s\tremaining: 30.1s\n",
            "2178:\ttotal: 3m 23s\tremaining: 30s\n",
            "2179:\ttotal: 3m 24s\tremaining: 29.9s\n",
            "2180:\tlearn: 12080.3293276\ttest: 13172.1948093\tbest: 13172.1948093 (2180)\ttotal: 3m 24s\tremaining: 29.9s\n",
            "2181:\ttotal: 3m 24s\tremaining: 29.8s\n",
            "2182:\ttotal: 3m 24s\tremaining: 29.7s\n",
            "2183:\ttotal: 3m 24s\tremaining: 29.6s\n",
            "2184:\ttotal: 3m 24s\tremaining: 29.5s\n",
            "2185:\tlearn: 12071.3500318\ttest: 13167.4750624\tbest: 13167.4750624 (2185)\ttotal: 3m 24s\tremaining: 29.4s\n",
            "2186:\ttotal: 3m 24s\tremaining: 29.3s\n",
            "2187:\ttotal: 3m 24s\tremaining: 29.2s\n",
            "2188:\ttotal: 3m 24s\tremaining: 29.1s\n",
            "2189:\ttotal: 3m 24s\tremaining: 29s\n",
            "2190:\tlearn: 12062.1874852\ttest: 13160.9629248\tbest: 13160.9629248 (2190)\ttotal: 3m 24s\tremaining: 28.9s\n",
            "2191:\ttotal: 3m 24s\tremaining: 28.8s\n",
            "2192:\ttotal: 3m 25s\tremaining: 28.7s\n",
            "2193:\ttotal: 3m 25s\tremaining: 28.6s\n",
            "2194:\ttotal: 3m 25s\tremaining: 28.5s\n",
            "2195:\tlearn: 12055.2511268\ttest: 13155.8135218\tbest: 13155.8135218 (2195)\ttotal: 3m 25s\tremaining: 28.4s\n",
            "2196:\ttotal: 3m 25s\tremaining: 28.3s\n",
            "2197:\ttotal: 3m 25s\tremaining: 28.2s\n",
            "2198:\ttotal: 3m 25s\tremaining: 28.1s\n",
            "2199:\ttotal: 3m 25s\tremaining: 28s\n",
            "2200:\tlearn: 12046.4892475\ttest: 13150.7081197\tbest: 13150.7081197 (2200)\ttotal: 3m 25s\tremaining: 27.9s\n",
            "2201:\ttotal: 3m 25s\tremaining: 27.8s\n",
            "2202:\ttotal: 3m 25s\tremaining: 27.8s\n",
            "2203:\ttotal: 3m 25s\tremaining: 27.7s\n",
            "2204:\ttotal: 3m 26s\tremaining: 27.6s\n",
            "2205:\tlearn: 12040.9132943\ttest: 13147.3109891\tbest: 13147.3109891 (2205)\ttotal: 3m 26s\tremaining: 27.5s\n",
            "2206:\ttotal: 3m 26s\tremaining: 27.4s\n",
            "2207:\ttotal: 3m 26s\tremaining: 27.3s\n",
            "2208:\ttotal: 3m 26s\tremaining: 27.2s\n",
            "2209:\ttotal: 3m 26s\tremaining: 27.1s\n",
            "2210:\tlearn: 12033.3308987\ttest: 13143.5851456\tbest: 13143.5851456 (2210)\ttotal: 3m 26s\tremaining: 27s\n",
            "2211:\ttotal: 3m 26s\tremaining: 26.9s\n",
            "2212:\ttotal: 3m 26s\tremaining: 26.8s\n",
            "2213:\ttotal: 3m 26s\tremaining: 26.7s\n",
            "2214:\ttotal: 3m 26s\tremaining: 26.6s\n",
            "2215:\tlearn: 12025.7057964\ttest: 13139.9356567\tbest: 13139.9356567 (2215)\ttotal: 3m 26s\tremaining: 26.5s\n",
            "2216:\ttotal: 3m 26s\tremaining: 26.4s\n",
            "2217:\ttotal: 3m 27s\tremaining: 26.3s\n",
            "2218:\ttotal: 3m 27s\tremaining: 26.2s\n",
            "2219:\ttotal: 3m 27s\tremaining: 26.1s\n",
            "2220:\tlearn: 12016.5944979\ttest: 13132.8217416\tbest: 13132.8217416 (2220)\ttotal: 3m 27s\tremaining: 26s\n",
            "2221:\ttotal: 3m 27s\tremaining: 25.9s\n",
            "2222:\ttotal: 3m 27s\tremaining: 25.8s\n",
            "2223:\ttotal: 3m 27s\tremaining: 25.8s\n",
            "2224:\ttotal: 3m 27s\tremaining: 25.7s\n",
            "2225:\tlearn: 12002.4686463\ttest: 13120.8185467\tbest: 13120.8185467 (2225)\ttotal: 3m 27s\tremaining: 25.6s\n",
            "2226:\ttotal: 3m 27s\tremaining: 25.5s\n",
            "2227:\ttotal: 3m 27s\tremaining: 25.4s\n",
            "2228:\ttotal: 3m 27s\tremaining: 25.3s\n",
            "2229:\ttotal: 3m 27s\tremaining: 25.2s\n",
            "2230:\tlearn: 11992.9163032\ttest: 13114.6441809\tbest: 13114.6441809 (2230)\ttotal: 3m 28s\tremaining: 25.1s\n",
            "2231:\ttotal: 3m 28s\tremaining: 25s\n",
            "2232:\ttotal: 3m 28s\tremaining: 24.9s\n",
            "2233:\ttotal: 3m 28s\tremaining: 24.8s\n",
            "2234:\ttotal: 3m 28s\tremaining: 24.7s\n",
            "2235:\tlearn: 11976.8748582\ttest: 13104.4967899\tbest: 13104.4967899 (2235)\ttotal: 3m 28s\tremaining: 24.6s\n",
            "2236:\ttotal: 3m 28s\tremaining: 24.5s\n",
            "2237:\ttotal: 3m 28s\tremaining: 24.4s\n",
            "2238:\ttotal: 3m 28s\tremaining: 24.3s\n",
            "2239:\ttotal: 3m 28s\tremaining: 24.2s\n",
            "2240:\tlearn: 11964.8470745\ttest: 13095.8596662\tbest: 13095.8596662 (2240)\ttotal: 3m 28s\tremaining: 24.1s\n",
            "2241:\ttotal: 3m 28s\tremaining: 24s\n",
            "2242:\ttotal: 3m 29s\tremaining: 23.9s\n",
            "2243:\ttotal: 3m 29s\tremaining: 23.9s\n",
            "2244:\ttotal: 3m 29s\tremaining: 23.8s\n",
            "2245:\tlearn: 11954.3955595\ttest: 13087.7324355\tbest: 13087.7324355 (2245)\ttotal: 3m 29s\tremaining: 23.7s\n",
            "2246:\ttotal: 3m 29s\tremaining: 23.6s\n",
            "2247:\ttotal: 3m 29s\tremaining: 23.5s\n",
            "2248:\ttotal: 3m 29s\tremaining: 23.4s\n",
            "2249:\ttotal: 3m 29s\tremaining: 23.3s\n",
            "2250:\tlearn: 11948.7520519\ttest: 13083.6015247\tbest: 13083.6015247 (2250)\ttotal: 3m 30s\tremaining: 23.2s\n",
            "2251:\ttotal: 3m 30s\tremaining: 23.2s\n",
            "2252:\ttotal: 3m 30s\tremaining: 23.1s\n",
            "2253:\ttotal: 3m 30s\tremaining: 23s\n",
            "2254:\ttotal: 3m 30s\tremaining: 22.9s\n",
            "2255:\tlearn: 11941.5144166\ttest: 13078.4132973\tbest: 13078.4132973 (2255)\ttotal: 3m 31s\tremaining: 22.8s\n",
            "2256:\ttotal: 3m 31s\tremaining: 22.8s\n",
            "2257:\ttotal: 3m 31s\tremaining: 22.7s\n",
            "2258:\ttotal: 3m 31s\tremaining: 22.6s\n",
            "2259:\ttotal: 3m 32s\tremaining: 22.5s\n",
            "2260:\tlearn: 11932.2260790\ttest: 13071.5012689\tbest: 13071.5012689 (2260)\ttotal: 3m 32s\tremaining: 22.4s\n",
            "2261:\ttotal: 3m 32s\tremaining: 22.4s\n",
            "2262:\ttotal: 3m 32s\tremaining: 22.3s\n",
            "2263:\ttotal: 3m 32s\tremaining: 22.2s\n",
            "2264:\ttotal: 3m 33s\tremaining: 22.1s\n",
            "2265:\tlearn: 11918.1980648\ttest: 13060.2189127\tbest: 13060.2189127 (2265)\ttotal: 3m 33s\tremaining: 22s\n",
            "2266:\ttotal: 3m 33s\tremaining: 21.9s\n",
            "2267:\ttotal: 3m 33s\tremaining: 21.8s\n",
            "2268:\ttotal: 3m 33s\tremaining: 21.7s\n",
            "2269:\ttotal: 3m 33s\tremaining: 21.6s\n",
            "2270:\tlearn: 11908.9796111\ttest: 13053.4505343\tbest: 13053.4505343 (2270)\ttotal: 3m 33s\tremaining: 21.5s\n",
            "2271:\ttotal: 3m 33s\tremaining: 21.4s\n",
            "2272:\ttotal: 3m 33s\tremaining: 21.4s\n",
            "2273:\ttotal: 3m 33s\tremaining: 21.3s\n",
            "2274:\ttotal: 3m 33s\tremaining: 21.2s\n",
            "2275:\tlearn: 11899.9312081\ttest: 13047.5621746\tbest: 13047.5621746 (2275)\ttotal: 3m 34s\tremaining: 21.1s\n",
            "2276:\ttotal: 3m 34s\tremaining: 21s\n",
            "2277:\ttotal: 3m 34s\tremaining: 20.9s\n",
            "2278:\ttotal: 3m 34s\tremaining: 20.8s\n",
            "2279:\ttotal: 3m 34s\tremaining: 20.7s\n",
            "2280:\tlearn: 11886.5663138\ttest: 13036.7638286\tbest: 13036.7638286 (2280)\ttotal: 3m 34s\tremaining: 20.6s\n",
            "2281:\ttotal: 3m 34s\tremaining: 20.5s\n",
            "2282:\ttotal: 3m 34s\tremaining: 20.4s\n",
            "2283:\ttotal: 3m 34s\tremaining: 20.3s\n",
            "2284:\ttotal: 3m 34s\tremaining: 20.2s\n",
            "2285:\tlearn: 11882.8244229\ttest: 13034.0888309\tbest: 13034.0888309 (2285)\ttotal: 3m 34s\tremaining: 20.1s\n",
            "2286:\ttotal: 3m 34s\tremaining: 20s\n",
            "2287:\ttotal: 3m 34s\tremaining: 19.9s\n",
            "2288:\ttotal: 3m 35s\tremaining: 19.8s\n",
            "2289:\ttotal: 3m 35s\tremaining: 19.7s\n",
            "2290:\tlearn: 11871.6298098\ttest: 13024.6726319\tbest: 13024.6726319 (2290)\ttotal: 3m 35s\tremaining: 19.6s\n",
            "2291:\ttotal: 3m 35s\tremaining: 19.5s\n",
            "2292:\ttotal: 3m 35s\tremaining: 19.4s\n",
            "2293:\ttotal: 3m 35s\tremaining: 19.3s\n",
            "2294:\ttotal: 3m 35s\tremaining: 19.2s\n",
            "2295:\tlearn: 11860.1339197\ttest: 13015.0881940\tbest: 13015.0881940 (2295)\ttotal: 3m 35s\tremaining: 19.2s\n",
            "2296:\ttotal: 3m 35s\tremaining: 19.1s\n",
            "2297:\ttotal: 3m 35s\tremaining: 19s\n",
            "2298:\ttotal: 3m 35s\tremaining: 18.9s\n",
            "2299:\ttotal: 3m 35s\tremaining: 18.8s\n",
            "2300:\tlearn: 11844.0295793\ttest: 13002.9271133\tbest: 13002.9271133 (2300)\ttotal: 3m 35s\tremaining: 18.7s\n",
            "2301:\ttotal: 3m 36s\tremaining: 18.6s\n",
            "2302:\ttotal: 3m 36s\tremaining: 18.5s\n",
            "2303:\ttotal: 3m 36s\tremaining: 18.4s\n",
            "2304:\ttotal: 3m 36s\tremaining: 18.3s\n",
            "2305:\tlearn: 11836.0333161\ttest: 12996.4628590\tbest: 12996.4628590 (2305)\ttotal: 3m 36s\tremaining: 18.2s\n",
            "2306:\ttotal: 3m 36s\tremaining: 18.1s\n",
            "2307:\ttotal: 3m 36s\tremaining: 18s\n",
            "2308:\ttotal: 3m 36s\tremaining: 17.9s\n",
            "2309:\ttotal: 3m 36s\tremaining: 17.8s\n",
            "2310:\tlearn: 11823.8727532\ttest: 12987.2847827\tbest: 12987.2847827 (2310)\ttotal: 3m 36s\tremaining: 17.7s\n",
            "2311:\ttotal: 3m 36s\tremaining: 17.6s\n",
            "2312:\ttotal: 3m 36s\tremaining: 17.5s\n",
            "2313:\ttotal: 3m 36s\tremaining: 17.4s\n",
            "2314:\ttotal: 3m 37s\tremaining: 17.3s\n",
            "2315:\tlearn: 11811.6982135\ttest: 12977.8918782\tbest: 12977.8918782 (2315)\ttotal: 3m 37s\tremaining: 17.3s\n",
            "2316:\ttotal: 3m 37s\tremaining: 17.2s\n",
            "2317:\ttotal: 3m 37s\tremaining: 17.1s\n",
            "2318:\ttotal: 3m 37s\tremaining: 17s\n",
            "2319:\ttotal: 3m 37s\tremaining: 16.9s\n",
            "2320:\tlearn: 11798.9149079\ttest: 12969.0114552\tbest: 12969.0114552 (2320)\ttotal: 3m 37s\tremaining: 16.8s\n",
            "2321:\ttotal: 3m 37s\tremaining: 16.7s\n",
            "2322:\ttotal: 3m 37s\tremaining: 16.6s\n",
            "2323:\ttotal: 3m 37s\tremaining: 16.5s\n",
            "2324:\ttotal: 3m 37s\tremaining: 16.4s\n",
            "2325:\tlearn: 11788.7041039\ttest: 12960.9851679\tbest: 12960.9851679 (2325)\ttotal: 3m 37s\tremaining: 16.3s\n",
            "2326:\ttotal: 3m 38s\tremaining: 16.2s\n",
            "2327:\ttotal: 3m 38s\tremaining: 16.1s\n",
            "2328:\ttotal: 3m 38s\tremaining: 16s\n",
            "2329:\ttotal: 3m 38s\tremaining: 15.9s\n",
            "2330:\tlearn: 11779.2542570\ttest: 12954.2983206\tbest: 12954.2983206 (2330)\ttotal: 3m 38s\tremaining: 15.8s\n",
            "2331:\ttotal: 3m 38s\tremaining: 15.7s\n",
            "2332:\ttotal: 3m 38s\tremaining: 15.6s\n",
            "2333:\ttotal: 3m 38s\tremaining: 15.5s\n",
            "2334:\ttotal: 3m 38s\tremaining: 15.4s\n",
            "2335:\tlearn: 11771.2145106\ttest: 12947.5907671\tbest: 12947.5907671 (2335)\ttotal: 3m 38s\tremaining: 15.4s\n",
            "2336:\ttotal: 3m 38s\tremaining: 15.3s\n",
            "2337:\ttotal: 3m 38s\tremaining: 15.2s\n",
            "2338:\ttotal: 3m 38s\tremaining: 15.1s\n",
            "2339:\ttotal: 3m 39s\tremaining: 15s\n",
            "2340:\tlearn: 11757.3922655\ttest: 12938.3246888\tbest: 12938.3246888 (2340)\ttotal: 3m 39s\tremaining: 14.9s\n",
            "2341:\ttotal: 3m 39s\tremaining: 14.8s\n",
            "2342:\ttotal: 3m 39s\tremaining: 14.7s\n",
            "2343:\ttotal: 3m 39s\tremaining: 14.6s\n",
            "2344:\ttotal: 3m 39s\tremaining: 14.5s\n",
            "2345:\tlearn: 11745.7007007\ttest: 12929.4805019\tbest: 12929.4805019 (2345)\ttotal: 3m 39s\tremaining: 14.4s\n",
            "2346:\ttotal: 3m 39s\tremaining: 14.3s\n",
            "2347:\ttotal: 3m 39s\tremaining: 14.2s\n",
            "2348:\ttotal: 3m 39s\tremaining: 14.1s\n",
            "2349:\ttotal: 3m 39s\tremaining: 14s\n",
            "2350:\tlearn: 11734.2312111\ttest: 12921.1746792\tbest: 12921.1746792 (2350)\ttotal: 3m 39s\tremaining: 13.9s\n",
            "2351:\ttotal: 3m 39s\tremaining: 13.8s\n",
            "2352:\ttotal: 3m 40s\tremaining: 13.7s\n",
            "2353:\ttotal: 3m 40s\tremaining: 13.7s\n",
            "2354:\ttotal: 3m 40s\tremaining: 13.6s\n",
            "2355:\tlearn: 11727.4020079\ttest: 12916.5196397\tbest: 12916.5196397 (2355)\ttotal: 3m 40s\tremaining: 13.5s\n",
            "2356:\ttotal: 3m 40s\tremaining: 13.4s\n",
            "2357:\ttotal: 3m 40s\tremaining: 13.3s\n",
            "2358:\ttotal: 3m 40s\tremaining: 13.2s\n",
            "2359:\ttotal: 3m 40s\tremaining: 13.1s\n",
            "2360:\tlearn: 11716.1724529\ttest: 12909.5687868\tbest: 12909.5687868 (2360)\ttotal: 3m 40s\tremaining: 13s\n",
            "2361:\ttotal: 3m 40s\tremaining: 12.9s\n",
            "2362:\ttotal: 3m 40s\tremaining: 12.8s\n",
            "2363:\ttotal: 3m 40s\tremaining: 12.7s\n",
            "2364:\ttotal: 3m 40s\tremaining: 12.6s\n",
            "2365:\tlearn: 11704.5088416\ttest: 12900.5602435\tbest: 12900.5602435 (2365)\ttotal: 3m 41s\tremaining: 12.5s\n",
            "2366:\ttotal: 3m 41s\tremaining: 12.4s\n",
            "2367:\ttotal: 3m 41s\tremaining: 12.3s\n",
            "2368:\ttotal: 3m 41s\tremaining: 12.2s\n",
            "2369:\ttotal: 3m 41s\tremaining: 12.1s\n",
            "2370:\tlearn: 11691.6696290\ttest: 12890.7829779\tbest: 12890.7829779 (2370)\ttotal: 3m 41s\tremaining: 12s\n",
            "2371:\ttotal: 3m 41s\tremaining: 12s\n",
            "2372:\ttotal: 3m 41s\tremaining: 11.9s\n",
            "2373:\ttotal: 3m 41s\tremaining: 11.8s\n",
            "2374:\ttotal: 3m 41s\tremaining: 11.7s\n",
            "2375:\tlearn: 11678.7558737\ttest: 12883.5292952\tbest: 12883.5292952 (2375)\ttotal: 3m 41s\tremaining: 11.6s\n",
            "2376:\ttotal: 3m 41s\tremaining: 11.5s\n",
            "2377:\ttotal: 3m 42s\tremaining: 11.4s\n",
            "2378:\ttotal: 3m 42s\tremaining: 11.3s\n",
            "2379:\ttotal: 3m 42s\tremaining: 11.2s\n",
            "2380:\tlearn: 11670.2525645\ttest: 12876.1034912\tbest: 12876.1034912 (2380)\ttotal: 3m 42s\tremaining: 11.1s\n",
            "2381:\ttotal: 3m 42s\tremaining: 11s\n",
            "2382:\ttotal: 3m 42s\tremaining: 10.9s\n",
            "2383:\ttotal: 3m 42s\tremaining: 10.8s\n",
            "2384:\ttotal: 3m 42s\tremaining: 10.7s\n",
            "2385:\tlearn: 11660.1147502\ttest: 12867.8766114\tbest: 12867.8766114 (2385)\ttotal: 3m 42s\tremaining: 10.6s\n",
            "2386:\ttotal: 3m 42s\tremaining: 10.5s\n",
            "2387:\ttotal: 3m 42s\tremaining: 10.4s\n",
            "2388:\ttotal: 3m 42s\tremaining: 10.4s\n",
            "2389:\ttotal: 3m 42s\tremaining: 10.3s\n",
            "2390:\tlearn: 11648.9791501\ttest: 12859.9059723\tbest: 12859.9059723 (2390)\ttotal: 3m 43s\tremaining: 10.2s\n",
            "2391:\ttotal: 3m 43s\tremaining: 10.1s\n",
            "2392:\ttotal: 3m 43s\tremaining: 9.98s\n",
            "2393:\ttotal: 3m 43s\tremaining: 9.89s\n",
            "2394:\ttotal: 3m 43s\tremaining: 9.8s\n",
            "2395:\tlearn: 11639.4757257\ttest: 12852.4607561\tbest: 12852.4607561 (2395)\ttotal: 3m 43s\tremaining: 9.71s\n",
            "2396:\ttotal: 3m 43s\tremaining: 9.62s\n",
            "2397:\ttotal: 3m 43s\tremaining: 9.53s\n",
            "2398:\ttotal: 3m 44s\tremaining: 9.44s\n",
            "2399:\ttotal: 3m 44s\tremaining: 9.35s\n",
            "2400:\tlearn: 11628.0784495\ttest: 12843.3978586\tbest: 12843.3978586 (2400)\ttotal: 3m 44s\tremaining: 9.26s\n",
            "2401:\ttotal: 3m 44s\tremaining: 9.18s\n",
            "2402:\ttotal: 3m 45s\tremaining: 9.09s\n",
            "2403:\ttotal: 3m 45s\tremaining: 9s\n",
            "2404:\ttotal: 3m 45s\tremaining: 8.91s\n",
            "2405:\tlearn: 11618.0540022\ttest: 12836.3486508\tbest: 12836.3486508 (2405)\ttotal: 3m 45s\tremaining: 8.82s\n",
            "2406:\ttotal: 3m 46s\tremaining: 8.74s\n",
            "2407:\ttotal: 3m 46s\tremaining: 8.65s\n",
            "2408:\ttotal: 3m 46s\tremaining: 8.56s\n",
            "2409:\ttotal: 3m 46s\tremaining: 8.47s\n",
            "2410:\tlearn: 11607.2973278\ttest: 12828.1580070\tbest: 12828.1580070 (2410)\ttotal: 3m 47s\tremaining: 8.38s\n",
            "2411:\ttotal: 3m 47s\tremaining: 8.29s\n",
            "2412:\ttotal: 3m 47s\tremaining: 8.19s\n",
            "2413:\ttotal: 3m 47s\tremaining: 8.1s\n",
            "2414:\ttotal: 3m 47s\tremaining: 8s\n",
            "2415:\tlearn: 11598.0532500\ttest: 12820.6778489\tbest: 12820.6778489 (2415)\ttotal: 3m 47s\tremaining: 7.91s\n",
            "2416:\ttotal: 3m 47s\tremaining: 7.81s\n",
            "2417:\ttotal: 3m 47s\tremaining: 7.72s\n",
            "2418:\ttotal: 3m 47s\tremaining: 7.62s\n",
            "2419:\ttotal: 3m 47s\tremaining: 7.53s\n",
            "2420:\tlearn: 11589.4831630\ttest: 12814.9525514\tbest: 12814.9525514 (2420)\ttotal: 3m 47s\tremaining: 7.43s\n",
            "2421:\ttotal: 3m 47s\tremaining: 7.34s\n",
            "2422:\ttotal: 3m 47s\tremaining: 7.25s\n",
            "2423:\ttotal: 3m 48s\tremaining: 7.15s\n",
            "2424:\ttotal: 3m 48s\tremaining: 7.05s\n",
            "2425:\tlearn: 11577.7869392\ttest: 12805.8909481\tbest: 12805.8909481 (2425)\ttotal: 3m 48s\tremaining: 6.96s\n",
            "2426:\ttotal: 3m 48s\tremaining: 6.87s\n",
            "2427:\ttotal: 3m 48s\tremaining: 6.77s\n",
            "2428:\ttotal: 3m 48s\tremaining: 6.68s\n",
            "2429:\ttotal: 3m 48s\tremaining: 6.58s\n",
            "2430:\tlearn: 11564.6844976\ttest: 12796.1693308\tbest: 12796.1693308 (2430)\ttotal: 3m 48s\tremaining: 6.49s\n",
            "2431:\ttotal: 3m 48s\tremaining: 6.39s\n",
            "2432:\ttotal: 3m 48s\tremaining: 6.3s\n",
            "2433:\ttotal: 3m 48s\tremaining: 6.21s\n",
            "2434:\ttotal: 3m 48s\tremaining: 6.11s\n",
            "2435:\tlearn: 11556.4265583\ttest: 12790.2939125\tbest: 12790.2939125 (2435)\ttotal: 3m 49s\tremaining: 6.02s\n",
            "2436:\ttotal: 3m 49s\tremaining: 5.92s\n",
            "2437:\ttotal: 3m 49s\tremaining: 5.83s\n",
            "2438:\ttotal: 3m 49s\tremaining: 5.73s\n",
            "2439:\ttotal: 3m 49s\tremaining: 5.64s\n",
            "2440:\tlearn: 11549.2277473\ttest: 12784.9646233\tbest: 12784.9646233 (2440)\ttotal: 3m 49s\tremaining: 5.54s\n",
            "2441:\ttotal: 3m 49s\tremaining: 5.45s\n",
            "2442:\ttotal: 3m 49s\tremaining: 5.36s\n",
            "2443:\ttotal: 3m 49s\tremaining: 5.26s\n",
            "2444:\ttotal: 3m 49s\tremaining: 5.17s\n",
            "2445:\tlearn: 11539.6078498\ttest: 12777.0069257\tbest: 12777.0069257 (2445)\ttotal: 3m 49s\tremaining: 5.07s\n",
            "2446:\ttotal: 3m 49s\tremaining: 4.98s\n",
            "2447:\ttotal: 3m 49s\tremaining: 4.88s\n",
            "2448:\ttotal: 3m 50s\tremaining: 4.79s\n",
            "2449:\ttotal: 3m 50s\tremaining: 4.7s\n",
            "2450:\tlearn: 11528.1329249\ttest: 12767.6593163\tbest: 12767.6593163 (2450)\ttotal: 3m 50s\tremaining: 4.6s\n",
            "2451:\ttotal: 3m 50s\tremaining: 4.51s\n",
            "2452:\ttotal: 3m 50s\tremaining: 4.41s\n",
            "2453:\ttotal: 3m 50s\tremaining: 4.32s\n",
            "2454:\ttotal: 3m 50s\tremaining: 4.22s\n",
            "2455:\tlearn: 11517.4997119\ttest: 12761.3167117\tbest: 12761.3167117 (2455)\ttotal: 3m 50s\tremaining: 4.13s\n",
            "2456:\ttotal: 3m 50s\tremaining: 4.04s\n",
            "2457:\ttotal: 3m 50s\tremaining: 3.94s\n",
            "2458:\ttotal: 3m 50s\tremaining: 3.85s\n",
            "2459:\ttotal: 3m 50s\tremaining: 3.75s\n",
            "2460:\tlearn: 11511.7428372\ttest: 12757.2763910\tbest: 12757.2763910 (2460)\ttotal: 3m 50s\tremaining: 3.66s\n",
            "2461:\ttotal: 3m 51s\tremaining: 3.56s\n",
            "2462:\ttotal: 3m 51s\tremaining: 3.47s\n",
            "2463:\ttotal: 3m 51s\tremaining: 3.38s\n",
            "2464:\ttotal: 3m 51s\tremaining: 3.28s\n",
            "2465:\tlearn: 11506.0224575\ttest: 12753.3680730\tbest: 12753.3680730 (2465)\ttotal: 3m 51s\tremaining: 3.19s\n",
            "2466:\ttotal: 3m 51s\tremaining: 3.1s\n",
            "2467:\ttotal: 3m 51s\tremaining: 3s\n",
            "2468:\ttotal: 3m 51s\tremaining: 2.91s\n",
            "2469:\ttotal: 3m 51s\tremaining: 2.81s\n",
            "2470:\tlearn: 11499.3757287\ttest: 12748.8618601\tbest: 12748.8618601 (2470)\ttotal: 3m 51s\tremaining: 2.72s\n",
            "2471:\ttotal: 3m 51s\tremaining: 2.63s\n",
            "2472:\ttotal: 3m 51s\tremaining: 2.53s\n",
            "2473:\ttotal: 3m 51s\tremaining: 2.44s\n",
            "2474:\ttotal: 3m 52s\tremaining: 2.34s\n",
            "2475:\tlearn: 11491.5961054\ttest: 12742.8725570\tbest: 12742.8725570 (2475)\ttotal: 3m 52s\tremaining: 2.25s\n",
            "2476:\ttotal: 3m 52s\tremaining: 2.16s\n",
            "2477:\ttotal: 3m 52s\tremaining: 2.06s\n",
            "2478:\ttotal: 3m 52s\tremaining: 1.97s\n",
            "2479:\ttotal: 3m 52s\tremaining: 1.87s\n",
            "2480:\tlearn: 11481.7362735\ttest: 12737.1770249\tbest: 12737.1770249 (2480)\ttotal: 3m 52s\tremaining: 1.78s\n",
            "2481:\ttotal: 3m 52s\tremaining: 1.69s\n",
            "2482:\ttotal: 3m 52s\tremaining: 1.59s\n",
            "2483:\ttotal: 3m 52s\tremaining: 1.5s\n",
            "2484:\ttotal: 3m 52s\tremaining: 1.41s\n",
            "2485:\tlearn: 11471.6613546\ttest: 12730.0268737\tbest: 12730.0268737 (2485)\ttotal: 3m 52s\tremaining: 1.31s\n",
            "2486:\ttotal: 3m 52s\tremaining: 1.22s\n",
            "2487:\ttotal: 3m 53s\tremaining: 1.12s\n",
            "2488:\ttotal: 3m 53s\tremaining: 1.03s\n",
            "2489:\ttotal: 3m 53s\tremaining: 937ms\n",
            "2490:\tlearn: 11463.7574934\ttest: 12723.3607328\tbest: 12723.3607328 (2490)\ttotal: 3m 53s\tremaining: 843ms\n",
            "2491:\ttotal: 3m 53s\tremaining: 750ms\n",
            "2492:\ttotal: 3m 53s\tremaining: 656ms\n",
            "2493:\ttotal: 3m 53s\tremaining: 563ms\n",
            "2494:\ttotal: 3m 54s\tremaining: 469ms\n",
            "2495:\tlearn: 11454.2338803\ttest: 12714.7766690\tbest: 12714.7766690 (2495)\ttotal: 3m 54s\tremaining: 376ms\n",
            "2496:\ttotal: 3m 54s\tremaining: 282ms\n",
            "2497:\ttotal: 3m 54s\tremaining: 188ms\n",
            "2498:\ttotal: 3m 54s\tremaining: 94ms\n",
            "2499:\tlearn: 11444.9261306\ttest: 12710.5383441\tbest: 12710.5383441 (2499)\ttotal: 3m 55s\tremaining: 0us\n",
            "bestTest = 12710.53834\n",
            "bestIteration = 2499\n",
            "12710.544879315743\n"
          ]
        }
      ],
      "source": [
        "# param = {'iterations': 2500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "# param = {'iterations': 3500, 'depth': 8, 'learning_rate': 0.0995, 'random_strength': 0, 'bagging_temperature': 0.0821, 'od_type': 'Iter', 'od_wait': 44}\n",
        "param = {'iterations': 2500, 'depth': 12, 'learning_rate': 0.0995, 'random_strength': 7, 'bagging_temperature': 0.01563641973518426, 'od_type': 'Iter', 'od_wait': 44}\n",
        "\n",
        "model = CatBoostRegressor(**param, \n",
        "                          loss_function='RMSE', \n",
        "                          eval_metric='MAE', \n",
        "                          task_type='GPU', \n",
        "                          verbose=True)\n",
        "\n",
        "\n",
        "model.fit(X_train_vall_comp, Y_train_vall_comp, \n",
        "          eval_set=(X_valid, y_valid), \n",
        "          cat_features = cat_features,\n",
        "          early_stopping_rounds=100)\n",
        "\n",
        "preds = model.predict(X_valid)\n",
        "mae = mean_absolute_error(y_valid, preds)\n",
        "print (mae)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "FSFKYCTdm0HH",
        "outputId": "8d8cbcdf-b4e1-4b8e-a7ec-510c893d6171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       apartment_id   city  dong  house_area  built_year  floor  \\\n",
              "0                 0  busan   197  101.647190        1993      3   \n",
              "1                 0  busan   197   91.511175        1993     12   \n",
              "2                 0  busan   197  125.865988        1993      2   \n",
              "3                 0  busan   197  101.647190        1993      8   \n",
              "4                 0  busan   197  101.647190        1993     13   \n",
              "...             ...    ...   ...         ...         ...    ...   \n",
              "85092          4419  seoul    37  100.821957        2014      8   \n",
              "85093          4419  seoul    37  101.431912        2014     11   \n",
              "85094          4419  seoul    37  121.201627        2014     12   \n",
              "85095          4419  seoul    37  137.192013        2014      3   \n",
              "85096          4419  seoul    37  100.821957        2014      4   \n",
              "\n",
              "       transaction_year  transaction_month  transaction_day  dong_mean_price  \\\n",
              "0                  2023                  1                3    198805.335373   \n",
              "1                  2023                  2                1    198805.335373   \n",
              "2                  2023                  2                2    198805.335373   \n",
              "3                  2023                  2                3    198805.335373   \n",
              "4                  2023                  3                3    198805.335373   \n",
              "...                 ...                ...              ...              ...   \n",
              "85092              2023                 10                3    800066.806614   \n",
              "85093              2023                 10                3    800066.806614   \n",
              "85094              2023                 11                1    800066.806614   \n",
              "85095              2023                 11                3    800066.806614   \n",
              "85096              2023                 11                3    800066.806614   \n",
              "\n",
              "         bus_lat    bus_long  seoul_lat  seoul_long  \n",
              "0      35.149929  129.006071   0.000000    0.000000  \n",
              "1      35.149929  129.006071   0.000000    0.000000  \n",
              "2      35.149929  129.006071   0.000000    0.000000  \n",
              "3      35.149929  129.006071   0.000000    0.000000  \n",
              "4      35.149929  129.006071   0.000000    0.000000  \n",
              "...          ...         ...        ...         ...  \n",
              "85092   0.000000    0.000000  37.452039  127.070842  \n",
              "85093   0.000000    0.000000  37.452039  127.070842  \n",
              "85094   0.000000    0.000000  37.452039  127.070842  \n",
              "85095   0.000000    0.000000  37.452039  127.070842  \n",
              "85096   0.000000    0.000000  37.452039  127.070842  \n",
              "\n",
              "[85097 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc27ea92-b16b-4070-b897-c677a8830e5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>apartment_id</th>\n",
              "      <th>city</th>\n",
              "      <th>dong</th>\n",
              "      <th>house_area</th>\n",
              "      <th>built_year</th>\n",
              "      <th>floor</th>\n",
              "      <th>transaction_year</th>\n",
              "      <th>transaction_month</th>\n",
              "      <th>transaction_day</th>\n",
              "      <th>dong_mean_price</th>\n",
              "      <th>bus_lat</th>\n",
              "      <th>bus_long</th>\n",
              "      <th>seoul_lat</th>\n",
              "      <th>seoul_long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>198805.335373</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>91.511175</td>\n",
              "      <td>1993</td>\n",
              "      <td>12</td>\n",
              "      <td>2023</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>198805.335373</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>125.865988</td>\n",
              "      <td>1993</td>\n",
              "      <td>2</td>\n",
              "      <td>2023</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>198805.335373</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>8</td>\n",
              "      <td>2023</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>198805.335373</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>busan</td>\n",
              "      <td>197</td>\n",
              "      <td>101.647190</td>\n",
              "      <td>1993</td>\n",
              "      <td>13</td>\n",
              "      <td>2023</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>198805.335373</td>\n",
              "      <td>35.149929</td>\n",
              "      <td>129.006071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85092</th>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>100.821957</td>\n",
              "      <td>2014</td>\n",
              "      <td>8</td>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>800066.806614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85093</th>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>101.431912</td>\n",
              "      <td>2014</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>800066.806614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85094</th>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>121.201627</td>\n",
              "      <td>2014</td>\n",
              "      <td>12</td>\n",
              "      <td>2023</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>800066.806614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85095</th>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>137.192013</td>\n",
              "      <td>2014</td>\n",
              "      <td>3</td>\n",
              "      <td>2023</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>800066.806614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85096</th>\n",
              "      <td>4419</td>\n",
              "      <td>seoul</td>\n",
              "      <td>37</td>\n",
              "      <td>100.821957</td>\n",
              "      <td>2014</td>\n",
              "      <td>4</td>\n",
              "      <td>2023</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>800066.806614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.452039</td>\n",
              "      <td>127.070842</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>85097 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc27ea92-b16b-4070-b897-c677a8830e5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc27ea92-b16b-4070-b897-c677a8830e5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc27ea92-b16b-4070-b897-c677a8830e5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "X_test = get_features_train(test.copy() , features_to_del = None)\n",
        "X_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee5IWgAbruxv"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(X_test)\n",
        "df_sub = test[[\"index\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# needed mean is: \n",
        "mean = 578775.706038251 \n",
        "std = 429421.5452772647"
      ],
      "metadata": {
        "id": "jFfRIAZxmRM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef = mean/preds.mean()\n",
        "coef"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEQVvsmTnJBp",
        "outputId": "f7361c9c-90d8-423e-fa34-27426fd76acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1095846227289217"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds1 = coef*preds\n",
        "preds1.mean(), preds1.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhOUZSJHmRCd",
        "outputId": "5554f91c-5b58-4cf3-91c8-c7e6482660a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578775.706038251, 429808.8690309193)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g5mgnHAu2gI",
        "outputId": "5dd920b6-e279-43cb-f8e6-43cad8ea2e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-2b898baa3866>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_sub[\"PRICE\"] = preds*coef\n"
          ]
        }
      ],
      "source": [
        "df_sub[\"PRICE\"] = preds*coef\n",
        "df_sub.to_csv(f\"sub22({coef}).csv\", index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub.PRICE.mean(), df_sub.PRICE.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tymQ6nfMrrWD",
        "outputId": "547b2ce6-b093-4ad8-8e85-b99d733528bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578775.7060382512, 425634.1660498236)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htWh-hqVrrTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnMSx0OdmERa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77156d1-8a8b-4669-ca3c-ed94b5238a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-f3dd755962e8>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_sub[\"PRICE\"] = preds*coef\n"
          ]
        }
      ],
      "source": [
        "df_sub[\"PRICE\"] = preds*coef\n",
        "df_sub.to_csv(f\"sub20({coef}).csv\", index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub[\"PRICE\"].mean(), df_sub[\"PRICE\"].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Px5kzJHnaKg",
        "outputId": "0abadb6b-8f00-465c-d5da-a0039ff41a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(580581.2865873021, 427413.9125539339)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJvd0IuPnaIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSpiCtniLDs1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwuLqLOLK-Ki",
        "outputId": "40b4a4a0-5ce1-4657-a3a9-56e3da574c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 11.661146159376182,\n",
              " 'city': 0.4859995012805072,\n",
              " 'dong': 12.930727783361572,\n",
              " 'house_area': 30.182339178855702,\n",
              " 'built_year': 6.779985210450806,\n",
              " 'floor': 3.179612752853449,\n",
              " 'transaction_year': 0.6402089456852865,\n",
              " 'transaction_month': 0.8251477915554555,\n",
              " 'transaction_day': 0.3208395444382476,\n",
              " 'transaction_time': 1.036320910447075,\n",
              " 'building_age': 2.042965509323038,\n",
              " 'bus_lat': 0.229974481993534,\n",
              " 'bus_long': 0.24798317722566662,\n",
              " 'seoul_lat': 10.175692768958545,\n",
              " 'seoul_long': 19.2610562841947}"
            ]
          },
          "execution_count": 298,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(zip(X_test.columns, model.feature_importances_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hk7VWefK-Dg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0PtOnB8HGhr",
        "outputId": "6e2a0092-929d-4151-82fe-6be647e20e5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 11.999280644939141,\n",
              " 'city': 0.4104072213881131,\n",
              " 'dong': 12.334593601488749,\n",
              " 'house_area': 29.87734391968986,\n",
              " 'built_year': 7.134389965708769,\n",
              " 'floor': 3.157095321138268,\n",
              " 'transaction_year': 2.3576714694108443,\n",
              " 'transaction_month': 0.6268237156239754,\n",
              " 'transaction_day': 0.33405104367651084,\n",
              " 'transaction_time': 1.9284742587053332,\n",
              " 'building_age': 1.6408140352965441,\n",
              " 'bus_lat': 0.19265560050439454,\n",
              " 'bus_long': 0.25426147178708586,\n",
              " 'seoul_lat': 10.012112320719552,\n",
              " 'seoul_long': 17.740025409923344}"
            ]
          },
          "execution_count": 278,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(zip(X_test.columns, model.feature_importances_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ4tbFwFBvOt",
        "outputId": "aecdf6a8-0de3-41de-e32b-1d7062343757"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 11.706811089384392,\n",
              " 'city': 0.37008685205879427,\n",
              " 'dong': 13.433071850041335,\n",
              " 'house_area': 29.397418772357998,\n",
              " 'built_year': 7.966563146812745,\n",
              " 'floor': 2.9667078683439705,\n",
              " 'lat': 9.797200764638436,\n",
              " 'long': 19.042972296041377,\n",
              " 'transaction_year': 2.049921285728468,\n",
              " 'transaction_month': 0.6463507603279064,\n",
              " 'transaction_day': 0.2898949635884091,\n",
              " 'transaction_time': 2.3330003506760724}"
            ]
          },
          "execution_count": 240,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(zip(X_test.columns, model.feature_importances_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ck8DGxz49su",
        "outputId": "22b41747-6508-4b7f-85c2-2aca64f7137c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 11.706811089384392,\n",
              " 'city': 0.37008685205879427,\n",
              " 'dong': 13.433071850041335,\n",
              " 'house_area': 29.397418772357998,\n",
              " 'built_year': 7.966563146812745,\n",
              " 'floor': 2.9667078683439705,\n",
              " 'lat': 9.797200764638436,\n",
              " 'long': 19.042972296041377,\n",
              " 'transaction_year': 2.049921285728468,\n",
              " 'transaction_month': 0.6463507603279064,\n",
              " 'transaction_day': 0.2898949635884091,\n",
              " 'transaction_time': 2.3330003506760724}"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(zip(X_test.columns, model.feature_importances_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj7NA21Vko53",
        "outputId": "baa9d481-e186-4bb1-e000-7e1bf771891a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apartment_id': 9.09205557153022,\n",
              " 'city': 0.5211245218211841,\n",
              " 'dong': 14.296653852147807,\n",
              " 'house_area': 31.340363470580805,\n",
              " 'built_year': 9.478478130308247,\n",
              " 'floor': 2.9154439426660583,\n",
              " 'lat': 9.982070420669311,\n",
              " 'long': 17.31296944373998,\n",
              " 'transaction_year': 2.3197817555988673,\n",
              " 'transaction_month': 0.6215174163740655,\n",
              " 'transaction_day': 0.2630945965985565,\n",
              " 'transaction_time': 1.8564468779651542}"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(zip(X_test.columns, model.feature_importances_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pWkuAt2ugCR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QJtYOUjx6EJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}